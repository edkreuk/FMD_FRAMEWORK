{
	"cells": [
		{
			"cell_type": "code",
			"source": [
				"%%configure -f\n",
				"{ \n",
				"    \"defaultLakehouse\": { \n",
				"        \"name\":  \"LH_CONFIGURATION\"\n",
				"    }\n",
				"}"
			],
			"outputs": [],
			"execution_count": null,
			"metadata": {
				"microsoft": {
					"language": "python",
					"language_group": "synapse_pyspark"
				}
			},
			"id": "3e4b1a66-8c9b-4be7-a7fa-cb5ba25d9fb6"
		},
		{
			"cell_type": "code",
			"source": [
				"# !!!!! create manually an sql database in the configuration workspace !!!!\n",
				"\n",
				"# target connections guid \n",
				"# vul de juiste ID in voor de betreffende connectie\n",
				"\n",
				"capacity_id = '560C908C-072A-478C-939C-CFC4398D2FDB' # Unique identifier for the capacity to be used.\n",
				"\n",
				"workspace_roles = [ # Keep emtpy [] if you only want to assign this to your personal account\n",
				"                    {\n",
				"                        \"principal\": {\n",
				"                            \"id\": \"ba4a392b-9596-46d8-9486-53eb2c2b22bf\",\n",
				"                            \"displayName\": \"sg-fabric-contributor\",\n",
				"                            \"type\": \"Group\"\n",
				"                        },\n",
				"                        \"role\": \"Member\"\n",
				"                        },\n",
				"                        {\n",
				"                        \"principal\": {\n",
				"                            \"id\": \"39b98289-1540-4866-b9f0-3935f9eeebd6\",\n",
				"                            \"displayName\": \"sg-fabric-admin\",\n",
				"                            \"type\": \"Group\"\n",
				"                        },\n",
				"                        \"role\": \"Admin\"\n",
				"                        }\n",
				"                    ]\n",
				"\n",
				"logging = {\n",
				"                    'workspace': {\n",
				"                        'name' : 'FMD_FRAMEWORK_LOG_MH', # Name of target workspace\n",
				"                        'roles' : workspace_roles,\n",
				"                        'capacity_id' : capacity_id\n",
				"                    },\n",
				"                    'WarehouseName' : 'WH_LOG' # Name of target logging warehouse\n",
				"}\n",
				"\n",
				"configuration = {\n",
				"                    'workspace': {\n",
				"                        'name' : 'FMD_FRAMEWORK_CONFIG_MH', # Name of target workspace\n",
				"                        'roles' : workspace_roles,\n",
				"                        'capacity_id' : capacity_id\n",
				"                    },\n",
				"                    'DatabaseName' : 'SQL_FMD_FRAMEWORK' # Name of target configuration SQL Database\n",
				"}\n",
				"\n",
				"environments = [\n",
				"                    {\n",
				"                        'environment_name' : 'development', # Name of target environment\n",
				"                        'workspaces': {\n",
				"                            'data' : {\n",
				"                                'name' : 'FMD_FRAMEWORK_DATA_MH3 (dvlm)', # Name of target code workspace for development\n",
				"                                'roles' : workspace_roles,\n",
				"                                'capacity_id' : capacity_id\n",
				"                            },\n",
				"                            'code' : {\n",
				"                                'name' : 'FMD_FRAMEWORK_CODE_MH3 (dvlm)', # Name of target data workspace for development\n",
				"                                'roles' : workspace_roles,\n",
				"                                'capacity_id' : capacity_id\n",
				"                            }\n",
				"                        },\n",
				"                        'connections' : {\n",
				"                            'CON_FMD_FABRIC_API' : '644f217a-8761-4f19-b1b7-a40bfe0ec2af', # Required Guid to the Fabric API (web v2) connection (https://api.fabric.microsoft.com/v1/)\n",
				"                            'CON_FMD_FABRIC_SQL' : '372237f9-709a-48f8-8fb2-ce06940c990e', # Required Guid to the Fabric SQL connection\n",
				"                            'CON_FMD_FABRIC_PIPELINES' : '6d8146c6-a438-47df-94e2-540c552eb6d7', # Required Guid to the Fabric datapipelines connection\n",
				"                            'CON_FMD_ASQL_01' : 'cf673e6a-13f6-4ebb-9cbb-4ba4ab390818', # Optional Guid to an Azure SQL database connection\n",
				"                            'CON_FMD_ASQL_02' : '11a8e5fe-fbca-4822-9ba4-9162cf56e6dd', # Optional Guid to an second Azure SQL database connection\n",
				"                            'CON_FMD_ADLS_01' : 'a0581b6e-5e38-46eb-bab2-7f08e9a35c30', # Optional Guid to an Azure SQL Datalake storage connection\n",
				"                            'CON_FMD_ADF_PIPELINES' : 'e93f565a-e2bc-4b60-900e-1907e825e37c' # Optional Guid to an Azure Datafactory connection\n",
				"                        }\n",
				"                    },\n",
				"                    {\n",
				"                        'environment_name' : 'production', # Name of target environment\n",
				"                        'workspaces': {\n",
				"                            'data' : {\n",
				"                                'name' : 'FMD_FRAMEWORK_DATA_MH (prod)',\n",
				"                                'roles' : workspace_roles,\n",
				"                                'capacity_id' : capacity_id\n",
				"                            },\n",
				"                            'code' : {\n",
				"                                'name' : 'FMD_FRAMEWORK_CODE_MH (prod)',\n",
				"                                'roles' : workspace_roles,\n",
				"                                'capacity_id' : capacity_id\n",
				"                            }\n",
				"                        },\n",
				"                        'connections' : {\n",
				"                            'CON_FMD_FABRIC_API' : '644f217a-8761-4f19-b1b7-a40bfe0ec2af',\n",
				"                            'CON_FMD_FABRIC_SQL' : '372237f9-709a-48f8-8fb2-ce06940c990e',\n",
				"                            'CON_FMD_FABRIC_PIPELINES' : '6d8146c6-a438-47df-94e2-540c552eb6d7',\n",
				"                            'CON_FMD_ASQL_01' : 'cf673e6a-13f6-4ebb-9cbb-4ba4ab390818',\n",
				"                            'CON_FMD_ASQL_02' : '11a8e5fe-fbca-4822-9ba4-9162cf56e6dd',\n",
				"                            'CON_FMD_ADLS_01' : 'a0581b6e-5e38-46eb-bab2-7f08e9a35c30',\n",
				"                            'CON_FMD_ADF_PIPELINES' : 'e93f565a-e2bc-4b60-900e-1907e825e37c'\n",
				"                        }\n",
				"                    }\n",
				"                ]\n",
				"\n",
				"# source file to read the deployment manifest from (LH_CONFIGURATION) \n",
				"deployment_file = 'deployment/FMD_deployment.json'"
			],
			"outputs": [],
			"execution_count": null,
			"metadata": {
				"tags": [
					"parameters"
				],
				"microsoft": {
					"language": "python",
					"language_group": "synapse_pyspark"
				}
			},
			"id": "1954c1d5-24fb-40dd-8784-7faf4ab0e12d"
		},
		{
			"cell_type": "code",
			"source": [
				"from json import loads, dumps\n",
				"import json\n",
				"import requests\n",
				"import base64\n",
				"import time\n",
				"import uuid\n",
				"import struct\n",
				"import pyodbc\n",
				"\n",
				"from typing import Callable, List, Dict, Optional, Any\n",
				"from datetime import datetime\n",
				"from time import sleep, time\n",
				"from dataclasses import dataclass, field"
			],
			"outputs": [],
			"execution_count": null,
			"metadata": {
				"microsoft": {
					"language": "python",
					"language_group": "synapse_pyspark"
				}
			},
			"id": "762b85d2-c208-42a6-98cf-5bdfef91c4e9"
		},
		{
			"cell_type": "code",
			"source": [
				"%run NB_FMD_DEPLOYMENT_UTILS"
			],
			"outputs": [],
			"execution_count": null,
			"metadata": {
				"jupyter": {
					"source_hidden": false,
					"outputs_hidden": false
				},
				"nteract": {
					"transient": {
						"deleting": false
					}
				},
				"microsoft": {
					"language": "python",
					"language_group": "synapse_pyspark"
				}
			},
			"id": "4d77bb62-d0e3-4601-8742-f37f7cc2195f"
		},
		{
			"cell_type": "code",
			"source": [
				"tasks=[]"
			],
			"outputs": [],
			"execution_count": null,
			"metadata": {
				"jupyter": {
					"source_hidden": false,
					"outputs_hidden": false
				},
				"nteract": {
					"transient": {
						"deleting": false
					}
				},
				"microsoft": {
					"language": "python",
					"language_group": "synapse_pyspark"
				}
			},
			"id": "31372683-48cc-427e-abea-8defcfaf8b27"
		},
		{
			"cell_type": "code",
			"source": [
				"fmd_api_access_token =  notebookutils.credentials.getToken('https://analysis.windows.net/powerbi/api')\n",
				"fabric_session = create_fabric_session(fabric_token = fmd_api_access_token)"
			],
			"outputs": [],
			"execution_count": null,
			"metadata": {
				"jupyter": {
					"source_hidden": false,
					"outputs_hidden": false
				},
				"nteract": {
					"transient": {
						"deleting": false
					}
				},
				"microsoft": {
					"language": "python",
					"language_group": "synapse_pyspark"
				}
			},
			"id": "4f177a49-b384-4d07-9895-423060188b64"
		},
		{
			"cell_type": "code",
			"source": [
				"# check if token is valid\n",
				"for token in [fmd_api_access_token]:\n",
				"    if not token:\n",
				"        continue\n",
				"    header, payload, signature = token.split('.')\n",
				"    payload += '=' * (-len(payload) % 4)  # Add padding\n",
				"    token_dict = loads(base64.urlsafe_b64decode(payload))\n",
				"    directory_id = token_dict.get(\"tid\")\n",
				"    timest = token_dict.get(\"exp\")\n",
				"    expiry = (datetime.fromtimestamp(timest) - datetime.now()).total_seconds() // 60\n",
				"    expiry_str = str(expiry) if expiry < 5 else str(expiry)\n",
				"    print(F\"token {directory_id} will expire in {expiry_str} minutes at\\t{datetime.fromtimestamp(timest)} UTC\")\n",
				"print(F\"Current time:\\t\\t\\t\\t\\t\\t\\t\\t\\t{datetime.now().replace(microsecond=0)} UTC\")"
			],
			"outputs": [],
			"execution_count": null,
			"metadata": {
				"microsoft": {
					"language": "python",
					"language_group": "synapse_pyspark"
				}
			},
			"id": "15902111-9dbf-4c18-bb00-496f36e98f82"
		},
		{
			"cell_type": "code",
			"source": [
				"# Create necessary workspaces \n",
				"start = time()\n",
				"\n",
				"for environment in environments:\n",
				"\n",
				"    print(f\"--------------------------\")\n",
				"    print(f\"Processing: {environment['environment_name']}\")\n",
				"    \n",
				"    # Loop through the workspace names and get their IDs\n",
				"    \n",
				"    for workspace in [environment['workspaces']['data'], environment['workspaces']['code'], configuration['workspace'], logging['workspace']]:\n",
				"        \n",
				"        print(f\" -----\")\n",
				"        print(f\" - Processing: data workspace {environment['environment_name']}\")\n",
				"        \n",
				"        # List all workspaces\n",
				"        workspaces_current = get_fabric_workspaces(fabric_session)        \n",
				"        \n",
				"        # Check if the displayName exists in the workspaces\n",
				"        matching_workspaces = [workspace_current for workspace_current in workspaces_current.get('value') if workspace_current['displayName'] == workspace['name']]\n",
				"        \n",
				"        if matching_workspaces:\n",
				"            print(f\" - Workspace '{workspace['name']}' found. Workspace ID: {matching_workspaces[0]['id']}\")\n",
				"            workspace['id'] = matching_workspaces[0]['id']\n",
				"        else:\n",
				"            print(f\" - Workspace '{workspace['name']}' not found. Creating new workspace...\")\n",
				"            workspace_created = fabric_request(fabric_session, F\"workspaces/\", 'POST', payload={\"displayName\": workspace['name']}, payloadtype='json')\n",
				"            workspace['id'] = workspace_created['id']\n",
				"            tasks.append({\"task_name\":f\"create item {workspace['name']} initially\", \"task_duration\": int(time() - start), \"status\": \"success\"})\n",
				"        \n",
				"        assign_fabric_workspace_capacity(fabric_session, workspace['id'], workspace['capacity_id'])\n",
				"        tasks.append({\"task_name\":f\"Workspace '{workspace['name']}' connected to capacity ID: {workspace['capacity_id']}\", \"task_duration\": int(time() - start), \"status\": \"success\"})\n",
				"        print(f\" - Workspace '{workspace['name']}' created with ID: {workspace['id']} and capacity_id: {workspace['capacity_id']}\")\n",
				"\n",
				"        # Check if roles exists or create them\n",
				"        print(f\" - Assiging Workspace roles\")\n",
				"        assign_fabric_workspace_roles(fabric_session, workspace['id'], workspace['roles'])\n",
				"\n",
				"    # Print the workspace IDs\n",
				"    print(f\"--------------------------\")\n",
				"    print(f\"Workspace ID for data workspace {environment['environment_name']}: {environment['workspaces']['data']['id']}\")\n",
				"    print(f\"Workspace ID for code workspace {environment['environment_name']}: {environment['workspaces']['code']['id']}\")\n"
			],
			"outputs": [],
			"execution_count": null,
			"metadata": {
				"microsoft": {
					"language": "python",
					"language_group": "synapse_pyspark"
				}
			},
			"id": "5d0a2af2-bfd3-4f3a-80e1-6f7e36bd23b3"
		},
		{
			"cell_type": "code",
			"source": [
				"# Read deployment manifest\n",
				"deployment_manifest = {}\n",
				"with open(f\"{notebookutils.fs.getMountPath('/default')}/Files/{deployment_file}\") as f:\n",
				"    deployment_manifest = json.load(f)"
			],
			"outputs": [],
			"execution_count": null,
			"metadata": {
				"microsoft": {
					"language": "python",
					"language_group": "synapse_pyspark"
				}
			},
			"id": "d2187040-28df-48bc-931c-f0612c64078d"
		},
		{
			"cell_type": "code",
			"source": [
				"# re-map databases\n",
				"\n",
				"for target_item in deployment_manifest['logging']['items']:\n",
				"    if target_item['type'] in ('Warehouse', 'SQLEndpoint'):\n",
				"        target_item['displayName'] = logging['WarehouseName']\n",
				"\n",
				"for target_item in deployment_manifest['configuration']['items']:\n",
				"    if target_item['type'] in ('SQLDatabase', 'SQLEndpoint'):\n",
				"        target_item['displayName'] = configuration['DatabaseName']"
			],
			"outputs": [],
			"execution_count": null,
			"metadata": {
				"microsoft": {
					"language": "python",
					"language_group": "synapse_pyspark"
				}
			},
			"id": "01a07a3d-9c14-4731-bd17-6022aaa9451c"
		},
		{
			"cell_type": "code",
			"source": [
				"def workspace_deployment(workspace_deployment_config, workspace_deployment_items, replace_collection, empty = True):\n",
				"    print(f\" --------------------------\")\n",
				"    print(f\" Processing code workspace {workspace_deployment_config['name']}\")\n",
				"    \n",
				"    target_items = get_fabric_items(fabric_session, workspace_deployment_config['id'])\n",
				"\n",
				"    for deployment_item in workspace_deployment_items:\n",
				"        for target_item in target_items['value']:\n",
				"            if target_item['displayName'] == deployment_item['displayName'] \\\n",
				"                    and target_item['type'] == deployment_item['type']:\n",
				"                print(f\" - Skip existing: {deployment_item['displayName']}, {deployment_item['type']}, {target_item['id']}\")\n",
				"                break\n",
				"        else:\n",
				"            if deployment_item['type'] == 'SQLDatabase':\n",
				"                raise Exception(f\"Please create manually the SQL database {deployment_item['displayName']} in the configuration workspace {configuration['workspace']['name']} and re-run this code.\")  \n",
				"                # skip because sqldatabase isn't supported yet\n",
				"            \n",
				"            print(f\" - Creating: {deployment_item['displayName']} {deployment_item['type']}\")\n",
				"            item = deployment_item.copy()\n",
				"            \n",
				"            if empty:\n",
				"                if item.get('definition'):\n",
				"                    print(f\" - Dropping definition\")\n",
				"                    item.pop('definition')\n",
				"\n",
				"            target_item = fabric_request(fabric_session, url=f\"workspaces/{workspace_deployment_config['id']}/items/\", method=\"POST\", payload=item, payloadtype='json')\n",
				"\n",
				"        deployment_item[\"new_id\"] = target_item['id']\n",
				"\n",
				"        if deployment_item['type'] in ('Warehouse', 'SQLDatabase'):\n",
				"            if deployment_item.get('org_endpoint', '') != '':\n",
				"                return_item = fabric_request(fabric_session, url=f\"workspaces/{workspace_deployment_config['id']}/{deployment_item['type']}s/{target_item['id']}\", method=\"GET\")\n",
				"                if deployment_item['type'] in ('Warehouse'):\n",
				"                    if return_item.get(\"properties\", {}).get(\"connectionString\", \"\") != '':\n",
				"                        deployment_item[\"connectionString\"] = return_item[\"properties\"][\"connectionString\"]\n",
				"                        replace_collection.append({\"old_id\": deployment_item[\"org_endpoint\"], \"new_id\": deployment_item[\"connectionString\"]})\n",
				"                \n",
				"                if deployment_item['type'] in ('SQLDatabase'):\n",
				"                    if return_item.get(\"properties\", {}).get(\"serverFqdn\", \"\") != '':\n",
				"                        deployment_item[\"connectionString\"] = return_item[\"properties\"][\"serverFqdn\"].replace(',1433', '')\n",
				"                        replace_collection.append({\"old_id\": deployment_item[\"org_endpoint\"], \"new_id\": deployment_item[\"connectionString\"]})\n",
				"                    if return_item.get(\"properties\", {}).get(\"databaseName\", \"\") != '':\n",
				"                        deployment_item[\"databaseName\"] = return_item[\"properties\"][\"databaseName\"]\n",
				"\n",
				"        replace_collection.append({\"old_id\": deployment_item[\"org_id\"], \"new_id\": deployment_item[\"new_id\"]})"
			],
			"outputs": [],
			"execution_count": null,
			"metadata": {
				"microsoft": {
					"language": "python",
					"language_group": "synapse_pyspark"
				}
			},
			"id": "b9687873-253c-41a5-978c-5a7e08b70a03"
		},
		{
			"cell_type": "code",
			"source": [
				"# create empty items if not exists\n",
				"\n",
				"for environment in environments:\n",
				"\n",
				"    print(f\"--------------------------\")\n",
				"    print(f\"Processing: {environment['environment_name']}\")\n",
				"\n",
				"    environment['guids_to_replace'] = []\n",
				"    workspace_deployment(environment['workspaces']['code'], deployment_manifest['items'], environment['guids_to_replace'], True)\n",
				"    environment['guids_to_replace'].append({\"old_id\": deployment_manifest[\"workspaces\"][\"workspace_code\"], \"new_id\": environment['workspaces']['code']['id']})\n",
				"    workspace_deployment(environment['workspaces']['data'], deployment_manifest['data'], environment['guids_to_replace'], True)\n",
				"    environment['guids_to_replace'].append({\"old_id\": deployment_manifest[\"workspaces\"][\"workspace_data\"], \"new_id\": environment['workspaces']['data']['id']})\n",
				"    workspace_deployment(logging['workspace'], deployment_manifest['logging']['items'], environment['guids_to_replace'], True)\n",
				"    environment['guids_to_replace'].append({\"old_id\": deployment_manifest[\"workspaces\"][\"workspace_logging\"], \"new_id\": logging['workspace']['id']})\n",
				"    workspace_deployment(configuration['workspace'], deployment_manifest['configuration']['items'], environment['guids_to_replace'], True)\n",
				"    environment['guids_to_replace'].append({\"old_id\": deployment_manifest[\"workspaces\"][\"workspace_config\"], \"new_id\": configuration['workspace']['id']})"
			],
			"outputs": [],
			"execution_count": null,
			"metadata": {
				"microsoft": {
					"language": "python",
					"language_group": "synapse_pyspark"
				}
			},
			"id": "2b4a984e-a9da-4adb-b2de-1dc9feb5172e"
		},
		{
			"cell_type": "code",
			"source": [
				"items_to_deploy = deployment_manifest[\"items\"]\n",
				"\n",
				"for environment in environments:\n",
				"    print(f\"--------------------------\")\n",
				"    print(f\"Processing: {environment['environment_name']}\")\n",
				"    \n",
				"    # Append the remaining pairs\n",
				"    environment['guids_to_replace'].append({\"old_id\": deployment_manifest[\"connections\"][\"CON_FMD_FABRIC_API\"], \"new_id\": environment['connections']['CON_FMD_FABRIC_API']})\n",
				"    environment['guids_to_replace'].append({\"old_id\": deployment_manifest[\"connections\"][\"CON_FMD_FABRIC_SQL\"], \"new_id\": environment['connections']['CON_FMD_FABRIC_SQL']})\n",
				"    environment['guids_to_replace'].append({\"old_id\": deployment_manifest[\"connections\"][\"CON_FMD_FABRIC_PIPELINES\"], \"new_id\": environment['connections']['CON_FMD_FABRIC_PIPELINES']})\n",
				"    environment['guids_to_replace'].append({\"old_id\": deployment_manifest[\"connections\"][\"CON_FMD_ASQL_01\"], \"new_id\": environment['connections']['CON_FMD_ASQL_01']})\n",
				"    environment['guids_to_replace'].append({\"old_id\": deployment_manifest[\"connections\"][\"CON_FMD_ASQL_02\"], \"new_id\": environment['connections']['CON_FMD_ASQL_02']})\n",
				"    environment['guids_to_replace'].append({\"old_id\": deployment_manifest[\"connections\"][\"CON_FMD_ADLS_01\"], \"new_id\": environment['connections']['CON_FMD_ADLS_01']})\n",
				"    environment['guids_to_replace'].append({\"old_id\": deployment_manifest[\"connections\"][\"CON_FMD_ADF_PIPELINES\"], \"new_id\": environment['connections']['CON_FMD_ADF_PIPELINES']})\n",
				"    \n",
				"    # Deploy items to workspace\n",
				"    existing_items = fabric_request(fabric_session, url=f\"workspaces/{environment['workspaces']['code']['id']}/items/\", method=\"GET\")\n",
				"    deploy_items(items_to_deploy, environment['guids_to_replace'], fmd_api_access_token, environment['workspaces']['code']['id'], existing_items[\"value\"])"
			],
			"outputs": [],
			"execution_count": null,
			"metadata": {
				"jupyter": {
					"source_hidden": false,
					"outputs_hidden": false
				},
				"nteract": {
					"transient": {
						"deleting": false
					}
				},
				"microsoft": {
					"language": "python",
					"language_group": "synapse_pyspark"
				}
			},
			"id": "2a8cf383-606a-4c88-a052-6972e040b898"
		},
		{
			"cell_type": "code",
			"source": [
				"driver = '{ODBC Driver 18 for SQL Server}'"
			],
			"outputs": [],
			"execution_count": null,
			"metadata": {
				"microsoft": {
					"language": "python",
					"language_group": "synapse_pyspark"
				}
			},
			"id": "13ce68b5-3a38-499e-8e48-cc635b19c1c4"
		},
		{
			"cell_type": "code",
			"source": [
				"# by timeout re-run\n",
				"\n",
				"for target_item in deployment_manifest['logging']['items']:\n",
				"    if target_item['type'] == 'Warehouse':\n",
				"        connstring = target_item[\"connectionString\"]\n",
				"        database = target_item['displayName']\n",
				"\n",
				"try:\n",
				"    i = 0\n",
				"    token = mssparkutils.credentials.getToken('https://analysis.windows.net/powerbi/api').encode(\"UTF-16-LE\")\n",
				"    token_struct = struct.pack(f'<I{len(token)}s', len(token), token)\n",
				"    print(f\"DRIVER={driver};SERVER={connstring};PORT=1433;DATABASE={database};\")\n",
				"    connection = pyodbc.connect(f\"DRIVER={driver};SERVER={connstring};PORT=1433;DATABASE={database};\", attrs_before={1256:token_struct}, timeout=12)\n",
				"\n",
				"    with connection.cursor() as cursor:\n",
				"        cursor.execute(\"SELECT 1\")  # Execute the warm-up query (a simple query like 'SELECT 1' can be used)\n",
				"        cursor.fetchone()\n",
				"        connection.timeout = 5  # Setting a lower timeout for subsequent queries\n",
				"        for i, query in enumerate(deployment_manifest[\"logging\"][\"queries\"]):\n",
				"            print(f' - execute \"{query}\"')\n",
				"            cursor.execute(query)\n",
				"            cursor.commit()\n",
				"    tasks.append({\"task_name\":f\"{workspace.get('displayName')} {database} query {i}\", \"task_duration\": 1, \"status\": f\"success\"})\n",
				"except pyodbc.OperationalError as e:\n",
				"    print(e) \n",
				"    tasks.append({\"task_name\":f\"{workspace.get('displayName')} {database} query {i}\", \"task_duration\": 1, \"status\": f\"pyodbc failed: {e}\"})\n",
				"except Exception as e:\n",
				"    print(e) \n",
				"    tasks.append({\"task_name\":f\"{workspace.get('displayName')} {database} query {i}\", \"task_duration\": 1, \"status\": f\"failed: {e}\"})"
			],
			"outputs": [],
			"execution_count": null,
			"metadata": {
				"microsoft": {
					"language": "python",
					"language_group": "synapse_pyspark"
				}
			},
			"id": "db772de2-d86a-41a7-b19a-e0ef9dc57fdb"
		},
		{
			"cell_type": "code",
			"source": [
				"deployment_manifest[\"configuration\"][\"queries\"].append(f'EXEC [integration].[sp_UpsertConnection] @ConnectionGuid = \"00000000-0000-0000-0000-000000000000\", @Name = \"CON_FMD_ONELAKE\", @Type = \"ONELAKE\", @IsActive = 1')\n",
				"deployment_manifest[\"configuration\"][\"queries\"].append(f'EXEC [integration].[sp_UpsertConnection] @ConnectionGuid = \"{(environments[0][\"connections\"][\"CON_FMD_ASQL_01\"])}\", @Name = \"CON_FMD_ASQL_01\", @Type = \"SQL\", @IsActive = 1')\n",
				"deployment_manifest[\"configuration\"][\"queries\"].append(f'EXEC [integration].[sp_UpsertConnection] @ConnectionGuid = \"{(environments[0][\"connections\"][\"CON_FMD_ASQL_02\"])}\", @Name = \"CON_FMD_ASQL_02\", @Type = \"SQL\", @IsActive = 1')\n",
				"deployment_manifest[\"configuration\"][\"queries\"].append(f'EXEC [integration].[sp_UpsertConnection] @ConnectionGuid = \"{(environments[0][\"connections\"][\"CON_FMD_ADLS_01\"])}\", @Name = \"CON_FMD_ADLS_01\", @Type = \"ADLS\", @IsActive = 1')\n",
				"deployment_manifest[\"configuration\"][\"queries\"].append(f'EXEC [integration].[sp_UpsertConnection] @ConnectionGuid = \"{(environments[0][\"connections\"][\"CON_FMD_ADF_PIPELINES\"])}\", @Name = \"CON_FMD_ADF_PIPELINES\", @Type = \"ADF\", @IsActive = 1')"
			],
			"outputs": [],
			"execution_count": null,
			"metadata": {
				"microsoft": {
					"language": "python",
					"language_group": "synapse_pyspark"
				}
			},
			"id": "471f7792-b72b-4aad-bc4c-1a8748facf52"
		},
		{
			"cell_type": "code",
			"source": [
				"deployment_manifest[\"configuration\"][\"queries\"].append(\"\"\"\n",
				"    DECLARE @DataSourceIdInternal INT = (SELECT DataSourceId FROM integration.DataSource WHERE Name = 'LH_DATA_LANDINGZONE')\n",
				"    DECLARE @ConnectionIdInternal INT = (SELECT ConnectionId FROM integration.Connection WHERE ConnectionGuid = '00000000-0000-0000-0000-000000000000')\n",
				"    EXECUTE [integration].[sp_UpsertDataSource] \n",
				"        @ConnectionId = @ConnectionIdInternal\n",
				"        ,@DataSourceId = @DataSourceIdInternal\n",
				"        ,@Name = 'LH_DATA_LANDINGZONE'\n",
				"        ,@Abbreviation = 'ONELAKE'\n",
				"        ,@Type = 'ONELAKE_TABLES_01'\n",
				"        ,@Description = 'ONELAKE_TABLES'\n",
				"        ,@IsActive = 1\n",
				"\"\"\")"
			],
			"outputs": [],
			"execution_count": null,
			"metadata": {
				"microsoft": {
					"language": "python",
					"language_group": "synapse_pyspark"
				}
			},
			"id": "1fab2dbf-1444-49c8-9006-539024d5e997"
		},
		{
			"cell_type": "code",
			"source": [
				"workspaces = []\n",
				"workspaces.append(logging['workspace'])\n",
				"workspaces.append(configuration['workspace'])\n",
				"\n",
				"for environment in environments:\n",
				"    workspaces.append(environment['workspaces']['code'])\n",
				"    workspaces.append(environment['workspaces']['data'])\n",
				"    \n",
				"for workspace in workspaces:\n",
				"    print(f'EXEC [integration].[sp_UpsertWorkspace](@WorkspaceId = \"{workspace[\"id\"]}\" ,@Name = \"{workspace[\"name\"]}\")')\n",
				"    deployment_manifest[\"configuration\"][\"queries\"].append(f'EXEC [integration].[sp_UpsertWorkspace] @WorkspaceId = \"{workspace[\"id\"]}\", @Name = \"{workspace[\"name\"]}\"')"
			],
			"outputs": [],
			"execution_count": null,
			"metadata": {
				"microsoft": {
					"language": "python",
					"language_group": "synapse_pyspark"
				}
			},
			"id": "0cb672c5-4a52-410d-b16f-2bfac3b49a80"
		},
		{
			"cell_type": "code",
			"source": [
				"for environment in environments:\n",
				"    existing_items = fabric_request(fabric_session, url=f\"workspaces/{environment['workspaces']['code']['id']}/items/\", method=\"GET\")\n",
				"    for item in existing_items.get('value', []):\n",
				"        if item['type'] == 'DataPipeline':\n",
				"            print(f'EXEC [integration].[sp_UpsertPipeline] @PipelineId = \"{item[\"id\"]}\", @WorkspaceId = \"{environment[\"workspaces\"][\"data\"][\"id\"]}\" ,@Name = \"{item[\"displayName\"]}\"')\n",
				"            deployment_manifest[\"configuration\"][\"queries\"].append(f'EXEC [integration].[sp_UpsertPipeline] @PipelineId = \"{item[\"id\"]}\", @WorkspaceId = \"{environment[\"workspaces\"][\"data\"][\"id\"]}\" ,@Name = \"{item[\"displayName\"]}\"')"
			],
			"outputs": [],
			"execution_count": null,
			"metadata": {
				"microsoft": {
					"language": "python",
					"language_group": "synapse_pyspark"
				}
			},
			"id": "3c90ef67-295b-4a98-a273-28f8873af045"
		},
		{
			"cell_type": "code",
			"source": [
				"for environment in environments:\n",
				"    existing_items = fabric_request(fabric_session, url=f\"workspaces/{environment['workspaces']['data']['id']}/items/\", method=\"GET\")\n",
				"    for item in existing_items.get('value', []):\n",
				"        if item['type'] == 'Lakehouse':\n",
				"            print(f'EXEC [integration].[sp_UpsertLakehouse] @LakehouseId = \"{item[\"id\"]}\", @WorkspaceId = \"{environment[\"workspaces\"][\"data\"][\"id\"]}\" ,@Name = \"{item[\"displayName\"]}\"')\n",
				"            deployment_manifest[\"configuration\"][\"queries\"].append(f'EXEC [integration].[sp_UpsertLakehouse] @LakehouseId = \"{item[\"id\"]}\", @WorkspaceId = \"{environment[\"workspaces\"][\"data\"][\"id\"]}\" ,@Name = \"{item[\"displayName\"]}\"')"
			],
			"outputs": [],
			"execution_count": null,
			"metadata": {
				"microsoft": {
					"language": "python",
					"language_group": "synapse_pyspark"
				}
			},
			"id": "932a14c4-9d6f-4566-9d8f-d567ea27b809"
		},
		{
			"cell_type": "code",
			"source": [
				"deployment_manifest[\"configuration\"][\"queries\"].append(\"\"\"\n",
				"    DECLARE @LandingzoneEntityIdInternal INT = (SELECT LandingzoneEntityId FROM integration.LandingzoneEntity WHERE SourceSchema = 'in' and SourceName = 'customer')\n",
				"    DECLARE @DataSourceIdInternal INT = (SELECT DataSourceId FROM integration.DataSource WHERE Name = 'LH_DATA_LANDINGZONE')\n",
				"    DECLARE @LakehouseIdInternal INT = (SELECT top 1 LakehouseId FROM integration.Lakehouse WHERE Name = 'LH_LANDINGZONE')\n",
				"    EXECUTE [integration].[sp_UpsertLandingzoneEntity] \n",
				"        @LandingzoneEntityId = @LandingzoneEntityIdInternal\n",
				"        ,@DataSourceId = @DataSourceIdInternal\n",
				"        ,@LakehouseId = @LakehouseIdInternal\n",
				"        ,@SourceSchema = 'in'\n",
				"        ,@SourceName = 'customer'\n",
				"        ,@FileName = 'customer'\n",
				"        ,@FilePath = 'in'\n",
				"        ,@FileType = 'parquet'\n",
				"        ,@IsIncremental = 0\n",
				"        ,@IsIncrementalColumn = ''\n",
				"        ,@IsActive = 1\n",
				"\"\"\")"
			],
			"outputs": [],
			"execution_count": null,
			"metadata": {
				"microsoft": {
					"language": "python",
					"language_group": "synapse_pyspark"
				}
			},
			"id": "7ebbd0e8-b82b-455d-93ab-54f99a02d699"
		},
		{
			"cell_type": "code",
			"source": [
				"deployment_manifest[\"configuration\"][\"queries\"].append(\"\"\"\n",
				"    DECLARE @LandingzoneEntityIdInternal INT = (SELECT LandingzoneEntityId FROM integration.LandingzoneEntity WHERE SourceSchema = 'in' and SourceName = 'customer')\n",
				"    DECLARE @BronzeLayerEntityIdInternal INT = (SELECT BronzeLayerEntityId FROM integration.BronzeLayerEntity WHERE [Schema] = 'in' and [Name] = 'customer')\n",
				"    DECLARE @LakehouseIdInternal INT = (SELECT top 1 LakehouseId FROM integration.Lakehouse WHERE Name = 'LH_BRONZE_LAYER')\n",
				"    EXECUTE [integration].[sp_UpsertBronzeLayerEntity] \n",
				"        @BronzeLayerEntityId = @BronzeLayerEntityIdInternal\n",
				"        ,@LandingzoneEntityId = @LandingzoneEntityIdInternal\n",
				"        ,@Schema = 'in'\n",
				"        ,@Name = 'customer'\n",
				"        ,@FileType = 'Delta'\n",
				"        ,@LakehouseId = @LakehouseIdInternal\n",
				"        ,@PrimaryKeys = 'CustomerId'\n",
				"        ,@IsActive = 1\n",
				"\"\"\")"
			],
			"outputs": [],
			"execution_count": null,
			"metadata": {
				"microsoft": {
					"language": "python",
					"language_group": "synapse_pyspark"
				}
			},
			"id": "480adf45-7385-47fa-b669-a1f394252d38"
		},
		{
			"cell_type": "code",
			"source": [
				"deployment_manifest[\"configuration\"][\"queries\"].append(\"\"\"\n",
				"    DECLARE @BronzeLayerEntityIdInternal INT = (SELECT BronzeLayerEntityId FROM integration.BronzeLayerEntity WHERE [Schema] = 'in' and [Name] = 'customer')\n",
				"    DECLARE @SilverLayerEntityIdInternal INT = (SELECT SilverLayerEntityId FROM integration.SilverLayerEntity WHERE [Schema] = 'in' and [Name] = 'customer')\n",
				"    DECLARE @LakehouseIdInternal INT = (SELECT top 1 LakehouseId FROM integration.Lakehouse WHERE Name = 'LH_SILVER_LAYER')\n",
				"    EXECUTE [integration].[sp_UpsertSilverLayerEntity] \n",
				"        @SilverLayerEntityId = @SilverLayerEntityIdInternal\n",
				"        ,@BronzeLayerEntityId = @BronzeLayerEntityIdInternal\n",
				"        ,@LakehouseId = @LakehouseIdInternal\n",
				"        ,@Name = 'customer'\n",
				"        ,@Schema = 'in'\n",
				"        ,@FileType = 'delta'\n",
				"        ,@IsActive = 1\n",
				"\"\"\")"
			],
			"outputs": [],
			"execution_count": null,
			"metadata": {
				"microsoft": {
					"language": "python",
					"language_group": "synapse_pyspark"
				}
			},
			"id": "ba6c1c95-806d-4f37-bddb-344c9fe58d32"
		},
		{
			"cell_type": "code",
			"source": [
				"for target_item in deployment_manifest['configuration']['items']:\n",
				"    if target_item['type'] == 'SQLDatabase':\n",
				"        connstring = target_item[\"connectionString\"]\n",
				"        database = target_item['databaseName']\n",
				"\n",
				"try:\n",
				"    i = 0\n",
				"    token = mssparkutils.credentials.getToken('https://analysis.windows.net/powerbi/api').encode(\"UTF-16-LE\")\n",
				"    token_struct = struct.pack(f'<I{len(token)}s', len(token), token)\n",
				"    print(f\"DRIVER={driver};SERVER={connstring};PORT=1433;DATABASE={database};\")\n",
				"    connection = pyodbc.connect(f\"DRIVER={driver};SERVER={connstring};PORT=1433;DATABASE={database};\", attrs_before={1256:token_struct}, timeout=12)\n",
				"\n",
				"    with connection.cursor() as cursor:\n",
				"        cursor.execute(\"SELECT 1\")  # Execute the warm-up query (a simple query like 'SELECT 1' can be used)\n",
				"        cursor.fetchone()\n",
				"        connection.timeout = 5  # Setting a lower timeout for subsequent queries\n",
				"        for i, query in enumerate(deployment_manifest[\"configuration\"][\"queries\"]):\n",
				"            print(f' - execute \"{query}\"')\n",
				"            cursor.execute(query)\n",
				"            cursor.commit()\n",
				"    tasks.append({\"task_name\":f\"{workspace.get('displayName')} {database} query {i}\", \"task_duration\": 1, \"status\": f\"success\"})\n",
				"except pyodbc.OperationalError as e:\n",
				"    print(e) \n",
				"    tasks.append({\"task_name\":f\"{workspace.get('displayName')} {database} query {i}\", \"task_duration\": 1, \"status\": f\"pyodbc failed: {e}\"})\n",
				"except Exception as e:\n",
				"    print(e) \n",
				"    tasks.append({\"task_name\":f\"{workspace.get('displayName')} {database} query {i}\", \"task_duration\": 1, \"status\": f\"failed: {e}\"})"
			],
			"outputs": [],
			"execution_count": null,
			"metadata": {
				"microsoft": {
					"language": "python",
					"language_group": "synapse_pyspark"
				}
			},
			"id": "657493f0-7d03-4fd3-81bc-ca2e7be3ed24"
		},
		{
			"cell_type": "code",
			"source": [
				"display(tasks)"
			],
			"outputs": [],
			"execution_count": null,
			"metadata": {
				"microsoft": {
					"language": "python",
					"language_group": "synapse_pyspark"
				},
				"collapsed": false
			},
			"id": "b3b7c873-295f-4e1d-ba84-b6689800f4c5"
		}
	],
	"metadata": {
		"language_info": {
			"name": "python"
		},
		"kernel_info": {
			"name": "synapse_pyspark"
		},
		"a365ComputeOptions": null,
		"sessionKeepAliveTimeout": 0,
		"microsoft": {
			"language": "python",
			"language_group": "synapse_pyspark",
			"ms_spell_check": {
				"ms_spell_check_language": "en"
			}
		},
		"nteract": {
			"version": "nteract-front-end@1.0.0"
		},
		"kernelspec": {
			"name": "synapse_pyspark",
			"language": "Python",
			"display_name": "Synapse PySpark"
		},
		"synapse_widget": {
			"version": "0.1",
			"state": {}
		},
		"spark_compute": {
			"compute_id": "/trident/default",
			"session_options": {
				"conf": {
					"spark.synapse.nbs.session.timeout": "1200000"
				}
			}
		},
		"dependencies": {
			"lakehouse": {}
		}
	},
	"nbformat": 4,
	"nbformat_minor": 5
}
