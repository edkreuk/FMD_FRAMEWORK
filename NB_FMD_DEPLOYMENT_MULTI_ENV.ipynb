{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3c6b110-b18e-4f89-9a8c-29e8c30bf171",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "\n",
    "\n",
    "   \"![FMD_Overview](https://github.com/edkreuk/FMD_FRAMEWORK/blob/main/Images/FMD_Overview.png?raw=true)\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5ff914-bf2c-47a1-90fa-489b16e653c0",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Create Lakehouse if not exists and Attach Lakehouse "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e36a04-9423-4d9d-84c9-ab4ac0edf4dd",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    notebookutils.lakehouse.create(name = \"LH_FMD_CONFIGURATION\")\n",
    "except Exception as ex:\n",
    "    print('Lakehouse already exists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4b1a66-8c9b-4be7-a7fa-cb5ba25d9fb6",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "%%configure -f\n",
    "{ \n",
    "    \"defaultLakehouse\": { \n",
    "        \"name\":  \"LH_FMD_CONFIGURATION\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9e8901-2c05-47cd-bf88-2c67b1d92189",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1954c1d5-24fb-40dd-8784-7faf4ab0e12d",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "deploy_from_github = True\n",
    "driver = '{ODBC Driver 18 for SQL Server}'\n",
    "# target connections guid \n",
    "# add the correct id for every connections\n",
    "# change the capacity id\n",
    "# change the workspace roles\n",
    "\n",
    "FrameworkName= 'DEMO' # max 6 characters for better visibility, no spaces and the end of the name\n",
    "\n",
    "capacity_id_dvlm = '098FA313-D63F-4F04-897E-3F25DCBC838C'       # Which capacity will be used for these workspaces in development\n",
    "capacity_id_prod = '098FA313-D63F-4F04-897E-3F25DCBC838C'       # Which capacity will be used for these workspaces in production\n",
    "capacity_id_config = 'CF23FF9B-FAD4-4C6A-B6C6-CFCC362CF93D'     #'Which capacity will be used for this workspace for the FMD Database\n",
    "\n",
    "\n",
    "workspace_roles = [ # Keep emtpy [] if you only want to assign this to your personal account\n",
    "                    {\n",
    "                        \"principal\": {\n",
    "                            \"id\": \"5c906b5c-d1d7-4984-b047-adacd8d795fe\",\n",
    "                            \"displayName\": \"sg-fmd-fabric-contributor\",\n",
    "                            \"type\": \"Group\"\n",
    "                        },\n",
    "                        \"role\": \"Contributor\"  # Admin, Member, Contributor, Viewer\n",
    "                        }\n",
    "                    ]\n",
    "\n",
    "\n",
    "configuration = {\n",
    "                    'workspace': {\n",
    "                        'name' : FrameworkName + ' CONFIG FMD', # Name of target workspace\n",
    "                        'roles' : workspace_roles,\n",
    "                        'capacity_id' : capacity_id_config\n",
    "                    },\n",
    "                       'DatabaseName' : 'SQL_'+FrameworkName+'_FRAMEWORK' # Name of target configuration SQL Database\n",
    "}\n",
    "environments = [\n",
    "                    {\n",
    "                        'environment_name' : 'development', # Name of target environment\n",
    "                        'workspaces': {\n",
    "                            'data' : {\n",
    "                                'name' : FrameworkName + ' DATA (D) FMD', # Name of target code workspace for development\n",
    "                                'roles' : workspace_roles,\n",
    "                                'capacity_id' : capacity_id_dvlm\n",
    "                            },\n",
    "                            'code' : {\n",
    "                                'name' : FrameworkName + ' CODE (D) FMD', # Name of target data workspace for development\n",
    "                                'roles' : workspace_roles,\n",
    "                                'capacity_id' : capacity_id_dvlm\n",
    "                            }\n",
    "                        },\n",
    "                        'connections' : {\n",
    "                            'CON_FMD_FABRIC_SQL' : '372237f9-709a-48f8-8fb2-ce06940c990e', # Required Guid to the Fabric SQL connection\n",
    "                            'CON_FMD_FABRIC_PIPELINES' : '6d8146c6-a438-47df-94e2-540c552eb6d7', # Required Guid to the Fabric datapipelines connection\n",
    "                            'CON_FMD_ADF_PIPELINES' : '02e107b8-e97e-4b00-a28c-668cf9ce3d9a' # Optional Guid to an Azure Datafactory connection\n",
    "                        }\n",
    "                    },\n",
    "                    {\n",
    "                        'environment_name' : 'production', # Name of target environment\n",
    "                        'workspaces': {\n",
    "                            'data' : {\n",
    "                                'name' : FrameworkName + ' DATA (P) FMD',\n",
    "                                'roles' : workspace_roles,\n",
    "                                'capacity_id' : capacity_id_prod\n",
    "                            },\n",
    "                            'code' : {\n",
    "                                'name' : FrameworkName + ' CODE (P) FMD',\n",
    "                                'roles' : workspace_roles,\n",
    "                                'capacity_id' : capacity_id_prod\n",
    "                            }\n",
    "                        },\n",
    "                        'connections' : {\n",
    "                            'CON_FMD_FABRIC_SQL' : '372237f9-709a-48f8-8fb2-ce06940c990e',  # Required Guid to the Fabric SQL connection\n",
    "                            'CON_FMD_FABRIC_PIPELINES' : '6d8146c6-a438-47df-94e2-540c552eb6d7',  # Required Guid to the Fabric datapipelines connection\n",
    "                            'CON_FMD_ADF_PIPELINES' : '02e107b8-e97e-4b00-a28c-668cf9ce3d9a'\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf68a409-68e4-4a24-9355-f41876ef24e2",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f62905-b62e-47cf-9fad-cdcf7c49d8a2",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "from json import loads, dumps\n",
    "import json\n",
    "import requests\n",
    "import base64\n",
    "import time\n",
    "import uuid\n",
    "import struct\n",
    "import pyodbc\n",
    "\n",
    "from typing import Callable, List, Dict, Optional, Any\n",
    "from datetime import datetime\n",
    "from time import sleep, time\n",
    "from dataclasses import dataclass, field"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f941e470-f4eb-4291-ad87-af33e80d963e",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Download Configuration File from Github or upload the file manually \n",
    "\n",
    "**Manually**: upload to Files/deployment/FMD_deployment.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32b87d2-d650-4535-b911-e6a44656f2d6",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Open deployment json file\n",
    "deployment_manifest = {}\n",
    "if deploy_from_github:\n",
    "    print(\"Downloading from Github to FMD_FRAMEWORK\")\n",
    "    url = 'https://raw.githubusercontent.com/edkreuk/FMD_FRAMEWORK/main/FMD_deployment.json'\n",
    "    github_download = requests.get(url)\n",
    "    folder_path = notebookutils.fs.getMountPath('/default') + \"/Files/deployment/\"\n",
    "    notebookutils.fs.mkdirs(f\"file://\" +folder_path)\n",
    "    with open(folder_path + \"FMD_deployment.json\", \"w\") as f:\n",
    "        f.write(json.dumps(github_download.json()))\n",
    "    \n",
    "\n",
    "    print(\"Read from FMD_FRAMEWORK Github\")\n",
    "\n",
    "\n",
    "# Read deployment manifest\n",
    "\n",
    "with open(f\"{notebookutils.fs.getMountPath('/default')}/Files/deployment/FMD_deployment.json\") as f:\n",
    "    deployment_manifest = json.load(f)\n",
    "    print(\"Read from FMD_FRAMEWORK LOCAL\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3a6a94-f6cf-47a7-861e-899c37fbaaa1",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Load deployment functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d77bb62-d0e3-4601-8742-f37f7cc2195f",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "%run NB_FMD_DEPLOYMENT_UTILS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31372683-48cc-427e-abea-8defcfaf8b27",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "tasks=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2489acf2",
   "metadata": {},
   "source": [
    "Create token and session to run notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f177a49-b384-4d07-9895-423060188b64",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "fmd_api_access_token =  notebookutils.credentials.getToken('https://analysis.windows.net/powerbi/api')\n",
    "fabric_session = create_fabric_session(fabric_token = fmd_api_access_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15902111-9dbf-4c18-bb00-496f36e98f82",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# check if token is valid\n",
    "for token in [fmd_api_access_token]:\n",
    "    if not token:\n",
    "        continue\n",
    "    header, payload, signature = token.split('.')\n",
    "    payload += '=' * (-len(payload) % 4)  # Add padding\n",
    "    token_dict = loads(base64.urlsafe_b64decode(payload))\n",
    "    directory_id = token_dict.get(\"tid\")\n",
    "    timest = token_dict.get(\"exp\")\n",
    "    expiry = (datetime.fromtimestamp(timest) - datetime.now()).total_seconds() // 60\n",
    "    expiry_str = str(expiry) if expiry < 5 else str(expiry)\n",
    "    print(F\"token {directory_id} will expire in {expiry_str} minutes at\\t{datetime.fromtimestamp(timest)} UTC\")\n",
    "print(F\"Current time:\\t\\t\\t\\t\\t\\t\\t\\t\\t{datetime.now().replace(microsecond=0)} UTC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d6b18c-24af-4aea-91e4-eda1ca69dadd",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Start Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0a2af2-bfd3-4f3a-80e1-6f7e36bd23b3",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Create necessary workspaces \n",
    "start = time()\n",
    "\n",
    "for environment in environments:\n",
    "\n",
    "    print(f\"--------------------------\")\n",
    "    print(f\"Processing: {environment['environment_name']}\")\n",
    "    \n",
    "    # Loop through the workspace names and get their IDs\n",
    "    \n",
    "    for workspace in [environment['workspaces']['data'], environment['workspaces']['code'], configuration['workspace']]:\n",
    "        \n",
    "        print(f\" -----\")\n",
    "        print(f\" - Processing: data workspace {environment['environment_name']}\")\n",
    "        \n",
    "        # List all workspaces\n",
    "        workspaces_current = get_fabric_workspaces(fabric_session)  \n",
    "   \n",
    "        \n",
    "        # Check if the displayName exists in the workspaces\n",
    "        matching_workspaces = [workspace_current for workspace_current in workspaces_current.get('value') if workspace_current['displayName'] == workspace['name']]\n",
    "        \n",
    "        if matching_workspaces:\n",
    "            print(f\" - Workspace '{workspace['name']}' found. Workspace ID: {matching_workspaces[0]['id']}\")\n",
    "            workspace['id'] = matching_workspaces[0]['id']\n",
    "        else:\n",
    "            print(f\" - Workspace '{workspace['name']}' not found. Creating new workspace...\")\n",
    "            workspace_created = fabric_request(fabric_session, F\"workspaces/\", 'POST', payload={\"displayName\": workspace['name']}, payloadtype='json')\n",
    "            workspace['id'] = workspace_created['id']\n",
    "            tasks.append({\"task_name\":f\"create item {workspace['name']} initially\", \"task_duration\": int(time() - start), \"status\": \"success\"})\n",
    "        \n",
    "        assign_fabric_workspace_capacity(fabric_session, workspace['id'], workspace['capacity_id'])\n",
    "        tasks.append({\"task_name\":f\"Workspace '{workspace['name']}' connected to capacity ID: {workspace['capacity_id']}\", \"task_duration\": int(time() - start), \"status\": \"success\"})\n",
    "\n",
    "        assign_fabric_workspace_identity(fabric_session, workspace['id'])\n",
    "\n",
    "\n",
    "\n",
    "        # Check if roles exists or create them\n",
    "        print(f\" - Assiging Workspace roles\")\n",
    "        assign_fabric_workspace_roles(fabric_session, workspace['id'], workspace['roles'])\n",
    "\n",
    "    # Print the workspace IDs\n",
    "    print(f\"--------------------------\")\n",
    "    print(f\"Workspace ID for data workspace {environment['environment_name']}: {environment['workspaces']['data']['id']}\")\n",
    "    print(f\"Workspace ID for code workspace {environment['environment_name']}: {environment['workspaces']['code']['id']}\")\n",
    "    print(f\"Workspace ID for config workspace: {configuration['workspace']['id']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a07a3d-9c14-4731-bd17-6022aaa9451c",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# re-map databases\n",
    "\n",
    "for target_item in deployment_manifest['configuration']['items']:\n",
    "    if target_item['type'] in ('SQLDatabase','SQLEndpoint'):\n",
    "        target_item['displayName'] = configuration['DatabaseName']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c6fff5-580d-4520-b57c-1447be51f6ba",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "def create_fabric_database(configuration, deployment_manifest, environment):\n",
    "    try:\n",
    "        workspace_deployment(configuration['workspace'], deployment_manifest['configuration']['items'], environment['guids_to_replace'], True)\n",
    "        environment['guids_to_replace'].append({\"old_id\": deployment_manifest[\"workspaces\"][\"workspace_config\"], \"new_id\": configuration['workspace']['id']})\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}. Retrying in 60 seconds...\")\n",
    "        sleep(60)\n",
    "        create_fabric_database(configuration, deployment_manifest, environment)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4a984e-a9da-4adb-b2de-1dc9feb5172e",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# create empty items if not exists\n",
    "    \n",
    "for environment in environments:\n",
    "\n",
    "    print(f\"--------------------------\")\n",
    "    print(f\"Processing: {environment['environment_name']}\")\n",
    "    environment['guids_to_replace'] = []\n",
    "    workspace_deployment(environment['workspaces']['code'], deployment_manifest['items'], environment['guids_to_replace'], True)\n",
    "    environment['guids_to_replace'].append({\"old_id\": deployment_manifest[\"workspaces\"][\"workspace_code\"], \"new_id\": environment['workspaces']['code']['id']})\n",
    "    workspace_deployment(environment['workspaces']['data'], deployment_manifest['data'], environment['guids_to_replace'], True)\n",
    "    environment['guids_to_replace'].append({\"old_id\": deployment_manifest[\"workspaces\"][\"workspace_data\"], \"new_id\": environment['workspaces']['data']['id']})\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b21c2b-e80d-4745-a946-4903fe04aa06",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "#Create Fabric SQL Database\n",
    "create_fabric_database(configuration, deployment_manifest, environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8cf383-606a-4c88-a052-6972e040b898",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "items_to_deploy = deployment_manifest[\"items\"]\n",
    "\n",
    "for environment in environments:\n",
    "    print(f\"--------------------------\")\n",
    "    print(f\"Processing: {environment['environment_name']}\")\n",
    "    # Deploy items to workspace\n",
    "    # Append the remaining pairs\n",
    "    environment['guids_to_replace'].append({\"old_id\": deployment_manifest[\"connections\"][\"CON_FMD_FABRIC_SQL\"], \"new_id\": environment['connections']['CON_FMD_FABRIC_SQL']})\n",
    "    environment['guids_to_replace'].append({\"old_id\": deployment_manifest[\"connections\"][\"CON_FMD_FABRIC_PIPELINES\"], \"new_id\": environment['connections']['CON_FMD_FABRIC_PIPELINES']})\n",
    "    environment['guids_to_replace'].append({\"old_id\": deployment_manifest[\"connections\"][\"CON_FMD_ADF_PIPELINES\"], \"new_id\": environment['connections']['CON_FMD_ADF_PIPELINES']})\n",
    "    environment['guids_to_replace'].append({\"old_id\": \"SQL_FMD_FRAMEWORK\", \"new_id\":  configuration['DatabaseName']})\n",
    "    \n",
    "    existing_items = fabric_request(fabric_session, url=f\"workspaces/{environment['workspaces']['code']['id']}/items/\", method=\"GET\")\n",
    "    deploy_items(items_to_deploy, environment['guids_to_replace'], fmd_api_access_token, environment['workspaces']['code']['id'], existing_items[\"value\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfdd0e6-4952-4222-a707-4d7dbb56cb71",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "#print(environment['guids_to_replace'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe144264-ab2b-416b-8da1-28173e25b3dc",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "#Get all the connections and add them to Manifest to automatically insert them in Framework\n",
    "#Make sure all connections are starting with CON_FMD to filter out\n",
    "connections = get_fabric_connections(fabric_session)\n",
    "\n",
    "for connection in connections['value']:\n",
    "    display_name = connection.get('displayName', '')\n",
    "    if display_name and display_name.startswith('CON_FMD'):\n",
    "        connection_type = connection.get('connectionDetails', {}).get('type', 'Unknown')\n",
    "        connection_id = connection.get('id')\n",
    "\n",
    "        exec_statement = (\n",
    "            f\"EXEC [integration].[sp_UpsertConnection] \"\n",
    "            f\"@ConnectionGuid = \\\"{connection_id}\\\", \"\n",
    "            f\"@Name = \\\"{display_name}\\\", \"\n",
    "            f\"@Type = \\\"{connection_type}\\\", \"\n",
    "            f\"@IsActive = 1\"\n",
    "        )\n",
    "\n",
    "        deployment_manifest[\"configuration\"][\"queries_stored_procedures\"].append(exec_statement)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471f7792-b72b-4aad-bc4c-1a8748facf52",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "deployment_manifest[\"configuration\"][\"queries_stored_procedures\"].append(f'EXEC [integration].[sp_UpsertConnection] @ConnectionGuid = \"00000000-0000-0000-0000-000000000000\", @Name = \"CON_FMD_ONELAKE\", @Type = \"ONELAKE\", @IsActive = 1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fab2dbf-1444-49c8-9006-539024d5e997",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "deployment_manifest[\"configuration\"][\"queries_stored_procedures\"].append(\"\"\"\n",
    "    DECLARE @DataSourceIdInternal INT = (SELECT DataSourceId FROM integration.DataSource WHERE Name = 'LH_DATA_LANDINGZONE' and Type='ONELAKE_TABLES_01')\n",
    "    DECLARE @ConnectionIdInternal INT = (SELECT ConnectionId FROM integration.Connection WHERE ConnectionGuid = '00000000-0000-0000-0000-000000000000')\n",
    "    EXECUTE [integration].[sp_UpsertDataSource] \n",
    "        @ConnectionId = @ConnectionIdInternal\n",
    "        ,@DataSourceId = @DataSourceIdInternal\n",
    "        ,@Name = 'LH_DATA_LANDINGZONE'\n",
    "        ,@Namespace = 'ONELAKE'\n",
    "        ,@Type = 'ONELAKE_TABLES_01'\n",
    "        ,@Description = 'ONELAKE_TABLES'\n",
    "        ,@IsActive = 1\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16190084-96d3-4e94-9374-c5eaa9d50c86",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "deployment_manifest[\"configuration\"][\"queries_stored_procedures\"].append(\"\"\"\n",
    "    DECLARE @DataSourceIdInternal INT = (SELECT DataSourceId FROM integration.DataSource WHERE Name = 'LH_DATA_LANDINGZONE' and Type ='ONELAKE_FILES_01')\n",
    "    DECLARE @ConnectionIdInternal INT = (SELECT ConnectionId FROM integration.Connection WHERE ConnectionGuid = '00000000-0000-0000-0000-000000000000')\n",
    "    EXECUTE [integration].[sp_UpsertDataSource] \n",
    "        @ConnectionId = @ConnectionIdInternal\n",
    "        ,@DataSourceId = @DataSourceIdInternal\n",
    "        ,@Name = 'LH_DATA_LANDINGZONE'\n",
    "        ,@Namespace = 'ONELAKE'\n",
    "        ,@Type = 'ONELAKE_FILES_01'\n",
    "        ,@Description = 'ONELAKE_FILES'\n",
    "        ,@IsActive = 1\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb672c5-4a52-410d-b16f-2bfac3b49a80",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "workspaces = []\n",
    "workspaces.append(configuration['workspace'])\n",
    "\n",
    "for environment in environments:\n",
    "    workspaces.append(environment['workspaces']['code'])\n",
    "    workspaces.append(environment['workspaces']['data'])\n",
    "    \n",
    "for workspace in workspaces:\n",
    "    print(f'EXEC [integration].[sp_UpsertWorkspace](@WorkspaceId = \"{workspace[\"id\"]}\" ,@Name = \"{workspace[\"name\"]}\")')\n",
    "    deployment_manifest[\"configuration\"][\"queries_stored_procedures\"].append(f'EXEC [integration].[sp_UpsertWorkspace] @WorkspaceId = \"{workspace[\"id\"]}\", @Name = \"{workspace[\"name\"]}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c90ef67-295b-4a98-a273-28f8873af045",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "for environment in environments:\n",
    "    existing_items = fabric_request(fabric_session, url=f\"workspaces/{environment['workspaces']['code']['id']}/items/\", method=\"GET\")\n",
    "    for item in existing_items.get('value', []):\n",
    "        if item['type'] == 'DataPipeline':\n",
    "            print(f'EXEC [integration].[sp_UpsertPipeline] @PipelineId = \"{item[\"id\"]}\", @WorkspaceId = \"{environment[\"workspaces\"][\"data\"][\"id\"]}\" ,@Name = \"{item[\"displayName\"]}\"')\n",
    "            deployment_manifest[\"configuration\"][\"queries_stored_procedures\"].append(f'EXEC [integration].[sp_UpsertPipeline] @PipelineId = \"{item[\"id\"]}\", @WorkspaceId = \"{environment[\"workspaces\"][\"data\"][\"id\"]}\" ,@Name = \"{item[\"displayName\"]}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932a14c4-9d6f-4566-9d8f-d567ea27b809",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "for environment in environments:\n",
    "    existing_items = fabric_request(fabric_session, url=f\"workspaces/{environment['workspaces']['data']['id']}/items/\", method=\"GET\")\n",
    "    for item in existing_items.get('value', []):\n",
    "        if item['type'] == 'Lakehouse':\n",
    "            print(f'EXEC [integration].[sp_UpsertLakehouse] @LakehouseId = \"{item[\"id\"]}\", @WorkspaceId = \"{environment[\"workspaces\"][\"data\"][\"id\"]}\" ,@Name = \"{item[\"displayName\"]}\"')\n",
    "            deployment_manifest[\"configuration\"][\"queries_stored_procedures\"].append(f'EXEC [integration].[sp_UpsertLakehouse] @LakehouseId = \"{item[\"id\"]}\", @WorkspaceId = \"{environment[\"workspaces\"][\"data\"][\"id\"]}\" ,@Name = \"{item[\"displayName\"]}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebbd0e8-b82b-455d-93ab-54f99a02d699",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "deployment_manifest[\"configuration\"][\"queries_stored_procedures\"].append(\"\"\"\n",
    "    DECLARE @LandingzoneEntityIdInternal INT = (SELECT LandingzoneEntityId FROM integration.LandingzoneEntity WHERE SourceSchema = 'in' and SourceName = 'customer')\n",
    "    DECLARE @DataSourceIdInternal INT = (SELECT DataSourceId FROM integration.DataSource WHERE Name = 'LH_DATA_LANDINGZONE' and Type='ONELAKE_TABLES_01')\n",
    "    DECLARE @LakehouseIdInternal INT = (SELECT top 1 LakehouseId FROM integration.Lakehouse WHERE Name = 'LH_DATA_LANDINGZONE')\n",
    "    EXECUTE [integration].[sp_UpsertLandingzoneEntity] \n",
    "        @LandingzoneEntityId = @LandingzoneEntityIdInternal\n",
    "        ,@DataSourceId = @DataSourceIdInternal\n",
    "        ,@LakehouseId = @LakehouseIdInternal\n",
    "        ,@SourceSchema = 'in'\n",
    "        ,@SourceName = 'customer'\n",
    "        ,@SourceCustomSelect = ''\n",
    "        ,@FileName = 'customer'\n",
    "        ,@FilePath = 'fmd'\n",
    "        ,@FileType = 'parquet'\n",
    "        ,@IsIncremental = 0\n",
    "        ,@IsIncrementalColumn = ''\n",
    "        ,@IsActive = 1\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480adf45-7385-47fa-b669-a1f394252d38",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "deployment_manifest[\"configuration\"][\"queries_stored_procedures\"].append(\"\"\"\n",
    "    DECLARE @LandingzoneEntityIdInternal INT = (SELECT LandingzoneEntityId FROM integration.LandingzoneEntity WHERE SourceSchema = 'in' and SourceName = 'customer')\n",
    "    DECLARE @BronzeLayerEntityIdInternal INT = (SELECT BronzeLayerEntityId FROM integration.BronzeLayerEntity WHERE [Schema] = 'in' and [Name] = 'customer')\n",
    "    DECLARE @LakehouseIdInternal INT = (SELECT top 1 LakehouseId FROM integration.Lakehouse WHERE Name = 'LH_BRONZE_LAYER')\n",
    "    EXECUTE [integration].[sp_UpsertBronzeLayerEntity] \n",
    "        @BronzeLayerEntityId = @BronzeLayerEntityIdInternal\n",
    "        ,@LandingzoneEntityId = @LandingzoneEntityIdInternal\n",
    "        ,@Schema = 'in'\n",
    "        ,@Name = 'customer'\n",
    "        ,@FileType = 'Delta'\n",
    "        ,@LakehouseId = @LakehouseIdInternal\n",
    "        ,@PrimaryKeys = 'CustomerId'\n",
    "        ,@IsActive = 1\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6c1c95-806d-4f37-bddb-344c9fe58d32",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "deployment_manifest[\"configuration\"][\"queries_stored_procedures\"].append(\"\"\"\n",
    "    DECLARE @BronzeLayerEntityIdInternal INT = (SELECT BronzeLayerEntityId FROM integration.BronzeLayerEntity WHERE [Schema] = 'in' and [Name] = 'customer')\n",
    "    DECLARE @SilverLayerEntityIdInternal INT = (SELECT SilverLayerEntityId FROM integration.SilverLayerEntity WHERE [Schema] = 'in' and [Name] = 'customer')\n",
    "    DECLARE @LakehouseIdInternal INT = (SELECT top 1 LakehouseId FROM integration.Lakehouse WHERE Name = 'LH_SILVER_LAYER')\n",
    "    EXECUTE [integration].[sp_UpsertSilverLayerEntity] \n",
    "        @SilverLayerEntityId = @SilverLayerEntityIdInternal\n",
    "        ,@BronzeLayerEntityId = @BronzeLayerEntityIdInternal\n",
    "        ,@LakehouseId = @LakehouseIdInternal\n",
    "        ,@Name = 'customer'\n",
    "        ,@Schema = 'in'\n",
    "        ,@FileType = 'delta'\n",
    "        ,@IsActive = 1\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657493f0-7d03-4fd3-81bc-ca2e7be3ed24",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "for target_item in deployment_manifest['configuration']['items']:\n",
    "    if target_item['type'] == 'SQLDatabase':\n",
    "        connstring = target_item[\"connectionString\"]\n",
    "        database = target_item['databaseName']\n",
    "\n",
    "try:\n",
    "    i = 0\n",
    "    token = mssparkutils.credentials.getToken('https://analysis.windows.net/powerbi/api').encode(\"UTF-16-LE\")\n",
    "    token_struct = struct.pack(f'<I{len(token)}s', len(token), token)\n",
    "    print(f\"DRIVER={driver};SERVER={connstring};PORT=1433;DATABASE={database};\")\n",
    "    connection = pyodbc.connect(f\"DRIVER={driver};SERVER={connstring};PORT=1433;DATABASE={database};\", attrs_before={1256:token_struct}, timeout=12)\n",
    "\n",
    "    with connection.cursor() as cursor:\n",
    "        cursor.execute(\"SELECT 1\")  # Execute the warm-up query (a simple query like 'SELECT 1' can be used)\n",
    "        cursor.fetchone()\n",
    "        connection.timeout = 10  # Setting a lower timeout for subsequent queries\n",
    "        for i, query in enumerate(deployment_manifest[\"configuration\"][\"queries_tables\"]):\n",
    "            print(f' - execute \"{query}\"')\n",
    "            cursor.execute(query)\n",
    "            cursor.commit()\n",
    "        for i, query in enumerate(deployment_manifest[\"configuration\"][\"queries_views\"]):\n",
    "            print(f' - execute \"{query}\"')\n",
    "            cursor.execute(query)\n",
    "            cursor.commit()\n",
    "        for i, query in enumerate(deployment_manifest[\"configuration\"][\"queries_stored_procedures\"]):\n",
    "            print(f' - execute \"{query}\"')\n",
    "            cursor.execute(query)\n",
    "            cursor.commit()\n",
    "        for i, query in enumerate(deployment_manifest[\"configuration\"][\"queries_logging\"]):\n",
    "            print(f' - execute \"{query}\"')\n",
    "            cursor.execute(query)\n",
    "            cursor.commit()\n",
    "    tasks.append({\"task_name\":f\"{workspace.get('displayName')} {database} query {i}\", \"task_duration\": 1, \"status\": f\"success\"})\n",
    "except pyodbc.OperationalError as e:\n",
    "    print(e) \n",
    "    tasks.append({\"task_name\":f\"{workspace.get('displayName')} {database} query {i}\", \"task_duration\": 1, \"status\": f\"pyodbc failed: {e}\"})\n",
    "except Exception as e:\n",
    "    print(e) \n",
    "    tasks.append({\"task_name\":f\"{workspace.get('displayName')} {database} query {i}\", \"task_duration\": 1, \"status\": f\"failed: {e}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b7c873-295f-4e1d-ba84-b6689800f4c5",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "display(tasks)"
   ]
  }
 ],
 "metadata": {
  "a365ComputeOptions": null,
  "dependencies": {
   "lakehouse": {}
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "sessionKeepAliveTimeout": 0,
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  },
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
