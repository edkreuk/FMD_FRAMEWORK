{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce6a663d-0e92-4d07-99e6-365c48b9c726",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "![FMD_Overview](https://github.com/edkreuk/FMD_FRAMEWORK/blob/main/Images/FMD_Overview.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b167cef1-f936-4ca4-b2b2-22a88592ac11",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "%pip install ms-fabric-cli pillow cairosvg --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f028883-3c89-4adf-8da2-9a1d7f5b979d",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "from time import sleep, time\n",
    "import json\n",
    "import shutil\n",
    "import re\n",
    "import requests\n",
    "import zipfile\n",
    "import yaml\n",
    "import struct\n",
    "import pyodbc\n",
    "import notebookutils\n",
    "import sempy.fabric as fabric\n",
    "import cairosvg\n",
    "import base64\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from requests.adapters import HTTPAdapter, Retry\n",
    "from io import BytesIO\n",
    "from zipfile import ZipFile "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ed7375-2e5f-4b5a-ae9f-672b337ddd94",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Configuration and Parameters\n",
    "\n",
    "**Fabric Administrator Role is required to create domain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbf0bf0-89ce-4eb9-bdc3-8efd8444ea4c",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "assign_icons = True                                 # Set to True to assign default icons to workspaces; set to False if you have already assigned custom icons\n",
    "\n",
    "load_demo_data= True                                # Set to True if you want to load the demo data, otherwise set to False\n",
    "lakehouse_schema_enabled = True                     # Set to True if you want to use the lakehouse schema, otherwise set to False\n",
    "\n",
    "driver = '{ODBC Driver 18 for SQL Server}' # Change this if you use a different driver\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa9cdee-13a8-4154-a7ce-6b13aec0d244",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Capacity settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6095d96-c13d-49fd-b388-7347598d2032",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "capacity_name_dvlm = \"Trial-Erwin\"                       # Which capacity will be used for these workspaces in development\n",
    "capacity_name_prod = 'Trial-Erwin'                       # Which capacity will be used for these workspaces in production\n",
    "capacity_name_config = 'Trial-Erwin'               # Which capacity will be used for this workspace for the Metadata Database\n",
    "capacity_name_business_domain_dvlm = 'Trial-Erwin'       # Which capacity will be used for the workspaces in this business domain\n",
    "capacity_name_business_domain_prod = 'Trial-Erwin'       # Which capacity will be used for the workspaces in this business domain\n",
    "reassign_capacity= False                                # Id set to False existing assigned capacities to workspaces will no be overwritten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac4aa59-bbe8-4937-a983-8ddf5859760e",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Domain and Framework settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e6f777-a65a-491b-ba14-0ce8b19c04ff",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "create_domains=  True                               # If you do not have a Fabric Admin role, you need to set this option to False. For domain creation the Fabric Admin role is needed\n",
    "\n",
    "domain_name='INTEGRATION'                           # Main Domain    for example INTEGRATION CODE(D) \n",
    "business_domain_names= ['FINANCE','SALES']          # Create business domains(sub)\n",
    "framework_post_fix= ''                              # post fix to be added at the end of workspace for example INTEGRATION CODE(D) FMD\n",
    "if framework_post_fix != '':\n",
    "   framework_post_fix= ' '+ framework_post_fix      #If empty leave as is else add a space before for better visibility\n",
    "\n",
    "domain_contributor_role = {\"type\": \"Contributors\",\"principals\": [{\"id\": \"86897900-8de5-4894-ae41-1b4d1642acda\",\"type\": \"Group\"}  ]}  # Which group(Object ID) can add or remove workspaces to this domain\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0443c89a-6aec-46c1-9456-37e8f4ab93aa",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Workspace Roles settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ab87f1-553e-426f-9c42-1f2232cd5e8f",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "workspace_roles_code = [ # Keep emtpy [] if you only want to assign this to your personal account\n",
    "                                        {\n",
    "                       \"principal\": {\n",
    "                            \"id\": '86897900-8de5-4894-ae41-1b4d1642acda',\n",
    "                            \"type\": \"Group\"\n",
    "                        },\n",
    "                        \"role\": \"admin\"  #(choose from 'admin', 'member', 'contributor', 'viewer')\n",
    "                        },\n",
    "                        {\n",
    "                       \"principal\": {\n",
    "                            \"id\": 'b0bf9847-5b7a-4560-947b-b149f71d303b',\n",
    "                            \"type\": \"ServicePrincipal\"\n",
    "                        },\n",
    "                        \"role\": \"contributor\"  #(choose from 'admin', 'member', 'contributor', 'viewer')\n",
    "                        }\n",
    "                    ]\n",
    "workspace_roles_data =  [ # Keep emtpy [] if you only want to assign this to your personal account\n",
    "                                        {\n",
    "                       \"principal\": {\n",
    "                            \"id\": '86897900-8de5-4894-ae41-1b4d1642acda',\n",
    "                            \"type\": \"Group\"\n",
    "                        },\n",
    "                        \"role\": \"admin\"  #(choose from 'admin', 'member', 'contributor', 'viewer')\n",
    "                        },    \n",
    "                         {\n",
    "                       \"principal\": {\n",
    "                            \"id\": '5c906b5c-d1d7-4984-b047-adacd8d795fe',\n",
    "                            \"type\": \"Group\"\n",
    "                        },\n",
    "                        \"role\": \"contributor\"  #(choose from 'admin', 'member', 'contributor', 'viewer')\n",
    "                        },\n",
    "                        {\n",
    "                       \"principal\": {\n",
    "                            \"id\": 'b0bf9847-5b7a-4560-947b-b149f71d303b',\n",
    "                            \"type\": \"ServicePrincipal\"\n",
    "                        },\n",
    "                        \"role\": \"contributor\"  #(choose from 'admin', 'member', 'contributor', 'viewer')\n",
    "                        }\n",
    "                    ]\n",
    "workspace_roles_data_business_domain = [ # Keep emtpy [] if you only want to assign this to your personal account\n",
    "                    {\n",
    "                      \"principal\": {\n",
    "                            \"id\": '86897900-8de5-4894-ae41-1b4d1642acda',\n",
    "                             \"type\": \"Group\"\n",
    "                        },\n",
    "                        \"role\": \"admin\"  #(choose from 'admin', 'member', 'contributor', 'viewer')\n",
    "                        }\n",
    "                    ]\n",
    "\n",
    "workspace_roles_code_business_domain = [ # Keep emtpy [] if you only want to assign this to your personal account\n",
    "                    {\n",
    "                      \"principal\": {\n",
    "                            \"id\": '86897900-8de5-4894-ae41-1b4d1642acda',\n",
    "                             \"type\": \"Group\"\n",
    "                        },\n",
    "                        \"role\": \"admin\"  #(choose from 'admin', 'member', 'contributor', 'viewer')\n",
    "                        }\n",
    "                    ]\n",
    "\n",
    "workspace_roles_reporting_business_domain = [ # Keep emtpy [] if you only want to assign this to your personal account\n",
    "                    {\n",
    "                        \"principal\": {\n",
    "                            \"id\": '86897900-8de5-4894-ae41-1b4d1642acda',\n",
    "                            \"type\": \"Group\"\n",
    "                        },\n",
    "                        \"role\": \"admin\"  #(choose from 'admin', 'member', 'contributor', 'viewer')\n",
    "                        }\n",
    "                    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85feeaf9-e255-44e2-b2c2-0598858f20fc",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Configuration settings  (Fabric Database)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799ce0af-8895-4abb-a526-4e30764dc379",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "configuration = {\n",
    "                    'workspace': {\n",
    "                        'name' : domain_name + ' CONFIG' +  framework_post_fix,             # Name of target workspace\n",
    "                        'roles' : workspace_roles_data,                                     # Roles to assign to the workspace\n",
    "                        'capacity_name' : capacity_name_config                              # Name of target capacity for the configuration workspace\n",
    "                    },\n",
    "                       'DatabaseName' : 'SQL_'+domain_name+'_FRAMEWORK'                     # Name of target configuration SQL Database\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32b09b0-c276-47f6-b223-57220f6af1c3",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Workspace configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ada47fa-6823-4cdc-bebe-3f573f4f2bd1",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "##### DO NOT CHANGE UNLESS SPECIFIED OTHERWISE, FE ADDING NEW ENVIRONMENTS ####\n",
    "environments = [\n",
    "                    {\n",
    "                        'environment_name' : 'development',                                     # Name of target environment\n",
    "                        'workspaces': {\n",
    "                            'data' : {\n",
    "                                'name' : domain_name + ' DATA (D)' +  framework_post_fix,       # Name of target code workspace for development\n",
    "                                'roles' : workspace_roles_data,                                 # Roles to assign to the workspace\n",
    "                                'capacity_name' : capacity_name_dvlm                            # Name of target data workspace for development\n",
    "                            },\n",
    "                            'code' : {\n",
    "                                'name' : domain_name + ' CODE (D)' +  framework_post_fix,       # Name of target data workspace for development\n",
    "                                'roles' : workspace_roles_code,                                 # Roles to assign to the workspace\n",
    "                                'capacity_name' : capacity_name_dvlm                            # Name of target code workspace for development\n",
    "                            },\n",
    "                        },\n",
    "                        'connections' : {\n",
    "                            'CON_FMD_FABRIC_SQL' : '372237f9-709a-48f8-8fb2-ce06940c990e',          # Required Guid to the Fabric SQL connection\n",
    "                            'CON_FMD_FABRIC_PIPELINES' : '6d8146c6-a438-47df-94e2-540c552eb6d7',    # Required Guid to the Fabric datapipelines connection\n",
    "                            'CON_FMD_ADF_PIPELINES' : '02e107b8-e97e-4b00-a28c-668cf9ce3d9a'        # 10000000-0000-0000-0000-000000000000'\n",
    "\n",
    "                        }\n",
    "                    },\n",
    "                    {\n",
    "                        'environment_name' : 'production',                                      # Name of target environment\n",
    "                        'workspaces': {\n",
    "                            'data' : {\n",
    "                                'name' : domain_name + ' DATA (P)' +  framework_post_fix,       # Name of target data workspace for production\n",
    "                                'roles' : workspace_roles_data,                                 # Roles to assign to the workspace\n",
    "                                'capacity_name' : capacity_name_prod                            # Name of target data workspace for production   \n",
    "                            },\n",
    "                            'code' : {\n",
    "                                'name' : domain_name + ' CODE (P)' +  framework_post_fix,       # Name of target code workspace for production\n",
    "                                'roles' : workspace_roles_code,                                 # Roles to assign to the workspace\n",
    "                                'capacity_name' : capacity_name_prod                            # Name of target code workspace for production\n",
    "                            },                            \n",
    "                        },\n",
    "                        'connections' : {\n",
    "                            'CON_FMD_FABRIC_SQL' : '372237f9-709a-48f8-8fb2-ce06940c990e',          # Required Guid to the Fabric SQL connection\n",
    "                            'CON_FMD_FABRIC_PIPELINES' : '6d8146c6-a438-47df-94e2-540c552eb6d7',    # Required Guid to the Fabric datapipelines connection\n",
    "                            'CON_FMD_ADF_PIPELINES' : '02e107b8-e97e-4b00-a28c-668cf9ce3d9a'        # 10000000-0000-0000-0000-000000000000'\n",
    "\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "###################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5faac4b5-58fd-4c94-af27-adfd443d43ee",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Domain Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ded5a6-4998-4f43-a7aa-5a9c26dad6a1",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "##### DO NOT CHANGE UNLESS SPECIFIED OTHERWISE, FE ADDING NEW ENVIRONMENTS ####\n",
    "business_domain_deployment = [\n",
    "                    {\n",
    "                        'environment_name' : 'development',                                 # Name of target environment\n",
    "                        'environment_short' : 'D',                                          # Short of target environment\n",
    "                        'workspaces': {\n",
    "                         \n",
    "                            'data' : {\n",
    "                                'roles' : workspace_roles_data_business_domain,             # Roles to assign to the workspace\n",
    "                                'capacity_name' : capacity_name_business_domain_dvlm        # Name of target data workspace for development\n",
    "                            },\n",
    "                            'code' : {\n",
    "                                'roles' : workspace_roles_code_business_domain,             # Roles to assign to the workspace\n",
    "                                'capacity_name' : capacity_name_business_domain_dvlm        # Name of target code workspace for development\n",
    "                            },\n",
    "                            'reporting' : {\n",
    "                            'roles' : workspace_roles_reporting_business_domain,            # Roles to assign to the workspace\n",
    "                            'capacity_name' : capacity_name_business_domain_dvlm            # Name of target code workspace for development\n",
    "                            }\n",
    "                        }\n",
    "                    },\n",
    "                    {\n",
    "                        'environment_name' : 'production',                                  # Name of target environment\n",
    "                        'environment_short' : 'P',                                          # Short of target environment\n",
    "                        'workspaces': {\n",
    "                         \n",
    "                            'data' : {\n",
    "                                'roles' : workspace_roles_data_business_domain,             # Roles to assign to the workspace\n",
    "                                'capacity_name' : capacity_name_business_domain_prod        # Name of target data workspace for development\n",
    "                            },\n",
    "                            'code' : {\n",
    "                                'roles' : workspace_roles_code_business_domain,             # Roles to assign to the workspace\n",
    "                                'capacity_name' : capacity_name_business_domain_prod        # Name of target code workspace for development\n",
    "                            },\n",
    "                            'reporting' : {\n",
    "                                'roles' : workspace_roles_reporting_business_domain,        # Roles to assign to the workspace\n",
    "                                'capacity_name' : capacity_name_business_domain_prod        # Name of target code workspace for development\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "###################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c0591f-d32c-4bcc-b8f6-4da1a98e18bf",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Icon settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208617c3-c49a-437f-a996-a986e557a1ca",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "# Workspace icon definition. Setting the icons to None will delete the existing icon of the workspaces specified.\n",
    "\n",
    "workspace_icon_def = {\n",
    "    \"icons\": {\n",
    "        \"code\": \"fmd_code_icon.png\",\n",
    "        \"data\": \"fmd_data_icon.png\",\n",
    "        \"config\": \"fmd_config_icon.png\",\n",
    "        \"reporting\": \"fmd_reporting_icon.png\",\n",
    "        \"business_domain\": \"fmd_gold_icon.png\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868f7b0f-a0ca-4d6c-ab64-b231eb3b0cba",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Repo Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e453b88-bb97-4378-b3fb-de6faa6b6d1d",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "#FMD Framework code\n",
    "##### DO NOT CHANGE UNLESS SPECIFIED OTHERWISE ####\n",
    "repo_owner = \"edkreuk\"              # Owner of the repository\n",
    "repo_name = \"FMD_FRAMEWORK\"         # Name of the repository\n",
    "branch = \"main\"                     #\"main\" is default                    \n",
    "folder_prefix = \"\"\n",
    "###################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32269821-d83f-41ac-a3d4-a19f8cb555de",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Download source & config files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22cc332-d939-4def-8971-84f762931ce2",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "def download_folders_as_zip(repo_owner, repo_name, output_zip, branch=\"main\", folders_to_extract=None, remove_folder_prefix=\"\"):\n",
    "    if folders_to_extract is None:\n",
    "        folders_to_extract = []\n",
    "\n",
    "    # Construct the URL for the GitHub API to download the repository as a zip file\n",
    "    url = f\"https://api.github.com/repos/{repo_owner}/{repo_name}/zipball/{branch}\"\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    # Ensure the directory for the output zip file exists\n",
    "    os.makedirs(os.path.dirname(output_zip), exist_ok=True)\n",
    "\n",
    "    # Create a zip file in memory from GitHub response\n",
    "    with zipfile.ZipFile(BytesIO(response.content)) as zipf:\n",
    "        # Open output zip in append mode\n",
    "        with zipfile.ZipFile(output_zip, 'w') as output_zipf:\n",
    "            \n",
    "            for file_info in zipf.infolist():\n",
    "                for folder in folders_to_extract:\n",
    "                    folder_path = f\"/{folder}\" if not folder.startswith(\"/\") else folder\n",
    "                    if re.sub(r'^.*?/', '/', file_info.filename).startswith(folder_path):\n",
    "                        file_data = zipf.read(file_info.filename)\n",
    "                        parts = file_info.filename.split('/')\n",
    "                        if remove_folder_prefix:\n",
    "                            parts = [p for p in parts if p != remove_folder_prefix]\n",
    "                        output_zipf.writestr('/'.join(parts[1:]), file_data)\n",
    "\n",
    "def uncompress_zip_to_folder(zip_path, extract_to):\n",
    "    os.makedirs(extract_to, exist_ok=True)\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "    os.remove(zip_path)\n",
    "\n",
    "# ✅ Combine all folders into one zip\n",
    "download_folders_as_zip(repo_owner, repo_name, output_zip = \"./builtin/src/src.zip\", branch = branch, folders_to_extract= [f\"{folder_prefix}/src\"] , remove_folder_prefix = f\"{folder_prefix}\")\n",
    "download_folders_as_zip(repo_owner, repo_name, output_zip = \"./builtin/config/config.zip\", branch = branch, folders_to_extract= [f\"{folder_prefix}/config\"] , remove_folder_prefix = f\"{folder_prefix}\")\n",
    "# ✅ Uncompress everything into ./builtin\n",
    "uncompress_zip_to_folder(zip_path = \"./builtin/config/config.zip\", extract_to= \"./builtin\")\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5c3526-9a63-430c-90e4-00ece0aebfa5",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# CLI Login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00e2fa8-e10d-45a9-82a7-266feaedc4ba",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "# Set environment parameters for Fabric CLI\n",
    "token = notebookutils.credentials.getToken('pbi')\n",
    "os.environ['FAB_TOKEN'] = token\n",
    "os.environ['FAB_TOKEN_ONELAKE'] = token\n",
    "\n",
    "tasks=[]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2171308-8526-4564-b8cb-21c588d2ae24",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Get existing connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0c6781-b3b5-47d0-a9ff-e192c7bdee9d",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "result = subprocess.run([\"fab\", \"api\", \"-X\", \"get\", \"connections\"], capture_output=True, text=True)\n",
    "connections=json.loads(result.stdout)[\"text\"]\n",
    "connection_list = [item['id'] for item in connections['value']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b39ba9-62fb-421b-be07-2607f79fccdb",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Deployment functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7202e909-634f-42f5-a8b6-034430532d7a",
   "metadata": {
    "jupyter": {
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# FABRIC CLI Utilities\n",
    "# -------------------------------\n",
    "\n",
    "def run_fab_command(command, capture_output=False, silently_continue=False, raw_output=False):\n",
    "    \"\"\"\n",
    "    Executes a Fabric CLI command with optional output capture and error handling.\n",
    "    \"\"\"\n",
    "    result = subprocess.run([\"fab\", \"-c\", command], capture_output=capture_output, text=True)\n",
    "    if not silently_continue and (result.returncode > 0 or result.stderr):\n",
    "        raise Exception(f\"Error running fab command. exit_code: '{result.returncode}'; stderr: '{result}'\")\n",
    "    if capture_output:\n",
    "        return result if raw_output else result.stdout.strip()\n",
    "    return None\n",
    "\n",
    "def get_cluster_url():\n",
    "    \"\"\"\n",
    "    Get the Fabric Cluster.\n",
    "    \"\"\"\n",
    "    response = run_fab_command(f\"api -A powerbi groups\", capture_output=True, silently_continue=True)\n",
    "    # Parse the JSON response\n",
    "    response_json = json.loads(response)[\"text\"]\n",
    "    # Extract the @odata.context URL\n",
    "    odata_context = response_json.get(\"@odata.context\", \"\")\n",
    "    # Use re.search instead of re.match\n",
    "    match = re.search(r\"(https://[^/]+/)\", odata_context)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    else:\n",
    "        print(\"Cluster URL not found.\")\n",
    "        return None\n",
    "    \n",
    "# -------------------------------\n",
    "# Domain Management\n",
    "# -------------------------------\n",
    "def create_fabric_domain(domain_name):\n",
    "    \"\"\"\n",
    "    Create a domain.\n",
    "    \"\"\"\n",
    "    start = time()\n",
    "    try:\n",
    "        run_fab_command(f'create .domains/{domain_name}.Domain',  capture_output=True, silently_continue=True)\n",
    "        print(f\"✅ {domain_name} Domain Created'\")\n",
    "    except Exception as e:\n",
    "        print(f\" ❌ Failed to create domain: {e}\")\n",
    "    assign_domain_description(domain_name)\n",
    "    assign_domain_contributor_roles(domain_contributor_role, domain_name)\n",
    "    tasks.append({\"task_name\": f\"Create or Update Domain {domain_name}\",\"task_duration\": int(time() - start),\"status\": \"success\"})\n",
    "\n",
    "def create_fabric_business_domain(domain_name, business_domain):\n",
    "    \"\"\"\n",
    "    Create a sub domain in a domain.\n",
    "    \"\"\"\n",
    "    start = time()\n",
    "    try:\n",
    "        run_fab_command(f'create .domains/{business_domain}.Domain -P parentDomainName={domain_name}',  capture_output=True, silently_continue=True)\n",
    "        print(f\"✅ {business_domain} created and assigned to '{domain_name}'\")\n",
    "    except Exception as e:\n",
    "        print(f\" ❌ Failed to create sub domain: {e}\")\n",
    "    assign_domain_description(domain_name)\n",
    "    tasks.append({\"task_name\": f\"Create or update Sub Domain {business_domain}\",\"task_duration\": int(time() - start),\"status\": \"success\"})\n",
    "\n",
    "def assign_fabric_domain(domain_name, workspace_name):\n",
    "    \"\"\"\n",
    "    Assigns a domain to the workspace.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        run_fab_command(f'assign .domains/{domain_name}.Domain -W {workspace_name}.Workspace -f',  capture_output=True, silently_continue=True)\n",
    "        print(f\"✅ {domain_name} domain assigned to '{workspace_name}'\")\n",
    "    except Exception as e:\n",
    "        print(f\" ❌ Failed to assign domain: {e}\")\n",
    "\n",
    "def assign_domain_description(domain_name):\n",
    "    \"\"\"\n",
    "    Assigns a standard description to an Domain.\n",
    "    \"\"\"\n",
    "    payload = 'Note: This Domain  was initially generated by the FMD Framework. For further details, please refer to the documentation at https://github.com/edkreuk/FMD_FRAMEWORK.'\n",
    "    try:\n",
    "        run_fab_command(f'set .domains/{domain_name}.Domain -q description -i {payload} -f', silently_continue=True)\n",
    "        print(f\"✅ Description applied to {domain_name} \")\n",
    "    except Exception as e:\n",
    "        print(f\" ❌ Failed to assign description: {e}\")\n",
    "\n",
    "def assign_domain_contributor_roles(domain_contributor_role,domain_name):\n",
    "    \"\"\"\n",
    "    Assigns the Contributor role to a domain for a specific security group.\n",
    "    \"\"\"\n",
    "    payload = 'SpecificUsersAndGroups'\n",
    "    run_fab_command(f'set .domains/{domain_name}.Domain -q contributorsScope -i {payload} -f', capture_output=True,silently_continue=True)\n",
    "    domain_id = get_domain_id_by_name(domain_name)\n",
    "    payload_role = json.dumps(domain_contributor_role)\n",
    "    try:\n",
    "        run_fab_command(f'api -X post admin/domains/{domain_id}/roleAssignments/bulkAssign -i \"{payload_role}\"',capture_output=True ,silently_continue=True  )\n",
    "        print(f\"✅ Contributor role applied to domain: {domain_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\" ❌ Failed to apply role: {e}\")\n",
    "\n",
    "def get_domain_id_by_name(domain_name):\n",
    "    \"\"\"\n",
    "    Retrieves the domain ID by its display name.\n",
    "    \"\"\"\n",
    "    result = run_fab_command(\"api -X get domains/\", capture_output=True, silently_continue=True)\n",
    "    domains = json.loads(result)[\"text\"][\"value\"]\n",
    "    normalized_name = domain_name.strip().lower()\n",
    "    match = next((w for w in domains if w['displayName'].strip().lower() == normalized_name), None)\n",
    "    return match['id'] if match else None\n",
    "# -------------------------------\n",
    "# Workspace Management\n",
    "# -------------------------------\n",
    "\n",
    "def get_workspace_id_by_name(workspace_name):\n",
    "    \"\"\"\n",
    "    Retrieves the workspace ID by its display name.\n",
    "    \"\"\"\n",
    "    result = run_fab_command(\"api -X get workspaces/\", capture_output=True, silently_continue=True)\n",
    "    workspaces = json.loads(result)[\"text\"][\"value\"]\n",
    "    normalized_name = workspace_name.strip().lower()\n",
    "    match = next((w for w in workspaces if w['displayName'].strip().lower() == normalized_name), None)\n",
    "    return match['id'] if match else None\n",
    "\n",
    "def ensure_workspace_exists(workspace, workspace_name):\n",
    "    \"\"\"\n",
    "    Ensures the workspace exists; creates it if not found.\n",
    "    Optionally reassigns capacity if requested.\n",
    "    \"\"\"\n",
    "    workspace_id = get_workspace_id_by_name(workspace_name)\n",
    "\n",
    "    if workspace_id:\n",
    "        print(f\" - Workspace '{workspace_name}' found. Workspace ID: {workspace_id}\")\n",
    "        if reassign_capacity:\n",
    "            print(f\" - Assigning capacity: {workspace['capacity_name']}\")\n",
    "            try:\n",
    "                run_fab_command(f'assign \".capacities/{workspace[\"capacity_name\"]}.Capacity\" 'f'-W \"{workspace_name}.Workspace\" -f',capture_output=True, silently_continue=True)\n",
    "                print(f\" ✅ Capacity assigned: {workspace['capacity_name']}\")\n",
    "            except Exception as e:\n",
    "                print(f\" ❌ Failed to assign capacity: {e}\")\n",
    "            return workspace_id, \"exists\"\n",
    "        else:\n",
    "            print(f\" ✅ Capacity assignment disabled\")\n",
    "            return workspace_id, \"exists\"\n",
    "    else:\n",
    "        print(f\" - Workspace '{workspace_name}' not found. Creating new workspace...\")\n",
    "        try:\n",
    "            run_fab_command(f'mkdir \"{workspace_name}.workspace\" -P capacityName=\"{workspace[\"capacity_name\"]}\"',capture_output=True, silently_continue=True)\n",
    "            print(f\" ✅ Workspace '{workspace_name}' created\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\" ❌ Failed to create workspace: {e}\")\n",
    "\n",
    "        # Verify creation\n",
    "        workspace_id = get_workspace_id_by_name(workspace_name)\n",
    "        if workspace_id:\n",
    "            print(f\" - Created workspace '{workspace_name}'. ID: {workspace_id}\")\n",
    "            return workspace_id, \"created\"\n",
    "        else:\n",
    "            raise RuntimeError(f\"Workspace '{workspace_name}' could not be created or found.\")\n",
    "# -------------------------------\n",
    "# Item Utilities\n",
    "# -------------------------------\n",
    "\n",
    "def get_item_id(workspace_name, name):\n",
    "    \"\"\"\n",
    "    Retrieves the item ID from a workspace.\n",
    "    \"\"\"\n",
    "    return run_fab_command(f\"get /{workspace_name}.Workspace/{name} -q id\", capture_output=True, silently_continue=True)\n",
    "\n",
    "def get_item_display_name(workspace_name, name):\n",
    "    \"\"\"\n",
    "    Retrieves the display name of an item.\n",
    "    \"\"\"\n",
    "    return run_fab_command(f\"get /{workspace_name}.Workspace/{name} -q displayName\", capture_output=True, silently_continue=True)\n",
    "\n",
    "def get_items(workspace_id, item_id=''):\n",
    "    \"\"\"\n",
    "    Retrieves item definitions or lists from a workspace.\n",
    "    \"\"\"\n",
    "    if item_id:\n",
    "        return run_fab_command(f\"api -X post workspaces/{workspace_id}/items/{item_id}/getDefinition\", capture_output=True, silently_continue=True)\n",
    "    return run_fab_command(f\"api -X get workspaces/{workspace_id}/items/{item_id}\", capture_output=True, silently_continue=True)\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# File and ID Replacement\n",
    "# -------------------------------\n",
    "\n",
    "def copy_to_tmp(name, child=None):\n",
    "    \"\"\"\n",
    "    Extracts item files from a ZIP archive to a temporary directory,\n",
    "    including all subfolders under the specified path.\n",
    "    Checks src/{name} first; if not found, checks src/business_domain/{name}.\n",
    "    Returns the path for the first match only.\n",
    "    \"\"\"\n",
    "    child_path = \"\" if child is None else f\".children/{child}/\"\n",
    "    shutil.rmtree(\"./builtin/tmp\", ignore_errors=True)\n",
    "    path2zip = \"./builtin/src/src.zip\"\n",
    "\n",
    "    prefixes = [\n",
    "        f\"src/{name}/{child_path}\",\n",
    "        f\"src/business_domain/{name}/{child_path}\"\n",
    "    ]\n",
    "\n",
    "    with ZipFile(path2zip) as archive:\n",
    "        for prefix in prefixes:\n",
    "            matched_files = [file for file in archive.namelist() if file.startswith(prefix)]\n",
    "            if matched_files:\n",
    "                for file in matched_files:\n",
    "                    archive.extract(file, \"./builtin/tmp\")\n",
    "                return f\"./builtin/tmp/{prefix}\"  # Return only the first matching prefix\n",
    "\n",
    "    return None  # Nothing found\n",
    "\n",
    "\n",
    "def replace_ids_in_folder(folder_path, mapping_table, environment_name):\n",
    "    \"\"\"\n",
    "    Replaces old IDs with new ones in specified file types within a folder.\n",
    "    \"\"\"\n",
    "    for root, _, files in os.walk(folder_path):\n",
    "        for file_name in files:\n",
    "            if file_name.endswith(('.py', '.json', '.pbir', '.platform', '.ipynb', '.tmdl')) and not file_name.endswith('report.json'):\n",
    "                file_path = os.path.join(root, file_name)\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    content = file.read()\n",
    "                    for mapping in mapping_table:\n",
    "                        if mapping[\"environment\"] in (environment_name, \"config\"):\n",
    "                            content = content.replace(mapping[\"old_id\"], mapping[\"new_id\"])\n",
    "                with open(file_path, 'w', encoding='utf-8') as file:\n",
    "                    file.write(content)\n",
    "\n",
    "def replace_ids_and_mark_inactive(folder_path, mapping_table, environment_name, target_guids):\n",
    "    \"\"\"\n",
    "    Replaces old IDs with new ones in JSON-based files and deactivates activities\n",
    "    that reference connections not in the target_guids list.\n",
    "\n",
    "    Parameters:\n",
    "    - folder_path (str): Path to the folder containing files to process.\n",
    "    - mapping_table (list): List of dictionaries with 'old_id', 'new_id', and 'environment'.\n",
    "    - environment_name (str): Current environment name to filter applicable mappings.\n",
    "    - target_guids (list): List of valid connection GUIDs to retain as active.\n",
    "\n",
    "    Returns:\n",
    "    - None. Files are modified in-place.\n",
    "    \"\"\"\n",
    "    def find_externalReferences_in_dict(j):\n",
    "        externalReferences = {}\n",
    "        for key, value in j.items():\n",
    "            if isinstance(value, dict):\n",
    "                externalReferences.update(find_externalReferences_in_dict(value))\n",
    "            if key == \"externalReferences\":\n",
    "                externalReferences[key] = value\n",
    "        return externalReferences\n",
    "\n",
    "    def should_deactivate(connection):\n",
    "        return (\n",
    "            connection not in target_guids and\n",
    "            connection not in ['@item().ConnectionGuid', '@pipeline().parameters.ConnectionGuid']\n",
    "        )\n",
    "\n",
    "    def process_nested_activities(activities):\n",
    "        for activity in activities:\n",
    "            result = find_externalReferences_in_dict(activity)\n",
    "            connection = result.get('externalReferences', {}).get('connection')\n",
    "            if connection and should_deactivate(connection):\n",
    "                print(f\"Deactivate activity {activity.get('name')} for connection {connection}\")\n",
    "                activity[\"state\"] = \"Inactive\"\n",
    "                activity[\"onInactiveMarkAs\"] = \"Succeeded\"\n",
    "\n",
    "    for root, _, files in os.walk(folder_path):\n",
    "        for file_name in files:\n",
    "            if file_name.endswith(('.py', '.json', '.pbir', '.platform', '.ipynb', '.tmdl')) and not file_name.endswith('report.json'):\n",
    "                file_path = os.path.join(root, file_name)\n",
    "\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    content = file.read()\n",
    "\n",
    "                # Replace IDs\n",
    "                for mapping in mapping_table:\n",
    "                    if mapping[\"environment\"] in (environment_name, \"config\"):\n",
    "                        content = content.replace(mapping[\"old_id\"], mapping[\"new_id\"])\n",
    "\n",
    "                try:\n",
    "                    data = json.loads(content)\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "\n",
    "                if not data or not data.get(\"properties\") or not data[\"properties\"].get(\"activities\"):\n",
    "                    continue\n",
    "\n",
    "                for activity in data[\"properties\"][\"activities\"]:\n",
    "                    process_nested_activities([activity])\n",
    "                    for key in [\"activities\", \"ifFalseActivities\", \"ifTrueActivities\"]:\n",
    "                        nested = activity.get(\"typeProperties\", {}).get(key)\n",
    "                        if nested:\n",
    "                            process_nested_activities(nested)\n",
    "\n",
    "                content = json.dumps(data, indent=2)\n",
    "                with open(file_path, 'w', encoding='utf-8') as file:\n",
    "                    file.write(content)\n",
    "\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Description and Identity Assignment\n",
    "# -------------------------------\n",
    "\n",
    "def assign_workspace_description(workspace_name):\n",
    "    \"\"\"\n",
    "    Assigns a standard description to the workspace.\n",
    "    \"\"\"\n",
    "    payload = 'Important: The items in this workspace are automatically generated by the FMD Framework. Each time the setup notebook is executed, all changes will be overwritten. For more information, please visit https://github.com/edkreuk/FMD_FRAMEWORK.'\n",
    "    try:\n",
    "        run_fab_command(f'set \"/{workspace_name}.workspace -q description -i {payload} -f', silently_continue=True)\n",
    "        print(f\" ✅ Description applied to  '{workspace_name}'\")\n",
    "    except Exception as e:\n",
    "        print(f\" ❌ Failed to apply description: {e}\")\n",
    "\n",
    "def assign_item_description(workspace_name, item):\n",
    "    \"\"\"\n",
    "    Assigns a standard description to an item.\n",
    "    \"\"\"\n",
    "    payload = 'Note: This item was initially generated by the FMD Framework. Any modifications may introduce breaking changes. For further details, please refer to the documentation at https://github.com/edkreuk/FMD_FRAMEWORK.'\n",
    "    try:\n",
    "        run_fab_command(f'set \"/{workspace_name}.workspace/{item} -q description -i {payload} -f', silently_continue=True)\n",
    "        print(f\" ✅ Description applied to {item} in '{workspace_name}'\")\n",
    "    except Exception as e:\n",
    "        print(f\" ❌ Failed to apply description: {e}\")\n",
    "\n",
    "def assign_managed_identity(workspace_name):\n",
    "    \"\"\"\n",
    "    Assigns a managed identity to the workspace.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        run_fab_command(f'create \"/{workspace_name}.workspace/.managedidentities/{workspace_name}.ManagedIdentity\"', silently_continue=True)\n",
    "        print(f\" ✅ Managed identity assigned to '{workspace_name}'\")\n",
    "    except Exception as e:\n",
    "        print(f\" ❌ Failed to assign managed identity: {e}\")\n",
    "\n",
    "# -------------------------------\n",
    "# Role Assignment\n",
    "# -------------------------------\n",
    "\n",
    "def assign_workspace_roles(workspace, workspace_name):\n",
    "    \"\"\"\n",
    "    Assigns roles to principals in the workspace.\n",
    "    \"\"\"\n",
    "    workspace_path = f\"/{workspace_name}.workspace\"\n",
    "    print(f\" - Assigning Workspace roles\")\n",
    "    for role in workspace['roles']:\n",
    "        try:\n",
    "            print(f\"Assigning role '{role['role']}' to '{role['principal']}' in workspace '{workspace_name}'\")\n",
    "            run_fab_command(f'acl set \"{workspace_path}\" -I {role[\"principal\"][\"id\"]} -R {role[\"role\"]} -f', capture_output=True, silently_continue=True)\n",
    "        except Exception as e:\n",
    "            print(f\" ❌ Failed to assign role: {e}\")\n",
    "\n",
    "# -------------------------------\n",
    "# Folder Handling\n",
    "# -------------------------------\n",
    "\n",
    "def get_workspace_folders(workspace_id):\n",
    "    \"\"\"\n",
    "    Retrieves all folders in a workspace.\n",
    "    \"\"\"\n",
    "    response = run_fab_command(f\"api workspaces/{workspace_id}/folders\", capture_output=True, silently_continue=True)\n",
    "    return json.loads(response).get('text', {}).get('value', [])\n",
    "\n",
    "def get_workspace_folder(workspace_id, folder_name):\n",
    "    \"\"\"\n",
    "    Retrieves folder metadata by name.\n",
    "    \"\"\"\n",
    "    for f in get_workspace_folders(workspace_id):\n",
    "        if f.get('displayName') == folder_name:\n",
    "            return f\n",
    "    return None\n",
    "\n",
    "def assign_item_to_folder(workspace_id, item_id, folder):\n",
    "    \"\"\"\n",
    "    Assigns an item to a folder, creating the folder if it doesn't exist.\n",
    "    \"\"\"\n",
    "    folder_details = get_workspace_folder(workspace_id, folder)\n",
    "    if folder_details is None:\n",
    "        payload = json.dumps({\"displayName\": folder})\n",
    "        print(f\"Folder does not exist, creating {payload}\")\n",
    "        folder_details = run_fab_command(f\"api -X post workspaces/{workspace_id}/folders -i {payload}\", capture_output=True, silently_continue=False)\n",
    "        folder_details = json.loads(folder_details).get('text', {})\n",
    "    payload = json.dumps({'folder': folder_details.get('id')})\n",
    "    #result = run_fab_command(f\"api -X post workspaces/{workspace_id}/items/{item_id}/move -i {payload}\", capture_output=True, silently_continue=False)\n",
    "    result = run_fab_command(f\"mv {workspace_name}.workspace/{item} {workspace_name}.workspace/{folder}.Folder  \", capture_output=True, silently_continue=False)\n",
    "\n",
    "\n",
    "def deploy_workspaces(domain_name,workspace, workspace_name, environment_name, old_id, mapping_table, tasks):\n",
    "    \"\"\"\n",
    "    Deploys a workspace by ensuring its existence, assigning identity, roles, and description.\n",
    "    Updates the mapping table and logs the deployment task.\n",
    "\n",
    "    Parameters:\n",
    "    - workspace (dict): Workspace configuration including name and capacity.\n",
    "    - environment_name (str): Target environment name.\n",
    "    - old_id (str): Previous workspace ID to be replaced.\n",
    "    - mapping_table (list): List to store ID mappings.\n",
    "    - tasks (list): List to store task execution logs.\n",
    "    \"\"\"\n",
    "    start = time()\n",
    "    print(\"\\n#############################################\")\n",
    "    print(f\" - Processing: workspace {workspace_name}\")\n",
    "\n",
    "    workspace_id, status = ensure_workspace_exists(workspace, workspace_name)\n",
    "    workspace[\"id\"] = workspace_id\n",
    "\n",
    "    print(\"--------------------------\")\n",
    "    print(f\"Updating Mapping Table: {environment_name}\")\n",
    "    mapping_table.append({\"Description\": workspace_name,\"environment\": environment_name,\"ItemType\": \"Workspace\",\"old_id\": old_id,\"new_id\": workspace_id })\n",
    "    mapping_table.append({\"Description\": workspace_name,\"environment\": environment_name,\"ItemType\": \"Workspace\",\"old_id\": \"00000000-0000-0000-0000-000000000000\",\"new_id\": workspace_id})\n",
    "\n",
    "    assign_managed_identity(workspace_name)\n",
    "    assign_workspace_roles(workspace,workspace_name)\n",
    "    assign_workspace_description(workspace_name)\n",
    "    if create_domains:\n",
    "        assign_fabric_domain(domain_name, workspace_name)  \n",
    "\n",
    "    tasks.append({\"task_name\": f\"Create or Update workspace {workspace_name}\",\"task_duration\": int(time() - start),\"status\": \"success\" })\n",
    "\n",
    "def deploy_item(workspace,workspace_name,name, mapping_table, environment_name, connection_list, tasks, lakehouse_schema_enabled, child=None, it=None):\n",
    "    \"\"\"\n",
    "    Deploys an item (Notebook, Lakehouse, DataPipeline) into a workspace.\n",
    "    Handles ID replacement, description assignment, and updates mapping and task logs.\n",
    "\n",
    "    Parameters:\n",
    "    - workspace (dict): Workspace configuration including name.\n",
    "    - name (str): Name of the item to deploy.\n",
    "    - mapping_table (list): List to store ID mappings.\n",
    "    - environment_name (str): Target environment name.\n",
    "    - connection_list (list): List of valid connection GUIDs.\n",
    "    - tasks (list): List to store task execution logs.\n",
    "    - lakehouse_schema_enabled (bool): Flag to enable schema creation for lakehouses.\n",
    "    - child (str, optional): Child item name if applicable.\n",
    "    - it (dict, optional): Item metadata including old ID.\n",
    "    \"\"\"\n",
    "    start = time()\n",
    "    print(\"\\n#############################################\")\n",
    "    print(f\"Deploying in {workspace_name}: {name}\")\n",
    "\n",
    "    tmp_path = copy_to_tmp(name, child )\n",
    "    name = name if child is None else child\n",
    "    workspace_id = get_workspace_id_by_name(workspace_name)\n",
    "    cli_parameter = ''\n",
    "\n",
    "    if \"Notebook\" in name:\n",
    "        cli_parameter += \" --format .py\"\n",
    "        result = run_fab_command(f\"import / {workspace_name}.Workspace/{name} -i {tmp_path} -f {cli_parameter}\",capture_output=True, silently_continue=True)\n",
    "        assign_item_description(workspace_name, name)\n",
    "        new_id = get_item_id(workspace_name, name)\n",
    "        mapping_type='Notebook'\n",
    "\n",
    "    elif \"Lakehouse\" in name:\n",
    "        if lakehouse_schema_enabled:\n",
    "            result = run_fab_command(f\"create {workspace_name}.Workspace/{name} -P enableschemas=true\",capture_output=True, silently_continue=True)\n",
    "        else:\n",
    "            result = run_fab_command(f\"create {workspace_name}.Workspace/{name} -P\", capture_output=True, silently_continue=True)\n",
    "        assign_item_description(workspace_name, name)\n",
    "        new_id = get_item_id(workspace_name, name)\n",
    "        mapping_type='Lakehouse'\n",
    "\n",
    "    elif \"DataPipeline\" in name:\n",
    "        print(f\"Replacing connections guid in {workspace['name']}: {name}\")\n",
    "        replace_ids_and_mark_inactive(tmp_path, mapping_table, environment_name, connection_list)\n",
    "        result = run_fab_command(f\"import / {workspace_name}.Workspace/{name} -i {tmp_path} -f {cli_parameter}\",capture_output=True, silently_continue=True)\n",
    "        assign_item_description(workspace_name, name)\n",
    "        new_id = get_item_id(workspace_name, name)\n",
    "        mapping_type='DataPipeline'\n",
    "\n",
    "    elif \"VariableLibrary\" in name:   #Not working yet, import is giving error back\n",
    "        print(f\"Creating VariableLibrary: {name}\")\n",
    "        result = run_fab_command(f\"import / {workspace_name}.Workspace/{name} -i {tmp_path} -f\", capture_output=True, silently_continue=True)\n",
    "        new_id = get_item_id(workspace_name, name)\n",
    "        mapping_type='VariableLibrary'\n",
    "\n",
    "    print(result)\n",
    "    if it:\n",
    "        mapping_table.append({\"Description\": name,\"environment\": environment_name,\"ItemType\": mapping_type, \"old_id\": it[\"id\"],\"new_id\": new_id})\n",
    "\n",
    "\n",
    "    tasks.append({\n",
    "        \"task_name\": f\"Create or Update item Definition {workspace_name} - {name}\",\"task_duration\": int(time() - start),\"status\": result })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67573a49-a80d-4515-882c-4685968d8c2e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Workspace Handling for Cluster Request\n",
    "# -------------------------------\n",
    "\n",
    "def invoke_fabric_request(method, url, payload=None):\n",
    "    \n",
    "    headers = {\n",
    "        \"Authorization\": \"Bearer \" + notebookutils.credentials.getToken(\"pbi\"),\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        session = requests.Session()\n",
    "        retries = Retry(total=3, backoff_factor=5, status_forcelist=[502, 503, 504])\n",
    "        adapter = HTTPAdapter(max_retries=retries)\n",
    "        session.mount('http://', adapter)\n",
    "        session.mount('https://', adapter)\n",
    "\n",
    "        response = session.request(method, url, headers=headers, json=payload, timeout=240)      \n",
    "        if (response.status_code == 202):\n",
    "            operation_id = response.headers.get('x-ms-operation-id')\n",
    "            \n",
    "            # Poll the operation status until it's done - sleep 2 seconds between polls\n",
    "            while True:\n",
    "                operation_state_response = invoke_fabric_api_request(\"get\", f\"operations/{operation_id}\")\n",
    "                operation_state = operation_state_response.json().get(\"status\")\n",
    "\n",
    "                if operation_state in [\"NotStarted\", \"Running\"]:\n",
    "                    time.sleep(2)\n",
    "                elif operation_state == \"Succeeded\":\n",
    "                    response = invoke_fabric_api_request(\"get\", f\"operations/{operation_id}/result\")\n",
    "                    break\n",
    "                else:\n",
    "                    break\n",
    "        \n",
    "        return response\n",
    "\n",
    "    except requests.RequestException as ex:\n",
    "        print(ex)\n",
    "\n",
    "def get_workspace_metadata(workspace_id):\n",
    "    response = invoke_fabric_request(\"get\", f\"{cluster_base_url}metadata/folders/{workspace_id}\")\n",
    "    response.raise_for_status()\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "def set_workspace_icon(workspace_id, base64_png):\n",
    "    if base64_png == \"\":\n",
    "        icon = \"\"\n",
    "    elif base64_png:\n",
    "        icon = f\"data:image/png;base64,{base64_png}\"\n",
    "\n",
    "    if icon is not None:\n",
    "        payload = { \"icon\": icon }\n",
    "        try:\n",
    "            response = invoke_fabric_request(\"put\", f\"{cluster_base_url}metadata/folders/{workspace_id}\", payload)\n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "        except:\n",
    "            print(f\"Could not set icon on workspace id {workspace_id}. Ensure that the user is admin on workspace.\")\n",
    "            return None\n",
    "# -------------------------------\n",
    "# FMD specific Icon functions\n",
    "# Inspiration and the code is coming from Peer, who wrote a blog post about this (https://peerinsights.hashnode.dev/automating-fabric-maintaining-workspace-icon-images) \n",
    "# -------------------------------\n",
    "\n",
    "\n",
    "icon_display_size = \"24\"\n",
    "default_icon = f\"<img height='{icon_display_size}' src='https://content.powerapps.com/resource/powerbiwfe/images/artifact-colored-icons.663f961f5a92d994a109.svg#c_group_workspace_24' />\"\n",
    "\n",
    "def convert_svg_base64_to_png_base64(base64_svg):\n",
    "    svg_data = base64.b64decode(base64_svg)\n",
    "    png_bytes = cairosvg.svg2png(bytestring=svg_data)\n",
    "    base64_png = base64.b64encode(png_bytes).decode()\n",
    "    return base64_png\n",
    "\n",
    "\n",
    "def fill_svg(base64_svg, fill_color):\n",
    "    try:\n",
    "        svg_data = base64.b64decode(base64_svg).decode('utf-8')\n",
    "        modified_svg = re.sub(r'fill=\"[^\"]+\"', f'fill=\"{fill_color}\"', svg_data)\n",
    "        return base64.b64encode(modified_svg.encode('utf-8')).decode('utf-8')\n",
    "    except:\n",
    "        print(\"Failed colorfill of image. Skipping\")\n",
    "\n",
    "def display_workspace_icons(workspaces):\n",
    "    html = \"<table width='100%'>\"\n",
    "    html += \"<th style='text-align:left'>Workspace name</th><th style='text-align:left'>Workspace ID</th><th style='text-align:left; width:100px'>Old icon</th><th style='text-align:left; width:100px'>New icon</th>\"\n",
    "    for workspace in workspaces:\n",
    "        html += f\"<tr><th style='text-align:left'>{workspace.get('displayName')}</td>\"\n",
    "        html += f\"<td style='text-align:left'>{workspace.get('id')}</td>\"\n",
    "        iconUrl = get_workspace_metadata(workspace.get('id')).get('iconUrl')\n",
    "        existing_icon = f\"<img height='{icon_display_size}' src='{cluster_base_url}{iconUrl}'/>\" if iconUrl is not None else default_icon\n",
    "        html += f\"<td style='text-align:left'>{existing_icon}</td>\"\n",
    "        new_icon = workspace.get('icon_base64img')\n",
    "        if workspace.get('icon_base64img',\"\") == \"\":\n",
    "            new_icon = default_icon\n",
    "        else:\n",
    "            new_icon = f\"<img height='{icon_display_size}' src='data:image/png;base64,{new_icon}' />\" if new_icon is not None else existing_icon\n",
    "        html += f\"<td style='text-align:left'>{new_icon}</td></tr>\"\n",
    "    \n",
    "    displayHTML(html)   \n",
    "\n",
    "def add_letter_to_base64_png(base64_png, letter, font_size=20, text_color=\"black\", bold=False):\n",
    "    image_data = base64.b64decode(base64_png)\n",
    "    image = Image.open(io.BytesIO(image_data))\n",
    "\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    \n",
    "    try:\n",
    "        font = ImageFont.truetype(\"arial.ttf\", font_size)\n",
    "    except IOError:\n",
    "        font = ImageFont.truetype(\"DejaVuSans-Bold.ttf\", font_size)\n",
    "        \n",
    "    padding = 0\n",
    "    text_bbox = draw.textbbox((0, 0), letter, font=font)  # Get bounding box\n",
    "    text_width = text_bbox[2] - text_bbox[0]\n",
    "    text_height = text_bbox[3] - text_bbox[1]\n",
    "    \n",
    "    text_x = image.width - text_width - padding\n",
    "    text_y = padding\n",
    "\n",
    "    if bold:\n",
    "        for offset in [(0, 0), (1, 0), (0, 1), (1, 1)]:\n",
    "            draw.text((text_x + offset[0], text_y + offset[1]), letter, font=font, fill=text_color)\n",
    "    else:\n",
    "        draw.text((text_x, text_y), letter, font=font, fill=text_color)\n",
    "\n",
    "    output_buffer = io.BytesIO()\n",
    "    image.save(output_buffer, format=\"PNG\")\n",
    "    new_base64_png = base64.b64encode(output_buffer.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "    return new_base64_png"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33eb5fa4-5bf7-408b-9630-1533ba0d031a",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Load configuration\n",
    "Create  workspace, identity and roles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937feb4e-a67f-486c-ad43-0ae628b71e34",
   "metadata": {
    "jupyter": {
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "base_path = './builtin/'\n",
    "config_path = os.path.join(base_path, 'config/item_config.yaml')\n",
    "\n",
    "with open(config_path, 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "\n",
    "deploy_order_path = os.path.join(base_path, 'config/item_deployment.json')\n",
    "with open(deploy_order_path, 'r') as file:\n",
    "        item_deployment =json.load(file)\n",
    "\n",
    "deploy_order_path = os.path.join(base_path, 'config/item_deployment_code_business_domain.json')\n",
    "with open(deploy_order_path, 'r') as file:\n",
    "        item_deployment_code_business_domain =json.load(file)\n",
    "\n",
    "deploy_order_path = os.path.join(base_path, 'config/item_deployment_data_business_domain.json')\n",
    "with open(deploy_order_path, 'r') as file:\n",
    "        item_deployment_data_business_domain =json.load(file)\n",
    "\n",
    "deploy_order_path = os.path.join(base_path, 'config/sql_deployment.json')\n",
    "with open(deploy_order_path, 'r') as file:\n",
    "        sql_deployment =json.load(file)\n",
    "\n",
    "deploy_order_path = os.path.join(base_path, 'config/data_deployment.json')\n",
    "with open(deploy_order_path, 'r') as file:\n",
    "        data_deployment =json.load(file)\n",
    "\n",
    "deploy_order_path = os.path.join(base_path, 'config/lakehouse_deployment.json')\n",
    "with open(deploy_order_path, 'r') as file:\n",
    "        lakehouse_deployment =json.load(file)\n",
    "\n",
    "deploy_icons_path = os.path.join(base_path, 'config/fabric_icons.xml')\n",
    "\n",
    "# Parse the XML file\n",
    "tree = ET.parse(deploy_icons_path)\n",
    "root = tree.getroot()\n",
    "\n",
    "# Create a dictionary to store icon name and base64\n",
    "fabric_icons_fmd = {}\n",
    "for item in root.findall('icon'):\n",
    "    name = item.find('name').text if item.find('name') is not None else \"No name\"\n",
    "    base64_str = item.find('base64').text if item.find('base64') is not None else \"\"\n",
    "    fabric_icons_fmd[name] = base64_str\n",
    "\n",
    "mapping_table=[]\n",
    "\n",
    "# Get cluster URL for use in metadata endpoints\n",
    "cluster_base_url = get_cluster_url()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3143250a-9c75-4f6a-9f44-78dde9852195",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1d9b38-da94-42a7-a561-99bc7d2d86ac",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Integration Domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b0bf14-919b-4092-9af8-bcb7f6d49c5a",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "if create_domains:\n",
    "    create_fabric_domain(domain_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c962f96-4a74-41ad-a6e0-397622eefbf9",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Deploy workspaces(Code and Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe3bd18-4ed8-4f57-b9c4-b83ca72768f9",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "for environment in environments:\n",
    "    print(f\"--------------------------\")\n",
    "    print(f\"Updating Workspace: {environment['environment_name']}\")\n",
    "    deploy_workspaces(domain_name,workspace=environment['workspaces']['code'], workspace_name=environment['workspaces']['code']['name'], environment_name=environment['environment_name'], old_id=config[\"workspaces\"][\"workspace_code\"], mapping_table=mapping_table, tasks=tasks)\n",
    "    deploy_workspaces(domain_name,workspace=environment['workspaces']['data'], workspace_name=environment['workspaces']['data']['name'], environment_name=environment['environment_name'], old_id=config[\"workspaces\"][\"workspace_data\"], mapping_table=mapping_table, tasks=tasks)\n",
    "    # Append the remaining connections\n",
    "    mapping_table.append({\"Description\": \"CON_FMD_FABRIC_SQL\" ,\"ItemType\": \"connection\", \"environment\": environment['environment_name'],\"old_id\": config['connections'][\"CON_FMD_FABRIC_SQL\"], \"new_id\": environment['connections']['CON_FMD_FABRIC_SQL']})\n",
    "    mapping_table.append({\"Description\": \"CON_FMD_FABRIC_PIPELINES\",\"ItemType\": \"connection\" ,\"environment\": environment['environment_name'] ,\"old_id\": config[\"connections\"][\"CON_FMD_FABRIC_PIPELINES\"], \"new_id\": environment['connections']['CON_FMD_FABRIC_PIPELINES']})\n",
    "    mapping_table.append({\"Description\": \"CON_FMD_ADF_PIPELINES\",\"ItemType\": \"connection\" ,\"environment\": environment['environment_name'] ,\"old_id\": config[\"connections\"][\"CON_FMD_ADF_PIPELINES\"], \"new_id\": environment['connections']['CON_FMD_ADF_PIPELINES']})\n",
    "\n",
    "print(f\"--------------------------\")   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b978e7-12ed-4697-a9fd-dc55a947a44e",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Deploy Workspace(Config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc31d7e-150e-440d-ad2d-95974b74babe",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Updating Workspace: Configuration\")\n",
    "deploy_workspaces(domain_name,workspace=configuration['workspace'], workspace_name=configuration['workspace']['name'], environment_name='config', old_id=config[\"workspaces\"][\"workspace_config\"], mapping_table=mapping_table, tasks=tasks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8be04e-94a6-4f70-ad10-de837a1913b7",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Business Domain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c278cbf-efca-46a6-b8b7-0b1d9a19aba9",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Deploy and create workspaces(Business Domains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d46666-6378-4aa8-81f5-0ea80eea941e",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "for business_domain_name in business_domain_names:\n",
    "    if create_domains:\n",
    "        create_fabric_domain(business_domain_name)\n",
    "\n",
    "    for business_domain in business_domain_deployment:\n",
    "        print(f\"--------------------------\")\n",
    "        print(f\"Updating Workspace: {business_domain['environment_name']}\")\n",
    "        deploy_workspaces(business_domain_name,workspace=business_domain['workspaces']['code'],workspace_name=business_domain_name + ' CODE ('+business_domain['environment_short']+')'+framework_post_fix,  environment_name=business_domain['environment_name'], old_id=config[\"workspaces\"][\"workspace_business_domain_code\"], mapping_table=mapping_table, tasks=tasks)\n",
    "        deploy_workspaces(business_domain_name,workspace=business_domain['workspaces']['data'],workspace_name=business_domain_name + ' DATA ('+business_domain['environment_short']+')'+framework_post_fix,  environment_name=business_domain['environment_name'], old_id=config[\"workspaces\"][\"workspace_business_domain_data\"], mapping_table=mapping_table, tasks=tasks)\n",
    "        deploy_workspaces(business_domain_name,workspace=business_domain['workspaces']['reporting'],workspace_name=business_domain_name + ' REPORTING ('+business_domain['environment_short']+')'+framework_post_fix, environment_name=business_domain['environment_name'], old_id=config[\"workspaces\"][\"workspace_business_domain_reporting\"], mapping_table=mapping_table, tasks=tasks)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7ae39f-a061-42d7-b33a-fa4c546b0574",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Lakehouses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcc14a9-0843-4f05-91c5-5efced447f73",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "for environment in environments:\n",
    "    print(f\"\\n--------------------------\")\n",
    "    print(f\"Processing: {environment['environment_name']}\")\n",
    "    for workspace in [environment['workspaces']['data']]:\n",
    "        exclude = []\n",
    "        for it in lakehouse_deployment:\n",
    "\n",
    "            new_id = None\n",
    "            \n",
    "            name = it[\"name\"]\n",
    "            type = it[\"type\"]\n",
    "\n",
    "            if name in exclude:\n",
    "                continue\n",
    "            deploy_item(workspace,workspace['name'],name,mapping_table, environment['environment_name'],connection_list, tasks, lakehouse_schema_enabled, None,it)\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8715fa24-16d7-4768-82d4-5c03e1a18258",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "for workspace_business_domain in business_domain_deployment:\n",
    "       \n",
    "        for business_domain in business_domain_names:\n",
    "\n",
    "            for workspace in [workspace_business_domain['workspaces']['data']]:\n",
    "                exclude = []\n",
    "                for it in item_deployment_data_business_domain:\n",
    "\n",
    "                    new_id = None\n",
    "                    \n",
    "                    name = it[\"name\"]\n",
    "                    type = it[\"type\"]\n",
    "\n",
    "                    if name in exclude:\n",
    "                        continue\n",
    "                        \n",
    "                    deploy_item(workspace_business_domain,business_domain + ' DATA ('+ workspace_business_domain['environment_short']+')'+framework_post_fix, name,mapping_table, workspace_business_domain['environment_name'],connection_list, tasks, lakehouse_schema_enabled,None,it)\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5a35e1-75e0-40f4-9e81-107f9654bccc",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Fabric Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834a61eb-31dc-48ee-ab6a-08b9cf3439c0",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "for target_item in data_deployment:\n",
    "    if target_item['type'] in ('SQLDatabase','SQLEndpoint'):\n",
    "        target_item['name'] = configuration['DatabaseName']\n",
    "\n",
    "start = time()\n",
    "print(f\"\\n -----\")\n",
    "print(f\" - Processing: workspace {configuration['workspace']['name']}\")\n",
    "workspace_id, status = ensure_workspace_exists(workspace=configuration['workspace'],workspace_name=configuration['workspace']['name'])\n",
    "empty = True\n",
    "   \n",
    "target_items = get_items(workspace_id)\n",
    "target_items = json.loads(target_items)[\"text\"]\n",
    "\n",
    "for deployment_item in data_deployment:\n",
    "    for target_item in target_items['value']:\n",
    "\n",
    "        if target_item['displayName'] == deployment_item['name'] \\\n",
    "                and target_item['type'] == deployment_item['type']:\n",
    "            print(f\" - Skip existing: {deployment_item['name']}, {deployment_item['type']}, {target_item['id']}\")\n",
    "            break\n",
    "    else:\n",
    "        \n",
    "        print(f\" - Creating: {deployment_item['name']} {deployment_item['type']} in workspace:{workspace_id} \")\n",
    "        item = deployment_item.copy()\n",
    "        \n",
    "        if empty:\n",
    "            if item.get('definition'):\n",
    "                print(f\" - Dropping definition\")\n",
    "                item.pop('definition')\n",
    "\n",
    "        # Construct the JSON payload\n",
    "        payload = json.dumps({\"displayName\": deployment_item['name'], \"Description\": \"Note: This item was initially generated by the FMD Framework. Any modifications may introduce breaking changes. For further details, please refer to the documentation at https://github.com/edkreuk/FMD_FRAMEWORK.\"})\n",
    "        if deployment_item['type'] in ('SQLDatabase'):\n",
    "            try:\n",
    "                raw_response = run_fab_command(f\"api -X post workspaces/{workspace_id}/SQLDatabases -i '{payload}'\",capture_output=True, silently_continue=True)\n",
    "                response = json.loads(raw_response)\n",
    "\n",
    "                if isinstance(response, dict) and response.get(\"status_code\", 200) >= 400:\n",
    "                    print(f\"{response.get('status_code')=}\\n{response.get('reason')=}\\n{response.get('text')=}\\n{url=}\\n{payload=}\\n{payloadtype=}\")\n",
    "                    if response.get(\"errorCode\"):\n",
    "                        print(f\"{response['errorCode']}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}. Retrying in 60 seconds...\")\n",
    "                sleep(60)\n",
    "                raw_response = run_fab_command(\n",
    "                    f\"api -X post workspaces/{workspace_id}/SQLDatabases -i '{payload}'\",\n",
    "                    capture_output=True,\n",
    "                    silently_continue=True\n",
    "                )\n",
    "                response = json.loads(raw_response)\n",
    "\n",
    "            #wait so SQL Database can be deployed and we can pick up the Guid after\n",
    "            sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf50a36-141d-4096-b169-d9b235678049",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Get Fabric database configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837c87ad-dbad-443d-84e8-08adf6e7dd21",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "for target_item in data_deployment:\n",
    "    if target_item['type'] in ('SQLDatabase','SQLEndpoint'):\n",
    "        target_item['name'] = configuration['DatabaseName']\n",
    "   \n",
    "start = time()\n",
    "print(f\"\\n -----\")\n",
    "print(f\" - Processing: workspace {configuration['workspace']['name']}\")\n",
    "workspace_id, status = ensure_workspace_exists(configuration['workspace'],configuration['workspace']['name'])\n",
    "empty = True\n",
    "\n",
    "   \n",
    "target_items = get_items(workspace_id)\n",
    "target_items = json.loads(target_items)[\"text\"]\n",
    "for deployment_item in data_deployment:\n",
    "    for target_item in target_items['value']:\n",
    "\n",
    "        print(f\" - Updating mapping table: {deployment_item['name']} {deployment_item['type']} \")\n",
    "  \n",
    "        if deployment_item['type'] in ('SQLDatabase'):\n",
    "            if deployment_item.get('endpoint', '') != '':\n",
    "                return_item =  run_fab_command(f\"api -X get workspaces/{workspace_id}/SQLDatabases/{target_item['id']}\" , capture_output = True, silently_continue= True)\n",
    "                return_item = json.loads(return_item)[\"text\"]\n",
    "                if deployment_item['type'] in ('SQLDatabase'):\n",
    "                    if return_item.get(\"properties\", {}).get(\"serverFqdn\", \"\") != '':\n",
    "                        deployment_item[\"connectionString\"] = return_item[\"properties\"][\"serverFqdn\"].replace(',1433', '')\n",
    "                        mapping_table.append({\"Description\":deployment_item['name'] , \"ItemType\": \"Database\", \"environment\": 'config', \"old_id\": config[\"database\"][\"id\"], \"new_id\": target_item['id']})\n",
    "                        mapping_table.append({\"Description\":deployment_item['name'] , \"ItemType\": \"Endpoint\", \"environment\": 'config', \"old_id\": deployment_item[\"endpoint\"], \"new_id\": deployment_item[\"connectionString\"]})\n",
    "                        mapping_table.append({\"Description\":deployment_item['name'] , \"ItemType\": \"Database\", \"environment\": 'config', \"old_id\": deployment_item[\"database\"], \"new_id\": return_item[\"properties\"][\"databaseName\"]})\n",
    "                        mapping_table.append({\"Description\":deployment_item['name'] , \"ItemType\": \"displayName\", \"environment\": 'config', \"old_id\": config[\"database\"][\"displayName\"], \"new_id\": return_item[\"displayName\"]})\n",
    "                        tasks.append({\"task_name\":f\"Update item  Definition  {configuration['workspace']['name']} - {name}\", \"task_duration\": int(time() - start), \"status\": \"success\"})\n",
    "                    if return_item.get(\"properties\", {}).get(\"databaseName\", \"\") != '':\n",
    "                        deployment_item[\"databaseName\"] = return_item[\"properties\"][\"databaseName\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cc05d7-a761-452b-8c24-32f3b3d24c02",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Deploy Workspace Icons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08774b7-57be-4e62-85fe-c218d3b191e6",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "seen = set()\n",
    "workspaces = []\n",
    "\n",
    "for item in mapping_table:\n",
    "    if item['ItemType'] == 'Workspace':\n",
    "        key = (item['Description'], item['new_id'])\n",
    "        if key not in seen:\n",
    "            seen.add(key)\n",
    "            workspaces.append({'displayName': item['Description'], 'id': item['new_id']})\n",
    "fabric_icons = fabric_icons_fmd \n",
    "\n",
    "for workspace in workspaces:\n",
    "    display_name = workspace['displayName'].lower()\n",
    "    \n",
    "    # Check if it's a \"business domain\" workspace\n",
    "    is_business_domain = any(sub.lower() in display_name for sub in business_domain_names) and 'data' in display_name\n",
    "    display_name = 'business_domain' if is_business_domain else display_name\n",
    "\n",
    "    # Assign icon\n",
    "    for icon_key, icon_value in workspace_icon_def['icons'].items():\n",
    "        if icon_key in display_name:\n",
    "            workspace[\"icon\"] = icon_value\n",
    "            workspace_icon = fabric_icons.get(icon_value)\n",
    "            break\n",
    "    else:\n",
    "        workspace[\"icon\"] = None\n",
    "        workspace_icon = None\n",
    "\n",
    "    workspace[\"icon_base64img\"] = workspace_icon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb4c7c1-3114-4976-8a8e-5ed065fcd1c3",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "if assign_icons:\n",
    "    # Dry run - Display pre and post icons based on specified workspace filters and workspace icon definition. Will NOT update any icons!\n",
    "    display_workspace_icons(workspaces)\n",
    "    sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2975c82e-fc9e-4016-9a82-fb335a1ee2b4",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "if assign_icons:\n",
    "    for workspace in workspaces:\n",
    "            set_workspace_icon(workspace.get('id'), workspace.get('icon_base64img'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efaf53a-7f5d-4136-915c-6a3dd4ea5513",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Deploy Items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296baf6d-620f-4d13-9bf9-85506b864b41",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "for environment in environments:\n",
    "    print(f\"\\n--------------------------\")\n",
    "    print(f\"Processing: {environment['environment_name']}\")\n",
    "    for workspace in [environment['workspaces']['code']]:\n",
    "        exclude = []\n",
    "        for it in item_deployment:\n",
    "\n",
    "            new_id = None\n",
    "            \n",
    "            name = it[\"name\"]\n",
    "            type = it[\"type\"]\n",
    "\n",
    "            if name in exclude:\n",
    "                continue\n",
    "            deploy_item(workspace,workspace['name'],name,mapping_table,environment['environment_name'],connection_list, tasks, lakehouse_schema_enabled,None,it)\n",
    "            \n",
    "            for child in it.get(\"children\",[]):\n",
    "                child_name = child[\"name\"]\n",
    "                deploy_item(workspace,workspace['name'],name,mapping_table, environment['environment_name'],connection_list, tasks, lakehouse_schema_enabled,child_name,child) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c321c689-995e-4943-ad05-db9db2d885f8",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "for workspace_business_domain in business_domain_deployment:\n",
    "    for business_domain_name in business_domain_names:\n",
    "        for workspace in [workspace_business_domain['workspaces']['code']]:\n",
    "                exclude = []\n",
    "                for it in item_deployment_code_business_domain:\n",
    "\n",
    "                    new_id = None\n",
    "                    \n",
    "                    name = it[\"name\"]\n",
    "                    type = it[\"type\"]\n",
    "\n",
    "                    if name in exclude:\n",
    "                        continue\n",
    "                        \n",
    "                    deploy_item(workspace_business_domain,business_domain_name + ' CODE ('+ workspace_business_domain['environment_short']+')'+framework_post_fix, name,mapping_table, workspace_business_domain['environment_name'],connection_list, tasks, lakehouse_schema_enabled,None,it)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc252ff-6f50-41f9-9c6d-165300837bb6",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Create SQL deployment Manifest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1031bbb1-ae9a-4a26-8e14-e942a4fca2f8",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Add Connection to Fabric Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438153da-a957-4c48-9394-3cfc7c8aa014",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "custom_sql_deployment = {\"queries_stored_procedures\": []}\n",
    "for connection in connections['value']:\n",
    "    \n",
    "    display_name = connection.get('displayName', '')\n",
    "    if display_name and display_name.startswith('CON_FMD'):\n",
    "        connection_type = connection.get('connectionDetails', {}).get('type', 'Unknown')\n",
    "        connection_id = connection.get('id')\n",
    "      \n",
    "        exec_statement = (\n",
    "            f\"EXEC [integration].[sp_UpsertConnection] \"\n",
    "            f\"@ConnectionGuid = \\\"{connection_id}\\\", \"\n",
    "            f\"@Name = \\\"{display_name}\\\", \"\n",
    "            f\"@Type = \\\"{connection_type}\\\", \"\n",
    "            f\"@IsActive = 1\"\n",
    "        )\n",
    "        custom_sql_deployment[\"queries_stored_procedures\"].append(exec_statement)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed267d7-4aca-445c-9189-243307a972e6",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Add Workspaces to Fabric Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee6f1cd-8aaa-4c94-be39-6721312e6cde",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "#add all created workspace to database\n",
    "unique_items = {}\n",
    "for item in mapping_table:\n",
    "    if item.get(\"ItemType\") == \"Workspace\":\n",
    "        unique_items[item[\"new_id\"]] = item[\"Description\"]\n",
    "\n",
    "# Convert to list of tuples or dicts\n",
    "workspaces = [{\"Description\": desc, \"new_id\": nid} for nid, desc in unique_items.items()]\n",
    "\n",
    "for workspace in workspaces:\n",
    "    print(f'EXEC [integration].[sp_UpsertWorkspace](@WorkspaceId = \"{workspace[\"new_id\"]}\" ,@Name = \"{workspace[\"Description\"]}\")')\n",
    "    custom_sql_deployment[\"queries_stored_procedures\"].append(f'EXEC [integration].[sp_UpsertWorkspace] @WorkspaceId = \"{workspace[\"new_id\"]}\", @Name = \"{workspace[\"Description\"]}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aacd228-4711-4836-a45d-0818ea6dc14e",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Add Data Pipelines to Fabric Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa8751f-ddad-47ed-9de2-1fbe2530d76f",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "for environment in environments:\n",
    "    result = run_fab_command(f\"api -X get workspaces/{environment['workspaces']['code']['id']}/items\", capture_output=True, silently_continue=True)\n",
    "    existing_items = json.loads(result)['text']\n",
    "    for item in existing_items.get('value', []):\n",
    "        if item['type'] == 'DataPipeline':\n",
    "            print(f'EXEC [integration].[sp_UpsertPipeline] @PipelineId = \"{item[\"id\"]}\", @WorkspaceId = \"{environment[\"workspaces\"][\"code\"][\"id\"]}\" ,@Name = \"{item[\"displayName\"]}\"')\n",
    "            custom_sql_deployment[\"queries_stored_procedures\"].append(f'EXEC [integration].[sp_UpsertPipeline] @PipelineId = \"{item[\"id\"]}\", @WorkspaceId = \"{environment[\"workspaces\"][\"data\"][\"id\"]}\" ,@Name = \"{item[\"displayName\"]}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9c1eb2-b4a4-448b-82a8-fdeef790d1e8",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "for workspace_business_domain in business_domain_deployment:\n",
    "    for business_domain_name in business_domain_names:\n",
    "        for workspace in [workspace_business_domain['workspaces']['code']]:\n",
    "                workspace_id = get_workspace_id_by_name(business_domain_name + ' CODE ('+ workspace_business_domain['environment_short']+')'+framework_post_fix)\n",
    "                result = run_fab_command(f\"api -X get workspaces/{workspace_id}/items\", capture_output=True, silently_continue=True)\n",
    "                existing_items = json.loads(result)['text']\n",
    "                for item in existing_items.get('value', []):\n",
    "                    if item['type'] == 'DataPipeline':\n",
    "                        print(f'EXEC [integration].[sp_UpsertPipeline] @PipelineId = \"{item[\"id\"]}\", @WorkspaceId = \"{workspace_id}\" ,@Name = \"{item[\"displayName\"]}\"')\n",
    "                        custom_sql_deployment[\"queries_stored_procedures\"].append(f'EXEC [integration].[sp_UpsertPipeline] @PipelineId = \"{item[\"id\"]}\", @WorkspaceId = \"{workspace_id}\" ,@Name = \"{item[\"displayName\"]}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4867745a-f0e7-4a9a-b326-5609b1b5bb3a",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Add Lakehouses to Fabric Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc60a4a-1e3d-4a57-b2f7-2a8cb23115c6",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "for environment in environments:\n",
    "    result = run_fab_command(f\"api -X get workspaces/{environment['workspaces']['data']['id']}/items\", capture_output=True, silently_continue=True)\n",
    "    existing_items = json.loads(result)['text']\n",
    "    for item in existing_items.get('value', []):\n",
    "        if item['type'] == 'Lakehouse':\n",
    "            print(f'EXEC [integration].[sp_UpsertLakehouse] @LakehouseId = \"{item[\"id\"]}\", @WorkspaceId = \"{environment[\"workspaces\"][\"data\"][\"id\"]}\" ,@Name = \"{item[\"displayName\"]}\"')\n",
    "            custom_sql_deployment[\"queries_stored_procedures\"].append(f'EXEC [integration].[sp_UpsertLakehouse] @LakehouseId = \"{item[\"id\"]}\", @WorkspaceId = \"{environment[\"workspaces\"][\"data\"][\"id\"]}\" ,@Name = \"{item[\"displayName\"]}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3144899e-74f6-4eb0-a203-525437d8c127",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "for workspace_business_domain in business_domain_deployment:\n",
    "    for business_domain_name in business_domain_names:\n",
    "        for workspace in [workspace_business_domain['workspaces']['data']]:\n",
    "                workspace_id = get_workspace_id_by_name(business_domain_name + ' DATA ('+ workspace_business_domain['environment_short']+')'+framework_post_fix)\n",
    "                result = run_fab_command(f\"api -X get workspaces/{workspace_id}/items\", capture_output=True, silently_continue=True)\n",
    "                existing_items = json.loads(result)['text']\n",
    "                for item in existing_items.get('value', []):\n",
    "                    if item['type'] == 'Lakehouse':\n",
    "                        print(f'EXEC [integration].[sp_UpsertLakehouse] @LakehouseId = \"{item[\"id\"]}\", @WorkspaceId = \"{workspace_id}\" ,@Name = \"{item[\"displayName\"]}\"')\n",
    "                        custom_sql_deployment[\"queries_stored_procedures\"].append(f'EXEC [integration].[sp_UpsertLakehouse] @LakehouseId = \"{item[\"id\"]}\", @WorkspaceId = \"{workspace_id}\" ,@Name = \"{item[\"displayName\"]}\"')\n",
    "                       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d60c485-1f24-4dbc-807b-45a71e14cf55",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Add Demo data for testing to Fabric Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a476a2-d34d-4540-ad24-439d447c5289",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "if load_demo_data:  \n",
    "    demo_sql_deployment = {\"queries_stored_procedures\": []}\n",
    "    demo_sql_deployment[\"queries_stored_procedures\"].append(f'EXEC [integration].[sp_UpsertConnection] @ConnectionGuid = \"00000000-0000-0000-0000-000000000000\", @Name = \"CON_FMD_ONELAKE\", @Type = \"ONELAKE\", @IsActive = 1')\n",
    "    demo_sql_deployment[\"queries_stored_procedures\"].append(\"\"\"\n",
    "        DECLARE @DataSourceIdInternal INT = (SELECT DataSourceId FROM integration.DataSource WHERE Name = 'LH_DATA_LANDINGZONE' and Type='ONELAKE_TABLES_01')\n",
    "        DECLARE @ConnectionIdInternal INT = (SELECT ConnectionId FROM integration.Connection WHERE ConnectionGuid = '00000000-0000-0000-0000-000000000000')\n",
    "        EXECUTE [integration].[sp_UpsertDataSource] \n",
    "            @ConnectionId = @ConnectionIdInternal\n",
    "            ,@DataSourceId = @DataSourceIdInternal\n",
    "            ,@Name = 'LH_DATA_LANDINGZONE'\n",
    "            ,@Namespace = 'ONELAKE'\n",
    "            ,@Type = 'ONELAKE_TABLES_01'\n",
    "            ,@Description = 'ONELAKE_TABLES'\n",
    "            ,@IsActive = 1\n",
    "    \"\"\")\n",
    "    demo_sql_deployment[\"queries_stored_procedures\"].append(\"\"\"\n",
    "        DECLARE @DataSourceIdInternal INT = (SELECT DataSourceId FROM integration.DataSource WHERE Name = 'LH_DATA_LANDINGZONE' and Type ='ONELAKE_FILES_01')\n",
    "        DECLARE @ConnectionIdInternal INT = (SELECT ConnectionId FROM integration.Connection WHERE ConnectionGuid = '00000000-0000-0000-0000-000000000000')\n",
    "        EXECUTE [integration].[sp_UpsertDataSource] \n",
    "            @ConnectionId = @ConnectionIdInternal\n",
    "            ,@DataSourceId = @DataSourceIdInternal\n",
    "            ,@Name = 'LH_DATA_LANDINGZONE'\n",
    "            ,@Namespace = 'ONELAKE'\n",
    "            ,@Type = 'ONELAKE_FILES_01'\n",
    "            ,@Description = 'ONELAKE_FILES'\n",
    "            ,@IsActive = 1\n",
    "    \"\"\")\n",
    "    demo_sql_deployment[\"queries_stored_procedures\"].append(\"\"\"\n",
    "        DECLARE @LandingzoneEntityIdInternal INT = (SELECT LandingzoneEntityId FROM integration.LandingzoneEntity WHERE SourceSchema = 'in' and SourceName = 'customer')\n",
    "        DECLARE @DataSourceIdInternal INT = (SELECT DataSourceId FROM integration.DataSource WHERE Name = 'LH_DATA_LANDINGZONE' and Type='ONELAKE_TABLES_01')\n",
    "        DECLARE @LakehouseIdInternal INT = (SELECT top 1 LakehouseId FROM integration.Lakehouse WHERE Name = 'LH_DATA_LANDINGZONE')\n",
    "        EXECUTE [integration].[sp_UpsertLandingzoneEntity] \n",
    "            @LandingzoneEntityId = @LandingzoneEntityIdInternal\n",
    "            ,@DataSourceId = @DataSourceIdInternal\n",
    "            ,@LakehouseId = @LakehouseIdInternal\n",
    "            ,@SourceSchema = 'in'\n",
    "            ,@SourceName = 'customer'\n",
    "            ,@SourceCustomSelect = ''\n",
    "            ,@FileName = 'customer'\n",
    "            ,@FilePath = 'fmd'\n",
    "            ,@FileType = 'parquet'\n",
    "            ,@IsIncremental = 0\n",
    "            ,@IsIncrementalColumn = ''\n",
    "            ,@IsActive = 1\n",
    "    \"\"\")\n",
    "    demo_sql_deployment[\"queries_stored_procedures\"].append(\"\"\"\n",
    "        DECLARE @LandingzoneEntityIdInternal INT = (SELECT LandingzoneEntityId FROM integration.LandingzoneEntity WHERE SourceSchema = 'in' and SourceName = 'customer')\n",
    "        DECLARE @BronzeLayerEntityIdInternal INT = (SELECT BronzeLayerEntityId FROM integration.BronzeLayerEntity WHERE [Schema] = 'in' and [Name] = 'customer')\n",
    "        DECLARE @LakehouseIdInternal INT = (SELECT top 1 LakehouseId FROM integration.Lakehouse WHERE Name = 'LH_BRONZE_LAYER')\n",
    "        EXECUTE [integration].[sp_UpsertBronzeLayerEntity] \n",
    "            @BronzeLayerEntityId = @BronzeLayerEntityIdInternal\n",
    "            ,@LandingzoneEntityId = @LandingzoneEntityIdInternal\n",
    "            ,@Schema = 'in'\n",
    "            ,@Name = 'customer'\n",
    "            ,@FileType = 'Delta'\n",
    "            ,@LakehouseId = @LakehouseIdInternal\n",
    "            ,@PrimaryKeys = 'CustomerId'\n",
    "            ,@IsActive = 1\n",
    "    \"\"\")\n",
    "    demo_sql_deployment[\"queries_stored_procedures\"].append(\"\"\"\n",
    "        DECLARE @BronzeLayerEntityIdInternal INT = (SELECT BronzeLayerEntityId FROM integration.BronzeLayerEntity WHERE [Schema] = 'in' and [Name] = 'customer')\n",
    "        DECLARE @SilverLayerEntityIdInternal INT = (SELECT SilverLayerEntityId FROM integration.SilverLayerEntity WHERE [Schema] = 'in' and [Name] = 'customer')\n",
    "        DECLARE @LakehouseIdInternal INT = (SELECT top 1 LakehouseId FROM integration.Lakehouse WHERE Name = 'LH_SILVER_LAYER')\n",
    "        EXECUTE [integration].[sp_UpsertSilverLayerEntity] \n",
    "            @SilverLayerEntityId = @SilverLayerEntityIdInternal\n",
    "            ,@BronzeLayerEntityId = @BronzeLayerEntityIdInternal\n",
    "            ,@LakehouseId = @LakehouseIdInternal\n",
    "            ,@Name = 'customer'\n",
    "            ,@Schema = 'in'\n",
    "            ,@FileType = 'delta'\n",
    "            ,@IsActive = 1\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e3bfba-b56e-47c7-a1e5-c764c2c3e622",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Deploy SQL Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb6c1ec-7fac-46d6-812a-50c00e413c5e",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "for target_item in data_deployment:\n",
    "    if isinstance(target_item, dict) and target_item.get('type') == 'SQLDatabase':\n",
    "        connstring = target_item.get(\"connectionString\")\n",
    "        database = target_item.get('databaseName')\n",
    "\n",
    "try:\n",
    "    i = 0\n",
    "\n",
    "    token = notebookutils.credentials.getToken('pbi').encode('utf-16-le')\n",
    "    token_struct = struct.pack(f'<I{len(token)}s', len(token), token)\n",
    "\n",
    "    print(f\"DRIVER={driver};SERVER={connstring};PORT=1433;DATABASE={database};\")\n",
    "    connection = pyodbc.connect(f\"DRIVER={driver};SERVER={connstring};PORT=1433;DATABASE={database};\", attrs_before={1256:token_struct}, timeout=12)\n",
    "\n",
    "    with connection.cursor() as cursor:\n",
    "        cursor.execute(\"SELECT 1\")  # Execute the warm-up query (a simple query like 'SELECT 1' can be used)\n",
    "        cursor.fetchone()\n",
    "        connection.timeout = 10  # Setting a lower timeout for subsequent queries\n",
    "    for it in sql_deployment:\n",
    "        for i, query in enumerate(it[\"queries_schemas\"]):\n",
    "            print(f' - execute \"{query}\"')\n",
    "            cursor.execute(query)\n",
    "            cursor.commit()\n",
    "        for i, query in enumerate(it[\"queries_tables\"]):\n",
    "            print(f' - execute \"{query}\"')\n",
    "            cursor.execute(query)\n",
    "            cursor.commit()\n",
    "        for i, query in enumerate(it[\"queries_views\"]):\n",
    "            print(f' - execute \"{query}\"')\n",
    "            cursor.execute(query)\n",
    "            cursor.commit()\n",
    "        for i, query in enumerate(it[\"queries_stored_procedures\"]):\n",
    "            print(f' - execute \"{query}\"')\n",
    "            cursor.execute(query)\n",
    "            cursor.commit()\n",
    "        for i, query in enumerate(it[\"queries_logging\"]):\n",
    "            print(f' - execute \"{query}\"')\n",
    "            cursor.execute(query)\n",
    "            cursor.commit()\n",
    "        for i, query in enumerate(custom_sql_deployment[\"queries_stored_procedures\"]):\n",
    "            print(f' - execute \"{query}\"')\n",
    "            cursor.execute(query)\n",
    "            cursor.commit()\n",
    "        for i, query in enumerate(demo_sql_deployment[\"queries_stored_procedures\"]):\n",
    "            print(f' - execute \"{query}\"')\n",
    "            cursor.execute(query)\n",
    "            cursor.commit()\n",
    "\n",
    "    tasks.append({\"task_name\":f\"{workspace.get('displayName')} {database} query {i}\", \"task_duration\": 1, \"status\": f\"success\"})\n",
    "except pyodbc.OperationalError as e:\n",
    "    print(e) \n",
    "    tasks.append({\"task_name\":f\"{workspace.get('displayName')} {database} query {i}\", \"task_duration\": 1, \"status\": f\"pyodbc failed: {e}\"})\n",
    "except Exception as e:\n",
    "    print(e) \n",
    "    tasks.append({\"task_name\":f\"{workspace.get('displayName')} {database} query {i}\", \"task_duration\": 1, \"status\": f\"failed: {e}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c61b1ee-8f24-4760-b2a6-f207bb10eea4",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "display(tasks)"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "lakehouse": {}
  },
  "kernel_info": {
   "jupyter_kernel_name": "python3.11",
   "name": "jupyter"
  },
  "kernelspec": {
   "display_name": "Jupyter",
   "language": "Jupyter",
   "name": "jupyter"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "jupyter_python",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
