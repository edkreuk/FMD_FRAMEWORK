{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce6a663d-0e92-4d07-99e6-365c48b9c726",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "![FMD_Overview](https://github.com/edkreuk/FMD_FRAMEWORK/blob/main/Images/FMD_Overview.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b167cef1-f936-4ca4-b2b2-22a88592ac11",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "%pip install ms-fabric-cli --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f028883-3c89-4adf-8da2-9a1d7f5b979d",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "from time import sleep, time\n",
    "import json\n",
    "from zipfile import ZipFile \n",
    "import shutil\n",
    "import re\n",
    "import requests\n",
    "import zipfile\n",
    "from io import BytesIO\n",
    "import yaml\n",
    "#import sempy.fabric as fabric\n",
    "import struct\n",
    "import pyodbc\n",
    "import notebookutils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ed7375-2e5f-4b5a-ae9f-672b337ddd94",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Configuration and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ada47fa-6823-4cdc-bebe-3f573f4f2bd1",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "load_demo_data_data= True         # Set to True if you want to load the demo data, otherwise set to False\n",
    "lakehouse_schema_enabled = True    # Set to True if you want to use the lakehouse schema, otherwise set to False\n",
    "\n",
    "driver = '{ODBC Driver 18 for SQL Server}' # Change this if you use a different driver\n",
    "# target connections guid \n",
    "# add the correct id for every connections\n",
    "# change the capacity id\n",
    "# change the workspace roles\n",
    "\n",
    "FrameworkName= 'DEMO' # max 6 characters for better visibility, no spaces and the end of the name\n",
    "datamesh_domains= ['FINANCE','HR']\n",
    "\n",
    "\n",
    "capacity_name_dvlm = \"Trial-Erwin\"                  # Which capacity will be used for these workspaces in development\n",
    "capacity_name_prod = 'Trial-Erwin'                  # Which capacity will be used for these workspaces in production\n",
    "capacity_name_config = 'Trial-Erwin'                #'Which capacity will be used for this workspace for the FMD Database\n",
    "\n",
    "\n",
    "## original data\n",
    "\n",
    "workspace_roles_code = [ # Keep emtpy [] if you only want to assign this to your personal account\n",
    "                    {\n",
    "                       \"principal\": {\n",
    "                            \"id\": '86897900-8de5-4894-ae41-1b4d1642acda',\n",
    "                            \"displayName\": \"sg-fmd-fabric-admin\", # Name of the group or user to assign the role to\n",
    "                            \"type\": \"Group\"\n",
    "                        },\n",
    "                        \"role\": \"admin\"  #(choose from 'admin', 'member', 'contributor', 'viewer')\n",
    "\n",
    "                        }\n",
    "                    ]\n",
    "workspace_roles_data = [ # Keep emtpy [] if you only want to assign this to your personal account\n",
    "                    {\n",
    "                       \"principal\": {\n",
    "                            \"id\": '86897900-8de5-4894-ae41-1b4d1642acda',\n",
    "                            \"displayName\": \"sg-fmd-fabric-admin\", # Name of the group or user to assign the role to\n",
    "                            \"type\": \"Group\"\n",
    "                        },\n",
    "                        \"role\": \"admin\"  #(choose from 'admin', 'member', 'contributor', 'viewer')\n",
    "\n",
    "\n",
    "                        }\n",
    "                    ]\n",
    "workspace_roles_gold = [ # Keep emtpy [] if you only want to assign this to your personal account\n",
    "                    {\n",
    "                      \"principal\": {\n",
    "                            \"id\": '86897900-8de5-4894-ae41-1b4d1642acda',\n",
    "                            \"displayName\": \"sg-fmd-fabric-admin\", # Name of the group or user to assign the role to\n",
    "                            \"type\": \"Group\"\n",
    "                        },\n",
    "                        \"role\": \"admin\"  #(choose from 'admin', 'member', 'contributor', 'viewer')\n",
    "\n",
    "\n",
    "                        }\n",
    "                    ]\n",
    "workspace_roles_reporting = [ # Keep emtpy [] if you only want to assign this to your personal account\n",
    "                    {\n",
    "                        \"principal\": {\n",
    "                            \"id\": '86897900-8de5-4894-ae41-1b4d1642acda',\n",
    "                            \"displayName\": \"sg-fmd-fabric-admin\", # Name of the group or user to assign the role to\n",
    "                            \"type\": \"Group\"\n",
    "                        },\n",
    "                        \"role\": \"admin\"  #(choose from 'admin', 'member', 'contributor', 'viewer')\n",
    "\n",
    "                        }\n",
    "                    ]\n",
    "\n",
    "configuration = {\n",
    "                    'workspace': {\n",
    "                        'name' : FrameworkName + ' CONFIG FMD',             # Name of target workspace\n",
    "                        'roles' : workspace_roles_data,                          # Roles to assign to the workspace\n",
    "                        'capacity_name' : capacity_name_config              # Name of target capacity for the configuration workspace\n",
    "                    },\n",
    "                       'DatabaseName' : 'SQL_'+FrameworkName+'_FRAMEWORK'   # Name of target configuration SQL Database\n",
    "}\n",
    "environments = [\n",
    "                    {\n",
    "                        'environment_name' : 'development',                 # Name of target environment\n",
    "                        'workspaces': {\n",
    "                            'data' : {\n",
    "                                'name' : FrameworkName + ' DATA (D) FMD',   # Name of target code workspace for development\n",
    "                                'roles' : workspace_roles_data,                  # Roles to assign to the workspace\n",
    "                                'capacity_name' : capacity_name_dvlm        # Name of target data workspace for development\n",
    "                            },\n",
    "                            'code' : {\n",
    "                                'name' : FrameworkName + ' CODE (D) FMD',   # Name of target data workspace for development\n",
    "                                'roles' : workspace_roles_code,                  # Roles to assign to the workspace\n",
    "                                'capacity_name' : capacity_name_dvlm        # Name of target code workspace for development\n",
    "                            },\n",
    "                            'gold' : {\n",
    "                             #   'name' : FrameworkName +' '+ datamesh_domains  + ' DATA (D) FMD',   # Name of target gold workspace for development\n",
    "                                'roles' : workspace_roles_data,                  # Roles to assign to the workspace\n",
    "                                'capacity_name' : capacity_name_dvlm        # Name of target code workspace for development\n",
    "                            },\n",
    "                                'reporting' : {\n",
    "                             #   'name' : FrameworkName +' '+ datamesh_domains  + ' REPORTING (D) FMD',   # Name of target reporting workspace for development\n",
    "                                'roles' : workspace_roles_reporting,                  # Roles to assign to the workspace\n",
    "                                'capacity_name' : capacity_name_dvlm        # Name of target code workspace for development\n",
    "                            }\n",
    "                        },\n",
    "                        'connections' : {\n",
    "                            'CON_FMD_FABRIC_SQL' : '372237f9-709a-48f8-8fb2-ce06940c990e',          # Required Guid to the Fabric SQL connection\n",
    "                            'CON_FMD_FABRIC_PIPELINES' : '6d8146c6-a438-47df-94e2-540c552eb6d7',    # Required Guid to the Fabric datapipelines connection\n",
    "                            'CON_FMD_ADF_PIPELINES' : '02e107b8-e97e-4b00-a28c-668cf9ce3d9a'        #'10000000-0000-0000-0000-000000000000'\n",
    "\n",
    "                        }\n",
    "                    },\n",
    "                    {\n",
    "                        'environment_name' : 'production',                  # Name of target environment\n",
    "                        'workspaces': {\n",
    "                            'data' : {\n",
    "                                'name' : FrameworkName + ' DATA (P) FMD',   # Name of target data workspace for production\n",
    "                                'roles' : workspace_roles_data,                  # Roles to assign to the workspace\n",
    "                                'capacity_name' : capacity_name_prod        # Name of target data workspace for production   \n",
    "                            },\n",
    "                            'code' : {\n",
    "                                'name' : FrameworkName + ' CODE (P) FMD',   # Name of target code workspace for production\n",
    "                                'roles' : workspace_roles_code,                  # Roles to assign to the workspace\n",
    "                                'capacity_name' : capacity_name_prod        # Name of target code workspace for production\n",
    "                            },                            \n",
    "                            'gold' : {\n",
    "                           #     'name' : FrameworkName +' '+ datamesh_domain + ' DATA (P) FMD',   # Name of target gold workspace for development\n",
    "                                'roles' : workspace_roles_gold,                  # Roles to assign to the workspace\n",
    "                                'capacity_name' : capacity_name_dvlm        # Name of target code workspace for development\n",
    "                            },\n",
    "                            'reporting' : {\n",
    "                           #     'name' : FrameworkName +' '+ datamesh_domain + ' REPORTING (P) FMD',   # Name of target reporting workspace for development\n",
    "                                'roles' : workspace_roles_reporting,                  # Roles to assign to the workspace\n",
    "                                'capacity_name' : capacity_name_prod        # Name of target code workspace for development\n",
    "                            }\n",
    "                        },\n",
    "                        'connections' : {\n",
    "                            'CON_FMD_FABRIC_SQL' : '372237f9-709a-48f8-8fb2-ce06940c990e',          # Required Guid to the Fabric SQL connection\n",
    "                            'CON_FMD_FABRIC_PIPELINES' : '6d8146c6-a438-47df-94e2-540c552eb6d7',    # Required Guid to the Fabric datapipelines connection\n",
    "                            'CON_FMD_ADF_PIPELINES' : '02e107b8-e97e-4b00-a28c-668cf9ce3d9a'        #'10000000-0000-0000-0000-000000000000'\n",
    "\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "\n",
    "domain_deployment = [\n",
    "                    {\n",
    "                        'environment_name' : 'development',                 # Name of target environment\n",
    "                        'environment_short' : 'D',                          # Short of target environment\n",
    "                        'workspaces': {\n",
    "                         \n",
    "                            'gold' : {\n",
    "                             #   'name' : FrameworkName +' '+ datamesh_domains  + ' DATA (D) FMD',   # Name of target gold workspace for development\n",
    "                                'roles' : workspace_roles_data,                  # Roles to assign to the workspace\n",
    "                                'capacity_name' : capacity_name_dvlm        # Name of target code workspace for development\n",
    "                            },\n",
    "                                'reporting' : {\n",
    "                             #   'name' : FrameworkName +' '+ datamesh_domains  + ' REPORTING (D) FMD',   # Name of target reporting workspace for development\n",
    "                                'roles' : workspace_roles_reporting,                  # Roles to assign to the workspace\n",
    "                                'capacity_name' : capacity_name_dvlm        # Name of target code workspace for development\n",
    "                            }\n",
    "                        }\n",
    "                    },\n",
    "                    {\n",
    "                        'environment_name' : 'production',                  # Name of target environment\n",
    "                        'environment_short' : 'P',                          # Short of target environment\n",
    "                        'workspaces': {\n",
    "                         \n",
    "                            'gold' : {\n",
    "                           #     'name' : FrameworkName +' '+ datamesh_domain + ' DATA (P) FMD',   # Name of target gold workspace for development\n",
    "                                'roles' : workspace_roles_gold,                  # Roles to assign to the workspace\n",
    "                                'capacity_name' : capacity_name_dvlm        # Name of target code workspace for development\n",
    "                            },\n",
    "                            'reporting' : {\n",
    "                           #     'name' : FrameworkName +' '+ datamesh_domain + ' REPORTING (P) FMD',   # Name of target reporting workspace for development\n",
    "                                'roles' : workspace_roles_reporting,                  # Roles to assign to the workspace\n",
    "                                'capacity_name' : capacity_name_prod        # Name of target code workspace for development\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868f7b0f-a0ca-4d6c-ab64-b231eb3b0cba",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Repo Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e453b88-bb97-4378-b3fb-de6faa6b6d1d",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "#FMD Framework code\n",
    "##### DO NET CHANGE UNLESS SPECIFIED OTHERWISE ####\n",
    "repo_owner = \"edkreuk\"              # Owner of the repository\n",
    "repo_name = \"FMD_FRAMEWORK\"         # Name of the repository\n",
    "branch = \"main\"                     #\"Main\" is default                    \n",
    "folder_prefix = \"\"\n",
    "###################################################\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32269821-d83f-41ac-a3d4-a19f8cb555de",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Download source & config files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22cc332-d939-4def-8971-84f762931ce2",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "def download_folder_as_zip(repo_owner, repo_name, output_zip, branch=\"main\", folder_to_extract=\"src\",  remove_folder_prefix = \"\"):\n",
    "    # Construct the URL for the GitHub API to download the repository as a zip file\n",
    "    url = f\"https://api.github.com/repos/{repo_owner}/{repo_name}/zipball/{branch}\"\n",
    "    \n",
    "    # Make a request to the GitHub API\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    folder_to_extract = f\"/{folder_to_extract}\" if folder_to_extract[0] != \"/\" else folder_to_extract\n",
    "    \n",
    "    # Ensure the directory for the output zip file exists\n",
    "    os.makedirs(os.path.dirname(output_zip), exist_ok=True)\n",
    "    \n",
    "    # Create a zip file in memory\n",
    "    with zipfile.ZipFile(BytesIO(response.content)) as zipf:\n",
    "        with zipfile.ZipFile(output_zip, 'w') as output_zipf:\n",
    "            for file_info in zipf.infolist():\n",
    "                parts = file_info.filename.split('/')\n",
    "                if  re.sub(r'^.*?/', '/', file_info.filename).startswith(folder_to_extract): \n",
    "                    # Extract only the specified folder\n",
    "                    file_data = zipf.read(file_info.filename)  \n",
    "                    if folder_prefix != \"\":\n",
    "                        parts.remove(remove_folder_prefix)\n",
    "                    output_zipf.writestr(('/'.join(parts[1:])), file_data)\n",
    "\n",
    "def uncompress_zip_to_folder(zip_path, extract_to):\n",
    "    # Ensure the directory for extraction exists\n",
    "    os.makedirs(extract_to, exist_ok=True)\n",
    "    \n",
    "    # Uncompress all files from the zip into the specified folder\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "    \n",
    "    # Delete the original zip file\n",
    "    os.remove(zip_path)\n",
    "\n",
    "download_folder_as_zip(repo_owner, repo_name, output_zip = \"./builtin/src/src.zip\", branch = branch, folder_to_extract= f\"{folder_prefix}/src\", remove_folder_prefix = f\"{folder_prefix}\")\n",
    "download_folder_as_zip(repo_owner, repo_name, output_zip = \"./builtin/config/config.zip\", branch = branch, folder_to_extract= f\"{folder_prefix}/config\" , remove_folder_prefix = f\"{folder_prefix}\")\n",
    "uncompress_zip_to_folder(zip_path = \"./builtin/config/config.zip\", extract_to= \"./builtin\")\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5c3526-9a63-430c-90e4-00ece0aebfa5",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## CLI Login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00e2fa8-e10d-45a9-82a7-266feaedc4ba",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "# Set environment parameters for Fabric CLI\n",
    "token = notebookutils.credentials.getToken('pbi')\n",
    "os.environ['FAB_TOKEN'] = token\n",
    "os.environ['FAB_TOKEN_ONELAKE'] = token\n",
    "\n",
    "tasks=[]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2171308-8526-4564-b8cb-21c588d2ae24",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Get connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0c6781-b3b5-47d0-a9ff-e192c7bdee9d",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "result = subprocess.run([\"fab\", \"api\", \"-X\", \"get\", \"connections\"], capture_output=True, text=True)\n",
    "\n",
    "#print(\"STDOUT:\", result.stdout)\n",
    "#print(\"STDERR:\", result.stderr)\n",
    "connections=json.loads(result.stdout)[\"text\"]\n",
    "connection_list = [item['id'] for item in connections['value']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b39ba9-62fb-421b-be07-2607f79fccdb",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Deployment functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7202e909-634f-42f5-a8b6-034430532d7a",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "# Fabric notebook source\n",
    "\n",
    "# METADATA ********************\n",
    "\n",
    "# META {\n",
    "# META   \"kernel_info\": {\n",
    "# META     \"name\": \"jupyter\",\n",
    "# META     \"jupyter_kernel_name\": \"python3.11\"\n",
    "# META   },\n",
    "# META   \"dependencies\": {\n",
    "# META     \"lakehouse\": {\n",
    "# META       \"default_lakehouse_name\": \"\",\n",
    "# META       \"default_lakehouse_workspace_id\": \"\"\n",
    "# META     }\n",
    "# META   }\n",
    "# META }\n",
    "\n",
    "# MARKDOWN ********************\n",
    "\n",
    "# ![FMD_Overview](https://github.com/edkreuk/FMD_FRAMEWORK/blob/main/Images/FMD_Overview.png?raw=true)\n",
    "\n",
    "# CELL ********************\n",
    "\n",
    "%pip install ms-fabric-cli pillow cairosvg --quiet\n",
    "\n",
    "# METADATA ********************\n",
    "\n",
    "# META {\n",
    "# META   \"language\": \"python\",\n",
    "# META   \"language_group\": \"jupyter_python\"\n",
    "# META }\n",
    "\n",
    "# CELL ********************\n",
    "\n",
    "import subprocess\n",
    "import os\n",
    "from time import sleep, time\n",
    "import json\n",
    "import shutil\n",
    "import re\n",
    "import requests\n",
    "import zipfile\n",
    "import yaml\n",
    "import struct\n",
    "import pyodbc\n",
    "import notebookutils\n",
    "import sempy.fabric as fabric\n",
    "import cairosvg\n",
    "import base64\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from requests.adapters import HTTPAdapter, Retry\n",
    "from io import BytesIO\n",
    "from zipfile import ZipFile \n",
    "\n",
    "# METADATA ********************\n",
    "\n",
    "# META {\n",
    "# META   \"language\": \"python\",\n",
    "# META   \"language_group\": \"jupyter_python\"\n",
    "# META }\n",
    "\n",
    "# MARKDOWN ********************\n",
    "\n",
    "# # Configuration and Parameters\n",
    "# \n",
    "# **Fabric Administrator Role is required to create domain**\n",
    "\n",
    "# CELL ********************\n",
    "\n",
    "FrameworkName= 'DEMO'                               # max 6 characters for better visibility, no spaces and the end of the name\n",
    "\n",
    "assign_icons = True                                 # Set to True to assign default icons to workspaces; set to False if you have already assigned custom icons\n",
    "\n",
    "load_demo_data= True                                # Set to True if you want to load the demo data, otherwise set to False\n",
    "lakehouse_schema_enabled = True                     # Set to True if you want to use the lakehouse schema, otherwise set to False\n",
    "\n",
    "driver = '{ODBC Driver 18 for SQL Server}' # Change this if you use a different driver\n",
    "\n",
    "\n",
    "# METADATA ********************\n",
    "\n",
    "# META {\n",
    "# META   \"language\": \"python\",\n",
    "# META   \"language_group\": \"jupyter_python\"\n",
    "# META }\n",
    "\n",
    "# MARKDOWN ********************\n",
    "\n",
    "# ## Capacity settings\n",
    "\n",
    "# CELL ********************\n",
    "\n",
    "capacity_name_dvlm = \"Trial-Erwin\"                  # Which capacity will be used for these workspaces in development\n",
    "capacity_name_prod = 'Trial-Erwin'                  # Which capacity will be used for these workspaces in production\n",
    "capacity_name_config = 'Trial-Erwin'                # Which capacity will be used for this workspace for the Metadata Database\n",
    "\n",
    "# METADATA ********************\n",
    "\n",
    "# META {\n",
    "# META   \"language\": \"python\",\n",
    "# META   \"language_group\": \"jupyter_python\"\n",
    "# META }\n",
    "\n",
    "# MARKDOWN ********************\n",
    "\n",
    "# ## Domain settings\n",
    "\n",
    "# CELL ********************\n",
    "\n",
    "create_domains=  True                               # If you do not have a Fabric Admin role, you need to set this option to False. For domain creation the Fabric Admin role is needed\n",
    "\n",
    "domain_name='FMD'                                   # Main Domain\n",
    "sub_domain_names= ['FINANCE','SALES']               # Create business domains(sub)\n",
    "\n",
    "domain_contributor_role = {\"type\": \"Contributors\",\"principals\": [{\"id\": \"86897900-8de5-4894-ae41-1b4d1642acda\",\"type\": \"Group\"}  ]}  # Which group(Object ID) can add or remove workspaces to this domain\n",
    "\n",
    "\n",
    "\n",
    "# METADATA ********************\n",
    "\n",
    "# META {\n",
    "# META   \"language\": \"python\",\n",
    "# META   \"language_group\": \"jupyter_python\"\n",
    "# META }\n",
    "\n",
    "# MARKDOWN ********************\n",
    "\n",
    "# ## Workspace Roles settings\n",
    "\n",
    "# CELL ********************\n",
    "\n",
    "workspace_roles_code = [ # Keep emtpy [] if you only want to assign this to your personal account\n",
    "                                        {\n",
    "                       \"principal\": {\n",
    "                            \"id\": '86897900-8de5-4894-ae41-1b4d1642acda',\n",
    "                            \"type\": \"Group\"\n",
    "                        },\n",
    "                        \"role\": \"admin\"  #(choose from 'admin', 'member', 'contributor', 'viewer')\n",
    "                        },\n",
    "                        {\n",
    "                       \"principal\": {\n",
    "                            \"id\": '9a507861-baca-4e94-b10f-36ea742b4ef2',\n",
    "                            \"type\": \"ServicePrincipal\"\n",
    "                        },\n",
    "                        \"role\": \"contributor\"  #(choose from 'admin', 'member', 'contributor', 'viewer')\n",
    "                        }\n",
    "                    ]\n",
    "workspace_roles_data =  [ # Keep emtpy [] if you only want to assign this to your personal account\n",
    "                                        {\n",
    "                       \"principal\": {\n",
    "                            \"id\": '86897900-8de5-4894-ae41-1b4d1642acda',\n",
    "                            \"type\": \"Group\"\n",
    "                        },\n",
    "                        \"role\": \"admin\"  #(choose from 'admin', 'member', 'contributor', 'viewer')\n",
    "                        },    \n",
    "                         {\n",
    "                       \"principal\": {\n",
    "                            \"id\": '5c906b5c-d1d7-4984-b047-adacd8d795fe',\n",
    "                            \"type\": \"Group\"\n",
    "                        },\n",
    "                        \"role\": \"contributor\"  #(choose from 'admin', 'member', 'contributor', 'viewer')\n",
    "                        }\n",
    "                    ]\n",
    "workspace_roles_gold = [ # Keep emtpy [] if you only want to assign this to your personal account\n",
    "                    {\n",
    "                      \"principal\": {\n",
    "                            \"id\": '86897900-8de5-4894-ae41-1b4d1642acda',\n",
    "                             \"type\": \"Group\"\n",
    "                        },\n",
    "                        \"role\": \"admin\"  #(choose from 'admin', 'member', 'contributor', 'viewer')\n",
    "                        }\n",
    "                    ]\n",
    "workspace_roles_reporting = [ # Keep emtpy [] if you only want to assign this to your personal account\n",
    "                    {\n",
    "                        \"principal\": {\n",
    "                            \"id\": '86897900-8de5-4894-ae41-1b4d1642acda',\n",
    "                            \"type\": \"Group\"\n",
    "                        },\n",
    "                        \"role\": \"admin\"  #(choose from 'admin', 'member', 'contributor', 'viewer')\n",
    "                        }\n",
    "                    ]\n",
    "\n",
    "# METADATA ********************\n",
    "\n",
    "# META {\n",
    "# META   \"language\": \"python\",\n",
    "# META   \"language_group\": \"jupyter_python\"\n",
    "# META }\n",
    "\n",
    "# MARKDOWN ********************\n",
    "\n",
    "# ## Configuration settings  (Fabric Database)\n",
    "\n",
    "\n",
    "# CELL ********************\n",
    "\n",
    "configuration = {\n",
    "                    'workspace': {\n",
    "                        'name' : FrameworkName + ' CONFIG FMD',             # Name of target workspace\n",
    "                        'roles' : workspace_roles_data,                     # Roles to assign to the workspace\n",
    "                        'capacity_name' : capacity_name_config              # Name of target capacity for the configuration workspace\n",
    "                    },\n",
    "                       'DatabaseName' : 'SQL_'+FrameworkName+'_FRAMEWORK'   # Name of target configuration SQL Database\n",
    "}\n",
    "\n",
    "# METADATA ********************\n",
    "\n",
    "# META {\n",
    "# META   \"language\": \"python\",\n",
    "# META   \"language_group\": \"jupyter_python\"\n",
    "# META }\n",
    "\n",
    "# MARKDOWN ********************\n",
    "\n",
    "# 1. ## Workspace configuration\n",
    "\n",
    "# CELL ********************\n",
    "\n",
    "\n",
    "##### DO NOT CHANGE UNLESS SPECIFIED OTHERWISE, FE ADDING NEW ENVIRONMENTS ####\n",
    "environments = [\n",
    "                    {\n",
    "                        'environment_name' : 'development',                 # Name of target environment\n",
    "                        'workspaces': {\n",
    "                            'data' : {\n",
    "                                'name' : FrameworkName + ' DATA (D) FMD',   # Name of target code workspace for development\n",
    "                                'roles' : workspace_roles_data,             # Roles to assign to the workspace\n",
    "                                'capacity_name' : capacity_name_dvlm        # Name of target data workspace for development\n",
    "                            },\n",
    "                            'code' : {\n",
    "                                'name' : FrameworkName + ' CODE (D) FMD',   # Name of target data workspace for development\n",
    "                                'roles' : workspace_roles_code,             # Roles to assign to the workspace\n",
    "                                'capacity_name' : capacity_name_dvlm        # Name of target code workspace for development\n",
    "                            },\n",
    "                        },\n",
    "                        'connections' : {\n",
    "                            'CON_FMD_FABRIC_SQL' : '372237f9-709a-48f8-8fb2-ce06940c990e',          # Required Guid to the Fabric SQL connection\n",
    "                            'CON_FMD_FABRIC_PIPELINES' : '6d8146c6-a438-47df-94e2-540c552eb6d7',    # Required Guid to the Fabric datapipelines connection\n",
    "                            'CON_FMD_ADF_PIPELINES' : '02e107b8-e97e-4b00-a28c-668cf9ce3d9a'        # 10000000-0000-0000-0000-000000000000'\n",
    "\n",
    "                        }\n",
    "                    },\n",
    "                    {\n",
    "                        'environment_name' : 'production',                  # Name of target environment\n",
    "                        'workspaces': {\n",
    "                            'data' : {\n",
    "                                'name' : FrameworkName + ' DATA (P) FMD',   # Name of target data workspace for production\n",
    "                                'roles' : workspace_roles_data,             # Roles to assign to the workspace\n",
    "                                'capacity_name' : capacity_name_prod        # Name of target data workspace for production   \n",
    "                            },\n",
    "                            'code' : {\n",
    "                                'name' : FrameworkName + ' CODE (P) FMD',   # Name of target code workspace for production\n",
    "                                'roles' : workspace_roles_code,             # Roles to assign to the workspace\n",
    "                                'capacity_name' : capacity_name_prod        # Name of target code workspace for production\n",
    "                            },                            \n",
    "                        },\n",
    "                        'connections' : {\n",
    "                            'CON_FMD_FABRIC_SQL' : '372237f9-709a-48f8-8fb2-ce06940c990e',          # Required Guid to the Fabric SQL connection\n",
    "                            'CON_FMD_FABRIC_PIPELINES' : '6d8146c6-a438-47df-94e2-540c552eb6d7',    # Required Guid to the Fabric datapipelines connection\n",
    "                            'CON_FMD_ADF_PIPELINES' : '02e107b8-e97e-4b00-a28c-668cf9ce3d9a'        # 10000000-0000-0000-0000-000000000000'\n",
    "\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "###################################################\n",
    "\n",
    "# METADATA ********************\n",
    "\n",
    "# META {\n",
    "# META   \"language\": \"python\",\n",
    "# META   \"language_group\": \"jupyter_python\"\n",
    "# META }\n",
    "\n",
    "# MARKDOWN ********************\n",
    "\n",
    "# ## Domain Configuration\n",
    "\n",
    "# CELL ********************\n",
    "\n",
    "##### DO NOT CHANGE UNLESS SPECIFIED OTHERWISE, FE ADDING NEW ENVIRONMENTS ####\n",
    "domain_deployment = [\n",
    "                    {\n",
    "                        'environment_name' : 'development',                 # Name of target environment\n",
    "                        'environment_short' : 'D',                          # Short of target environment\n",
    "                        'workspaces': {\n",
    "                         \n",
    "                            'gold' : {\n",
    "                                'roles' : workspace_roles_data,             # Roles to assign to the workspace\n",
    "                                'capacity_name' : capacity_name_dvlm        # Name of target code workspace for development\n",
    "                            },\n",
    "                                'reporting' : {\n",
    "                                 'roles' : workspace_roles_reporting,       # Roles to assign to the workspace\n",
    "                                'capacity_name' : capacity_name_dvlm        # Name of target code workspace for development\n",
    "                            }\n",
    "                        }\n",
    "                    },\n",
    "                    {\n",
    "                        'environment_name' : 'production',                  # Name of target environment\n",
    "                        'environment_short' : 'P',                          # Short of target environment\n",
    "                        'workspaces': {\n",
    "                         \n",
    "                            'gold' : {\n",
    "                                'roles' : workspace_roles_gold,             # Roles to assign to the workspace\n",
    "                                'capacity_name' : capacity_name_prod        # Name of target code workspace for development\n",
    "                            },\n",
    "                            'reporting' : {\n",
    "                                'roles' : workspace_roles_reporting,        # Roles to assign to the workspace\n",
    "                                'capacity_name' : capacity_name_prod        # Name of target code workspace for development\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "###################################################\n",
    "\n",
    "# METADATA ********************\n",
    "\n",
    "# META {\n",
    "# META   \"language\": \"python\",\n",
    "# META   \"language_group\": \"jupyter_python\"\n",
    "# META }\n",
    "\n",
    "# MARKDOWN ********************\n",
    "\n",
    "# ## Icon settings\n",
    "\n",
    "# CELL ********************\n",
    "\n",
    "# Workspace icon definition. Setting the icons to None will delete the existing icon of the workspaces specified.\n",
    "\n",
    "workspace_icon_def = {\n",
    "    \"icons\": {\n",
    "        \"code\": \"fmd_code_icon.png\",\n",
    "        \"data\": \"fmd_data_icon.png\",\n",
    "        \"config\": \"fmd_config_icon.png\",\n",
    "        \"reporting\": \"fmd_reporting_icon.png\",\n",
    "        \"gold\": \"fmd_gold_icon.png\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# METADATA ********************\n",
    "\n",
    "# META {\n",
    "# META   \"language\": \"python\",\n",
    "# META   \"language_group\": \"jupyter_python\"\n",
    "# META }\n",
    "\n",
    "# MARKDOWN ********************\n",
    "\n",
    "# # Repo Configuration\n",
    "\n",
    "# CELL ********************\n",
    "\n",
    "#FMD Framework code\n",
    "##### DO NOT CHANGE UNLESS SPECIFIED OTHERWISE ####\n",
    "repo_owner = \"edkreuk\"              # Owner of the repository\n",
    "repo_name = \"FMD_FRAMEWORK\"         # Name of the repository\n",
    "branch = \"main\"                     #\"main\" is default                    \n",
    "folder_prefix = \"\"\n",
    "###################################################\n",
    "\n",
    "\n",
    "# METADATA ********************\n",
    "\n",
    "# META {\n",
    "# META   \"language\": \"python\",\n",
    "# META   \"language_group\": \"jupyter_python\"\n",
    "# META }\n",
    "\n",
    "# MARKDOWN ********************\n",
    "\n",
    "# ## Download source & config files\n",
    "\n",
    "# CELL ********************\n",
    "\n",
    "def download_folder_as_zip(repo_owner, repo_name, output_zip, branch=\"main\", folder_to_extract=\"src\",  remove_folder_prefix = \"\"):\n",
    "    # Construct the URL for the GitHub API to download the repository as a zip file\n",
    "    url = f\"https://api.github.com/repos/{repo_owner}/{repo_name}/zipball/{branch}\"\n",
    "    \n",
    "    # Make a request to the GitHub API\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    folder_to_extract = f\"/{folder_to_extract}\" if folder_to_extract[0] != \"/\" else folder_to_extract\n",
    "    \n",
    "    # Ensure the directory for the output zip file exists\n",
    "    os.makedirs(os.path.dirname(output_zip), exist_ok=True)\n",
    "    \n",
    "    # Create a zip file in memory\n",
    "    with zipfile.ZipFile(BytesIO(response.content)) as zipf:\n",
    "        with zipfile.ZipFile(output_zip, 'w') as output_zipf:\n",
    "            for file_info in zipf.infolist():\n",
    "                parts = file_info.filename.split('/')\n",
    "                if  re.sub(r'^.*?/', '/', file_info.filename).startswith(folder_to_extract): \n",
    "                    # Extract only the specified folder\n",
    "                    file_data = zipf.read(file_info.filename)  \n",
    "                    if folder_prefix != \"\":\n",
    "                        parts.remove(remove_folder_prefix)\n",
    "                    output_zipf.writestr(('/'.join(parts[1:])), file_data)\n",
    "\n",
    "def uncompress_zip_to_folder(zip_path, extract_to):\n",
    "    # Ensure the directory for extraction exists\n",
    "    os.makedirs(extract_to, exist_ok=True)\n",
    "    \n",
    "    # Uncompress all files from the zip into the specified folder\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "    \n",
    "    # Delete the original zip file\n",
    "    os.remove(zip_path)\n",
    "\n",
    "download_folder_as_zip(repo_owner, repo_name, output_zip = \"./builtin/src/src.zip\", branch = branch, folder_to_extract= f\"{folder_prefix}/src\", remove_folder_prefix = f\"{folder_prefix}\")\n",
    "download_folder_as_zip(repo_owner, repo_name, output_zip = \"./builtin/config/config.zip\", branch = branch, folder_to_extract= f\"{folder_prefix}/config\" , remove_folder_prefix = f\"{folder_prefix}\")\n",
    "uncompress_zip_to_folder(zip_path = \"./builtin/config/config.zip\", extract_to= \"./builtin\")\n",
    " \n",
    "\n",
    "# METADATA ********************\n",
    "\n",
    "# META {\n",
    "# META   \"language\": \"python\",\n",
    "# META   \"language_group\": \"jupyter_python\"\n",
    "# META }\n",
    "\n",
    "# MARKDOWN ********************\n",
    "\n",
    "# # CLI Login\n",
    "\n",
    "# CELL ********************\n",
    "\n",
    "# Set environment parameters for Fabric CLI\n",
    "token = notebookutils.credentials.getToken('pbi')\n",
    "os.environ['FAB_TOKEN'] = token\n",
    "os.environ['FAB_TOKEN_ONELAKE'] = token\n",
    "\n",
    "tasks=[]\n",
    "\n",
    "\n",
    "# METADATA ********************\n",
    "\n",
    "# META {\n",
    "# META   \"language\": \"python\",\n",
    "# META   \"language_group\": \"jupyter_python\"\n",
    "# META }\n",
    "\n",
    "# MARKDOWN ********************\n",
    "\n",
    "# ## Get existing connections\n",
    "\n",
    "# CELL ********************\n",
    "\n",
    "result = subprocess.run([\"fab\", \"api\", \"-X\", \"get\", \"connections\"], capture_output=True, text=True)\n",
    "connections=json.loads(result.stdout)[\"text\"]\n",
    "connection_list = [item['id'] for item in connections['value']]\n",
    "\n",
    "# METADATA ********************\n",
    "\n",
    "# META {\n",
    "# META   \"language\": \"python\",\n",
    "# META   \"language_group\": \"jupyter_python\"\n",
    "# META }\n",
    "\n",
    "# MARKDOWN ********************\n",
    "\n",
    "# # Deployment functions\n",
    "\n",
    "# CELL ********************\n",
    "\n",
    "# -------------------------------\n",
    "# FABRIC CLI Utilities\n",
    "# -------------------------------\n",
    "\n",
    "def run_fab_command(command, capture_output=False, silently_continue=False, raw_output=False):\n",
    "    \"\"\"\n",
    "    Executes a Fabric CLI command with optional output capture and error handling.\n",
    "    \"\"\"\n",
    "    result = subprocess.run([\"fab\", \"-c\", command], capture_output=capture_output, text=True)\n",
    "    if not silently_continue and (result.returncode > 0 or result.stderr):\n",
    "        raise Exception(f\"Error running fab command. exit_code: '{result.returncode}'; stderr: '{result}'\")\n",
    "    if capture_output:\n",
    "        return result if raw_output else result.stdout.strip()\n",
    "    return None\n",
    "\n",
    "def get_cluster_url():\n",
    "    \"\"\"\n",
    "    Get the Fabric Cluster.\n",
    "    \"\"\"\n",
    "    response = run_fab_command(f\"api -A powerbi groups\", capture_output=True, silently_continue=True)\n",
    "    # Parse the JSON response\n",
    "    response_json = json.loads(response)[\"text\"]\n",
    "    # Extract the @odata.context URL\n",
    "    odata_context = response_json.get(\"@odata.context\", \"\")\n",
    "    # Use re.search instead of re.match\n",
    "    match = re.search(r\"(https://[^/]+/)\", odata_context)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    else:\n",
    "        print(\"Cluster URL not found.\")\n",
    "        return None\n",
    "    \n",
    "# -------------------------------\n",
    "# Domain Management\n",
    "# -------------------------------\n",
    "def create_fabric_domain(domain_name):\n",
    "    \"\"\"\n",
    "    Create a domain.\n",
    "    \"\"\"\n",
    "    start = time()\n",
    "    run_fab_command(f'create .domains/{domain_name}.Domain',  capture_output=True, silently_continue=True)\n",
    "    print(f\"✅ {domain_name} Created'\")\n",
    "    assign_domain_description(domain_name)\n",
    "    assign_domain_contributor_roles(domain_contributor_role, domain_name)\n",
    "    tasks.append({\"task_name\": f\"Create or Update Domain {domain_name}\",\"task_duration\": int(time() - start),\"status\": \"success\"})\n",
    "\n",
    "def create_fabric_sub_domain(domain_name, sub_domain):\n",
    "    \"\"\"\n",
    "    Create a sub domain in a domain.\n",
    "    \"\"\"\n",
    "    start = time()\n",
    "    run_fab_command(f'create .domains/{sub_domain}.Domain -P parentDomainName={domain_name}',  capture_output=True, silently_continue=True)\n",
    "    print(f\"✅ {sub_domain} created and assigned to '{domain_name}'\")\n",
    "    assign_domain_description(domain_name)\n",
    "    tasks.append({\"task_name\": f\"Create or update Sub Domain {sub_domain}\",\"task_duration\": int(time() - start),\"status\": \"success\"})\n",
    "\n",
    "def assign_fabric_domain(domain_name, workspace_name):\n",
    "    \"\"\"\n",
    "    Assigns a domain to the workspace.\n",
    "    \"\"\"\n",
    "    run_fab_command(f'assign .domains/{domain_name}.Domain -W {workspace_name}.Workspace -f',  capture_output=True, silently_continue=True)\n",
    "    print(f\"✅ {domain_name} Domain or Sub Domain  assigned to '{workspace_name}'\")\n",
    "\n",
    "def assign_domain_description(domain_name):\n",
    "    \"\"\"\n",
    "    Assigns a standard description to an Domain.\n",
    "    \"\"\"\n",
    "    payload = 'Note: This Domain  was initially generated by the FMD Framework. For further details, please refer to the documentation at https://github.com/edkreuk/FMD_FRAMEWORK.'\n",
    "    run_fab_command(f'set .domains/{domain_name}.Domain -q description -i {payload} -f', silently_continue=True)\n",
    "    print(f\"✅ Description applied to {domain_name} \")\n",
    "\n",
    "def assign_domain_contributor_roles(domain_contributor_role,domain_name):\n",
    "    \"\"\"\n",
    "    Assigns the Contributor role to a domain for a specific security group.\n",
    "    \"\"\"\n",
    "    payload = 'SpecificUsersAndGroups'\n",
    "    run_fab_command(f'set .domains/{domain_name}.Domain -q contributorsScope -i {payload} -f', capture_output=True,silently_continue=True)\n",
    "    domain_id = get_domain_id_by_name(domain_name)\n",
    "    payload_role = json.dumps(domain_contributor_role)\n",
    "    run_fab_command(f'api -X post admin/domains/{domain_id}/roleAssignments/bulkAssign -i \"{payload_role}\"',capture_output=True ,silently_continue=True  )\n",
    "\n",
    "    print(f\"✅ Contributor role applied to domain: {domain_name}\")\n",
    "\n",
    "def get_domain_id_by_name(domain_name):\n",
    "    \"\"\"\n",
    "    Retrieves the domain ID by its display name.\n",
    "    \"\"\"\n",
    "    result = run_fab_command(\"api -X get domains/\", capture_output=True, silently_continue=True)\n",
    "    domains = json.loads(result)[\"text\"][\"value\"]\n",
    "    normalized_name = domain_name.strip().lower()\n",
    "    match = next((w for w in domains if w['displayName'].strip().lower() == normalized_name), None)\n",
    "    return match['id'] if match else None\n",
    "# -------------------------------\n",
    "# Workspace Management\n",
    "# -------------------------------\n",
    "\n",
    "def get_workspace_id_by_name(workspace_name):\n",
    "    \"\"\"\n",
    "    Retrieves the workspace ID by its display name.\n",
    "    \"\"\"\n",
    "    result = run_fab_command(\"api -X get workspaces/\", capture_output=True, silently_continue=True)\n",
    "    workspaces = json.loads(result)[\"text\"][\"value\"]\n",
    "    normalized_name = workspace_name.strip().lower()\n",
    "    match = next((w for w in workspaces if w['displayName'].strip().lower() == normalized_name), None)\n",
    "    return match['id'] if match else None\n",
    "\n",
    "def ensure_workspace_exists(workspace, workspace_name):\n",
    "    \"\"\"\n",
    "    Ensures the workspace exists; creates it if not found.\n",
    "    \"\"\"\n",
    "    workspace_id = get_workspace_id_by_name(workspace_name)\n",
    "    if workspace_id:\n",
    "        print(f\" - Workspace '{workspace_name}' found. Workspace ID: {workspace_id}- assign capacity: {workspace['capacity_name']}\")\n",
    "        run_fab_command(f'assign \".capacities/{workspace[\"capacity_name\"]}.Capacity\" -W \"{workspace_name}.Workspace\" -f', silently_continue=True)\n",
    "        return workspace_id, \"exists\"\n",
    "\n",
    "    print(f\" - Workspace '{workspace_name}' not found. Creating new workspace...\")\n",
    "    run_fab_command(f'mkdir \"{workspace_name}.workspace\" -P capacityName=\"{workspace[\"capacity_name\"]}\"', silently_continue=True)\n",
    "    workspace_id = get_workspace_id_by_name(workspace_name)\n",
    "    if workspace_id:\n",
    "        print(f\" - Created workspace '{workspace_name}'. ID: {workspace_id}\")\n",
    "        return workspace_id, \"created\"\n",
    "    else:\n",
    "        raise RuntimeError(f\"Workspace '{workspace_name}' could not be created or found.\")\n",
    "\n",
    "# -------------------------------\n",
    "# Item Utilities\n",
    "# -------------------------------\n",
    "\n",
    "def get_item_id(workspace_name, name):\n",
    "    \"\"\"\n",
    "    Retrieves the item ID from a workspace.\n",
    "    \"\"\"\n",
    "    return run_fab_command(f\"get /{workspace_name}.Workspace/{name} -q id\", capture_output=True, silently_continue=True)\n",
    "\n",
    "def get_item_display_name(workspace_name, name):\n",
    "    \"\"\"\n",
    "    Retrieves the display name of an item.\n",
    "    \"\"\"\n",
    "    return run_fab_command(f\"get /{workspace_name}.Workspace/{name} -q displayName\", capture_output=True, silently_continue=True)\n",
    "\n",
    "def get_items(workspace_id, item_id=''):\n",
    "    \"\"\"\n",
    "    Retrieves item definitions or lists from a workspace.\n",
    "    \"\"\"\n",
    "    if item_id:\n",
    "        return run_fab_command(f\"api -X post workspaces/{workspace_id}/items/{item_id}/getDefinition\", capture_output=True, silently_continue=True)\n",
    "    return run_fab_command(f\"api -X get workspaces/{workspace_id}/items/{item_id}\", capture_output=True, silently_continue=True)\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# File and ID Replacement\n",
    "# -------------------------------\n",
    "\n",
    "def copy_to_tmp(name, child=None):\n",
    "    \"\"\"\n",
    "    Extracts item files from a ZIP archive to a temporary directory.\n",
    "    \"\"\"\n",
    "    child_path = \"\" if child is None else f\".children/{child}/\"\n",
    "    shutil.rmtree(\"./builtin/tmp\", ignore_errors=True)\n",
    "    path2zip = \"./builtin/src/src.zip\"\n",
    "    with ZipFile(path2zip) as archive:\n",
    "        for file in archive.namelist():\n",
    "            if file.startswith(f'src/{name}/{child_path}'):\n",
    "                archive.extract(file, './builtin/tmp')\n",
    "    return f\"./builtin/tmp/src/{name}/{child_path}\"\n",
    "\n",
    "def replace_ids_in_folder(folder_path, mapping_table, environment_name):\n",
    "    \"\"\"\n",
    "    Replaces old IDs with new ones in specified file types within a folder.\n",
    "    \"\"\"\n",
    "    for root, _, files in os.walk(folder_path):\n",
    "        for file_name in files:\n",
    "            if file_name.endswith(('.py', '.json', '.pbir', '.platform', '.ipynb', '.tmdl')) and not file_name.endswith('report.json'):\n",
    "                file_path = os.path.join(root, file_name)\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    content = file.read()\n",
    "                    for mapping in mapping_table:\n",
    "                        if mapping[\"environment\"] in (environment_name, \"config\"):\n",
    "                            content = content.replace(mapping[\"old_id\"], mapping[\"new_id\"])\n",
    "                with open(file_path, 'w', encoding='utf-8') as file:\n",
    "                    file.write(content)\n",
    "\n",
    "def replace_ids_and_mark_inactive(folder_path, mapping_table, environment_name, target_guids):\n",
    "    \"\"\"\n",
    "    Replaces old IDs with new ones in JSON-based files and deactivates activities\n",
    "    that reference connections not in the target_guids list.\n",
    "\n",
    "    Parameters:\n",
    "    - folder_path (str): Path to the folder containing files to process.\n",
    "    - mapping_table (list): List of dictionaries with 'old_id', 'new_id', and 'environment'.\n",
    "    - environment_name (str): Current environment name to filter applicable mappings.\n",
    "    - target_guids (list): List of valid connection GUIDs to retain as active.\n",
    "\n",
    "    Returns:\n",
    "    - None. Files are modified in-place.\n",
    "    \"\"\"\n",
    "    def find_externalReferences_in_dict(j):\n",
    "        externalReferences = {}\n",
    "        for key, value in j.items():\n",
    "            if isinstance(value, dict):\n",
    "                externalReferences.update(find_externalReferences_in_dict(value))\n",
    "            if key == \"externalReferences\":\n",
    "                externalReferences[key] = value\n",
    "        return externalReferences\n",
    "\n",
    "    def should_deactivate(connection):\n",
    "        return (\n",
    "            connection not in target_guids and\n",
    "            connection not in ['@item().ConnectionGuid', '@pipeline().parameters.ConnectionGuid']\n",
    "        )\n",
    "\n",
    "    def process_nested_activities(activities):\n",
    "        for activity in activities:\n",
    "            result = find_externalReferences_in_dict(activity)\n",
    "            connection = result.get('externalReferences', {}).get('connection')\n",
    "            if connection and should_deactivate(connection):\n",
    "                print(f\"Deactivate activity {activity.get('name')} for connection {connection}\")\n",
    "                activity[\"state\"] = \"Inactive\"\n",
    "                activity[\"onInactiveMarkAs\"] = \"Succeeded\"\n",
    "\n",
    "    for root, _, files in os.walk(folder_path):\n",
    "        for file_name in files:\n",
    "            if file_name.endswith(('.py', '.json', '.pbir', '.platform', '.ipynb', '.tmdl')) and not file_name.endswith('report.json'):\n",
    "                file_path = os.path.join(root, file_name)\n",
    "\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    content = file.read()\n",
    "\n",
    "                # Replace IDs\n",
    "                for mapping in mapping_table:\n",
    "                    if mapping[\"environment\"] in (environment_name, \"config\"):\n",
    "                        content = content.replace(mapping[\"old_id\"], mapping[\"new_id\"])\n",
    "\n",
    "                try:\n",
    "                    data = json.loads(content)\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "\n",
    "                if not data or not data.get(\"properties\") or not data[\"properties\"].get(\"activities\"):\n",
    "                    continue\n",
    "\n",
    "                for activity in data[\"properties\"][\"activities\"]:\n",
    "                    process_nested_activities([activity])\n",
    "                    for key in [\"activities\", \"ifFalseActivities\", \"ifTrueActivities\"]:\n",
    "                        nested = activity.get(\"typeProperties\", {}).get(key)\n",
    "                        if nested:\n",
    "                            process_nested_activities(nested)\n",
    "\n",
    "                content = json.dumps(data, indent=2)\n",
    "                with open(file_path, 'w', encoding='utf-8') as file:\n",
    "                    file.write(content)\n",
    "\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Description and Identity Assignment\n",
    "# -------------------------------\n",
    "\n",
    "def assign_workspace_description(workspace_name):\n",
    "    \"\"\"\n",
    "    Assigns a standard description to the workspace.\n",
    "    \"\"\"\n",
    "    payload = 'Important: The items in this workspace are automatically generated by the FMD Framework. Each time the setup notebook is executed, all changes will be overwritten. For more information, please visit https://github.com/edkreuk/FMD_FRAMEWORK.'\n",
    "    run_fab_command(f'set \"/{workspace_name}.workspace -q description -i {payload} -f', silently_continue=True)\n",
    "    print(f\" - Description applied to '{workspace_name}'\")\n",
    "\n",
    "def assign_item_description(workspace_name, item):\n",
    "    \"\"\"\n",
    "    Assigns a standard description to an item.\n",
    "    \"\"\"\n",
    "    payload = 'Note: This item was initially generated by the FMD Framework. Any modifications may introduce breaking changes. For further details, please refer to the documentation at https://github.com/edkreuk/FMD_FRAMEWORK.'\n",
    "    run_fab_command(f'set \"/{workspace_name}.workspace/{item} -q description -i {payload} -f', silently_continue=True)\n",
    "    print(f\" - Description applied to {item} in '{workspace_name}'\")\n",
    "\n",
    "def assign_managed_identity(workspace_name):\n",
    "    \"\"\"\n",
    "    Assigns a managed identity to the workspace.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        run_fab_command(f'create \"/{workspace_name}.workspace/.managedidentities/{workspace_name}.ManagedIdentity\"', silently_continue=True)\n",
    "        print(f\" - Managed identity assigned to '{workspace_name}'\")\n",
    "    except Exception as e:\n",
    "        print(f\" - Failed to assign managed identity: {e}\")\n",
    "\n",
    "# -------------------------------\n",
    "# Role Assignment\n",
    "# -------------------------------\n",
    "\n",
    "def assign_workspace_roles(workspace, workspace_name):\n",
    "    \"\"\"\n",
    "    Assigns roles to principals in the workspace.\n",
    "    \"\"\"\n",
    "    workspace_path = f\"/{workspace_name}.workspace\"\n",
    "    print(f\" - Assigning Workspace roles\")\n",
    "    for role in workspace['roles']:\n",
    "        try:\n",
    "            print(f\"Assigning role '{role['role']}' to '{role['principal']}' in workspace '{workspace_name}'\")\n",
    "            run_fab_command(f'acl set \"{workspace_path}\" -I {role[\"principal\"][\"id\"]} -R {role[\"role\"]} -f',silently_continue=False)\n",
    "        except Exception as e:\n",
    "            print(f\" - Failed to assign role: {e}\")\n",
    "\n",
    "# -------------------------------\n",
    "# Folder Handling\n",
    "# -------------------------------\n",
    "\n",
    "def get_workspace_folders(workspace_id):\n",
    "    \"\"\"\n",
    "    Retrieves all folders in a workspace.\n",
    "    \"\"\"\n",
    "    response = run_fab_command(f\"api workspaces/{workspace_id}/folders\", capture_output=True, silently_continue=True)\n",
    "    return json.loads(response).get('text', {}).get('value', [])\n",
    "\n",
    "def get_workspace_folder(workspace_id, folder_name):\n",
    "    \"\"\"\n",
    "    Retrieves folder metadata by name.\n",
    "    \"\"\"\n",
    "    for f in get_workspace_folders(workspace_id):\n",
    "        if f.get('displayName') == folder_name:\n",
    "            return f\n",
    "    return None\n",
    "\n",
    "def assign_item_to_folder(workspace_id, item_id, folder):\n",
    "    \"\"\"\n",
    "    Assigns an item to a folder, creating the folder if it doesn't exist.\n",
    "    \"\"\"\n",
    "    folder_details = get_workspace_folder(workspace_id, folder)\n",
    "    if folder_details is None:\n",
    "        payload = json.dumps({\"displayName\": folder})\n",
    "        print(f\"Folder does not exist, creating {payload}\")\n",
    "        folder_details = run_fab_command(f\"api -X post workspaces/{workspace_id}/folders -i {payload}\", capture_output=True, silently_continue=False)\n",
    "        folder_details = json.loads(folder_details).get('text', {})\n",
    "    payload = json.dumps({'folder': folder_details.get('id')})\n",
    "    #result = run_fab_command(f\"api -X post workspaces/{workspace_id}/items/{item_id}/move -i {payload}\", capture_output=True, silently_continue=False)\n",
    "    result = run_fab_command(f\"mv {workspace_name}.workspace/{item} {workspace_name}.workspace/{folder}.Folder  \", capture_output=True, silently_continue=False)\n",
    "\n",
    "\n",
    "def deploy_workspaces(domain_name,workspace, workspace_name, environment_name, old_id, mapping_table, tasks):\n",
    "    \"\"\"\n",
    "    Deploys a workspace by ensuring its existence, assigning identity, roles, and description.\n",
    "    Updates the mapping table and logs the deployment task.\n",
    "\n",
    "    Parameters:\n",
    "    - workspace (dict): Workspace configuration including name and capacity.\n",
    "    - environment_name (str): Target environment name.\n",
    "    - old_id (str): Previous workspace ID to be replaced.\n",
    "    - mapping_table (list): List to store ID mappings.\n",
    "    - tasks (list): List to store task execution logs.\n",
    "    \"\"\"\n",
    "    start = time()\n",
    "    print(\"\\n#############################################\")\n",
    "    print(f\" - Processing: workspace {workspace_name}\")\n",
    "\n",
    "    workspace_id, status = ensure_workspace_exists(workspace, workspace_name)\n",
    "    workspace[\"id\"] = workspace_id\n",
    "\n",
    "    print(\"--------------------------\")\n",
    "    print(f\"Updating Mapping Table: {environment_name}\")\n",
    "    mapping_table.append({\"Description\": workspace_name,\"environment\": environment_name,\"ItemType\": \"Workspace\",\"old_id\": old_id,\"new_id\": workspace_id })\n",
    "    mapping_table.append({\"Description\": workspace_name,\"environment\": environment_name,\"ItemType\": \"Workspace\",\"old_id\": \"00000000-0000-0000-0000-000000000000\",\"new_id\": workspace_id})\n",
    "\n",
    "    assign_managed_identity(workspace_name)\n",
    "    assign_workspace_roles(workspace,workspace_name)\n",
    "    assign_workspace_description(workspace_name)\n",
    "    assign_fabric_domain(domain_name, workspace_name)  \n",
    "\n",
    "    tasks.append({\"task_name\": f\"Create or Update workspace {workspace_name}\",\"task_duration\": int(time() - start),\"status\": \"success\" })\n",
    "\n",
    "def deploy_item(workspace,workspace_name,name, mapping_table, environment_name, connection_list, tasks, lakehouse_schema_enabled, child=None, it=None):\n",
    "    \"\"\"\n",
    "    Deploys an item (Notebook, Lakehouse, DataPipeline) into a workspace.\n",
    "    Handles ID replacement, description assignment, and updates mapping and task logs.\n",
    "\n",
    "    Parameters:\n",
    "    - workspace (dict): Workspace configuration including name.\n",
    "    - name (str): Name of the item to deploy.\n",
    "    - mapping_table (list): List to store ID mappings.\n",
    "    - environment_name (str): Target environment name.\n",
    "    - connection_list (list): List of valid connection GUIDs.\n",
    "    - tasks (list): List to store task execution logs.\n",
    "    - lakehouse_schema_enabled (bool): Flag to enable schema creation for lakehouses.\n",
    "    - child (str, optional): Child item name if applicable.\n",
    "    - it (dict, optional): Item metadata including old ID.\n",
    "    \"\"\"\n",
    "    start = time()\n",
    "    print(\"\\n#############################################\")\n",
    "    print(f\"Deploying in {workspace_name}: {name}\")\n",
    "\n",
    "    tmp_path = copy_to_tmp(name, child)\n",
    "    name = name if child is None else child\n",
    "    workspace_id = get_workspace_id_by_name(workspace_name)\n",
    "    cli_parameter = ''\n",
    "\n",
    "    if \"Notebook\" in name:\n",
    "        cli_parameter += \" --format .py\"\n",
    "        result = run_fab_command(f\"import / {workspace_name}.Workspace/{name} -i {tmp_path} -f {cli_parameter}\",capture_output=True, silently_continue=True)\n",
    "        assign_item_description(workspace_name, name)\n",
    "        new_id = get_item_id(workspace_name, name)\n",
    "        mapping_type='Notebook'\n",
    "\n",
    "    elif \"Lakehouse\" in name:\n",
    "        if lakehouse_schema_enabled:\n",
    "            result = run_fab_command(f\"create {workspace_name}.Workspace/{name} -P enableschemas=true\",capture_output=True, silently_continue=True)\n",
    "        else:\n",
    "            result = run_fab_command(f\"create {workspace_name}.Workspace/{name} -P\", capture_output=True, silently_continue=True)\n",
    "        assign_item_description(workspace_name, name)\n",
    "        new_id = get_item_id(workspace_name, name)\n",
    "        mapping_type='Lakehouse'\n",
    "\n",
    "    elif \"DataPipeline\" in name:\n",
    "        print(f\"Replacing connections guid in {workspace['name']}: {name}\")\n",
    "        replace_ids_and_mark_inactive(tmp_path, mapping_table, environment_name, connection_list)\n",
    "        result = run_fab_command(f\"import / {workspace_name}.Workspace/{name} -i {tmp_path} -f {cli_parameter}\",capture_output=True, silently_continue=True)\n",
    "        assign_item_description(workspace_name, name)\n",
    "        new_id = get_item_id(workspace_name, name)\n",
    "        mapping_type='DataPipeline'\n",
    "\n",
    "    elif \"VariableLibrary\" in name:   #Not working yet, import is giving error back\n",
    "        print(f\"Creating VariableLibrary: {name}\")\n",
    "        result = run_fab_command(f\"import / {workspace_name}.Workspace/{name} -i {tmp_path} -f\",      capture_output=True, silently_continue=True)\n",
    "        new_id = get_item_id(workspace_name, name)\n",
    "        mapping_type='VariableLibrary'\n",
    "\n",
    "    print(result)\n",
    "    if it:\n",
    "        mapping_table.append({\"Description\": name,\"environment\": environment_name,\"ItemType\": mapping_type, \"old_id\": it[\"id\"],\"new_id\": new_id})\n",
    "\n",
    "\n",
    "    tasks.append({\n",
    "        \"task_name\": f\"Create or Update item Definition {workspace_name} - {name}\",\"task_duration\": int(time() - start),\"status\": result })\n",
    "\n",
    "\n",
    "# METADATA ********************\n",
    "\n",
    "# META {\n",
    "# META   \"language\": \"python\",\n",
    "# META   \"language_group\": \"jupyter_python\"\n",
    "# META }\n",
    "\n",
    "# CELL ********************\n",
    "\n",
    "# -------------------------------\n",
    "# Workspace Handling for Cluster Request\n",
    "# -------------------------------\n",
    "\n",
    "def invoke_fabric_request(method, url, payload=None):\n",
    "    \n",
    "    headers = {\n",
    "        \"Authorization\": \"Bearer \" + notebookutils.credentials.getToken(\"pbi\"),\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        session = requests.Session()\n",
    "        retries = Retry(total=3, backoff_factor=5, status_forcelist=[502, 503, 504])\n",
    "        adapter = HTTPAdapter(max_retries=retries)\n",
    "        session.mount('http://', adapter)\n",
    "        session.mount('https://', adapter)\n",
    "\n",
    "        response = session.request(method, url, headers=headers, json=payload, timeout=240)      \n",
    "        if (response.status_code == 202):\n",
    "            operation_id = response.headers.get('x-ms-operation-id')\n",
    "            \n",
    "            # Poll the operation status until it's done - sleep 2 seconds between polls\n",
    "            while True:\n",
    "                operation_state_response = invoke_fabric_api_request(\"get\", f\"operations/{operation_id}\")\n",
    "                operation_state = operation_state_response.json().get(\"status\")\n",
    "\n",
    "                if operation_state in [\"NotStarted\", \"Running\"]:\n",
    "                    time.sleep(2)\n",
    "                elif operation_state == \"Succeeded\":\n",
    "                    response = invoke_fabric_api_request(\"get\", f\"operations/{operation_id}/result\")\n",
    "                    break\n",
    "                else:\n",
    "                    break\n",
    "        \n",
    "        return response\n",
    "\n",
    "    except requests.RequestException as ex:\n",
    "        print(ex)\n",
    "\n",
    "def get_workspace_metadata(workspace_id):\n",
    "    response = invoke_fabric_request(\"get\", f\"{cluster_base_url}metadata/folders/{workspace_id}\")\n",
    "    response.raise_for_status()\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "def set_workspace_icon(workspace_id, base64_png):\n",
    "    if base64_png == \"\":\n",
    "        icon = \"\"\n",
    "    elif base64_png:\n",
    "        icon = f\"data:image/png;base64,{base64_png}\"\n",
    "\n",
    "    if icon is not None:\n",
    "        payload = { \"icon\": icon }\n",
    "        try:\n",
    "            response = invoke_fabric_request(\"put\", f\"{cluster_base_url}metadata/folders/{workspace_id}\", payload)\n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "        except:\n",
    "            print(f\"Could not set icon on workspace id {workspace_id}. Ensure that the user is admin on workspace.\")\n",
    "            return None\n",
    "# -------------------------------\n",
    "# FMD specific Icon functions\n",
    "# Inspiration and the code is coming from Peer, who wrote a blog post about this (https://peerinsights.hashnode.dev/automating-fabric-maintaining-workspace-icon-images) \n",
    "# -------------------------------\n",
    "\n",
    "\n",
    "icon_display_size = \"24\"\n",
    "default_icon = f\"<img height='{icon_display_size}' src='https://content.powerapps.com/resource/powerbiwfe/images/artifact-colored-icons.663f961f5a92d994a109.svg#c_group_workspace_24' />\"\n",
    "\n",
    "def convert_svg_base64_to_png_base64(base64_svg):\n",
    "    svg_data = base64.b64decode(base64_svg)\n",
    "    png_bytes = cairosvg.svg2png(bytestring=svg_data)\n",
    "    base64_png = base64.b64encode(png_bytes).decode()\n",
    "    return base64_png\n",
    "\n",
    "\n",
    "def fill_svg(base64_svg, fill_color):\n",
    "    try:\n",
    "        svg_data = base64.b64decode(base64_svg).decode('utf-8')\n",
    "        modified_svg = re.sub(r'fill=\"[^\"]+\"', f'fill=\"{fill_color}\"', svg_data)\n",
    "        return base64.b64encode(modified_svg.encode('utf-8')).decode('utf-8')\n",
    "    except:\n",
    "        print(\"Failed colorfill of image. Skipping\")\n",
    "\n",
    "def display_workspace_icons(workspaces):\n",
    "    html = \"<table width='100%'>\"\n",
    "    html += \"<th style='text-align:left'>Workspace name</th><th style='text-align:left'>Workspace ID</th><th style='text-align:left; width:100px'>Old icon</th><th style='text-align:left; width:100px'>New icon</th>\"\n",
    "    for workspace in workspaces:\n",
    "        html += f\"<tr><th style='text-align:left'>{workspace.get('displayName')}</td>\"\n",
    "        html += f\"<td style='text-align:left'>{workspace.get('id')}</td>\"\n",
    "        iconUrl = get_workspace_metadata(workspace.get('id')).get('iconUrl')\n",
    "        existing_icon = f\"<img height='{icon_display_size}' src='{cluster_base_url}{iconUrl}'/>\" if iconUrl is not None else default_icon\n",
    "        html += f\"<td style='text-align:left'>{existing_icon}</td>\"\n",
    "        new_icon = workspace.get('icon_base64img')\n",
    "        if workspace.get('icon_base64img',\"\") == \"\":\n",
    "            new_icon = default_icon\n",
    "        else:\n",
    "            new_icon = f\"<img height='{icon_display_size}' src='data:image/png;base64,{new_icon}' />\" if new_icon is not None else existing_icon\n",
    "        html += f\"<td style='text-align:left'>{new_icon}</td></tr>\"\n",
    "    \n",
    "    displayHTML(html)   \n",
    "\n",
    "def add_letter_to_base64_png(base64_png, letter, font_size=20, text_color=\"black\", bold=False):\n",
    "    image_data = base64.b64decode(base64_png)\n",
    "    image = Image.open(io.BytesIO(image_data))\n",
    "\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    \n",
    "    try:\n",
    "        font = ImageFont.truetype(\"arial.ttf\", font_size)\n",
    "    except IOError:\n",
    "        font = ImageFont.truetype(\"DejaVuSans-Bold.ttf\", font_size)\n",
    "        \n",
    "    padding = 0\n",
    "    text_bbox = draw.textbbox((0, 0), letter, font=font)  # Get bounding box\n",
    "    text_width = text_bbox[2] - text_bbox[0]\n",
    "    text_height = text_bbox[3] - text_bbox[1]\n",
    "    \n",
    "    text_x = image.width - text_width - padding\n",
    "    text_y = padding\n",
    "\n",
    "    if bold:\n",
    "        for offset in [(0, 0), (1, 0), (0, 1), (1, 1)]:\n",
    "            draw.text((text_x + offset[0], text_y + offset[1]), letter, font=font, fill=text_color)\n",
    "    else:\n",
    "        draw.text((text_x, text_y), letter, font=font, fill=text_color)\n",
    "\n",
    "    output_buffer = io.BytesIO()\n",
    "    image.save(output_buffer, format=\"PNG\")\n",
    "    new_base64_png = base64.b64encode(output_buffer.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "    return new_base64_png\n",
    "\n",
    "# METADATA ********************\n",
    "\n",
    "# META {\n",
    "# META   \"language\": \"python\",\n",
    "# META   \"language_group\": \"jupyter_python\"\n",
    "# META }\n",
    "\n",
    "# MARKDOWN ********************\n",
    "\n",
    "# ## Load configuration\n",
    "# Create  workspace, identity and roles\n",
    "\n",
    "# CELL ********************\n",
    "\n",
    "base_path = './builtin/'\n",
    "config_path = os.path.join(base_path, 'config/item_config.yaml')\n",
    "\n",
    "with open(config_path, 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "\n",
    "deploy_order_path = os.path.join(base_path, 'config/item_deployment.json')\n",
    "with open(deploy_order_path, 'r') as file:\n",
    "        item_deployment =json.load(file)\n",
    "\n",
    "deploy_order_path = os.path.join(base_path, 'config/sql_deployment.json')\n",
    "with open(deploy_order_path, 'r') as file:\n",
    "        sql_deployment =json.load(file)\n",
    "\n",
    "deploy_order_path = os.path.join(base_path, 'config/data_deployment.json')\n",
    "with open(deploy_order_path, 'r') as file:\n",
    "        data_deployment =json.load(file)\n",
    "\n",
    "deploy_order_path = os.path.join(base_path, 'config/lakehouse_deployment.json')\n",
    "with open(deploy_order_path, 'r') as file:\n",
    "        lakehouse_deployment =json.load(file)\n",
    "\n",
    "deploy_icons_path = os.path.join(base_path, 'config/fabric_icons.xml')\n",
    "\n",
    "# Parse the XML file\n",
    "tree = ET.parse(deploy_icons_path)\n",
    "root = tree.getroot()\n",
    "\n",
    "# Create a dictionary to store icon name and base64\n",
    "fabric_icons_fmd = {}\n",
    "for item in root.findall('icon'):\n",
    "    name = item.find('name').text if item.find('name') is not None else \"No name\"\n",
    "    base64_str = item.find('base64').text if item.find('base64') is not None else \"\"\n",
    "    fabric_icons_fmd[name] = base64_str\n",
    "\n",
    "mapping_table=[]\n",
    "\n",
    "# Get cluster URL for use in metadata endpoints\n",
    "cluster_base_url = get_cluster_url()\n",
    "\n",
    "# METADATA ********************\n",
    "\n",
    "# META {\n",
    "# META   \"language\": \"python\",\n",
    "# META   \"language_group\": \"jupyter_python\"\n",
    "# META }\n",
    "\n",
    "# MARKDOWN ********************\n",
    "\n",
    "# # Deployment\n",
    "\n",
    "# MARKDOWN ********************\n",
    "\n",
    "# ## Main Domain\n",
    "\n",
    "# CELL ********************\n",
    "\n",
    "if create_domains:\n",
    "    create_fabric_domain(domain_name)\n",
    "\n",
    "# METADATA ********************\n",
    "\n",
    "# META {\n",
    "# META   \"language\": \"python\",\n",
    "# META   \"language_group\": \"jupyter_python\"\n",
    "# META }\n",
    "\n",
    "# MARKDOWN ********************\n",
    "\n",
    "# ### Deploy workspaces(Code and Data)\n",
    "\n",
    "# CELL ********************\n",
    "\n",
    "for environment in environments:\n",
    "    print(f\"--------------------------\")\n",
    "    print(f\"Updating Workspace: {environment['environment_name']}\")\n",
    "    deploy_workspaces(domain_name,workspace=environment['workspaces']['code'], workspace_name=environment['workspaces']['code']['name'], environment_name=environment['environment_name'], old_id=config[\"workspaces\"][\"workspace_code\"], mapping_table=mapping_table, tasks=tasks)\n",
    "    deploy_workspaces(domain_name,workspace=environment['workspaces']['data'], workspace_name=environment['workspaces']['data']['name'], environment_name=environment['environment_name'], old_id=config[\"workspaces\"][\"workspace_data\"], mapping_table=mapping_table, tasks=tasks)\n",
    "    # Append the remaining connections\n",
    "    mapping_table.append({\"Description\": \"CON_FMD_FABRIC_SQL\" ,\"ItemType\": \"connection\", \"environment\": environment['environment_name'],\"old_id\": config['connections'][\"CON_FMD_FABRIC_SQL\"], \"new_id\": environment['connections']['CON_FMD_FABRIC_SQL']})\n",
    "    mapping_table.append({\"Description\": \"CON_FMD_FABRIC_PIPELINES\",\"ItemType\": \"connection\" ,\"environment\": environment['environment_name'] ,\"old_id\": config[\"connections\"][\"CON_FMD_FABRIC_PIPELINES\"], \"new_id\": environment['connections']['CON_FMD_FABRIC_PIPELINES']})\n",
    "    mapping_table.append({\"Description\": \"CON_FMD_ADF_PIPELINES\",\"ItemType\": \"connection\" ,\"environment\": environment['environment_name'] ,\"old_id\": config[\"connections\"][\"CON_FMD_ADF_PIPELINES\"], \"new_id\": environment['connections']['CON_FMD_ADF_PIPELINES']})\n",
    "\n",
    "print(f\"--------------------------\")   \n",
    "\n",
    "\n",
    "# METADATA ********************\n",
    "\n",
    "# META {\n",
    "# META   \"language\": \"python\",\n",
    "# META   \"language_group\": \"jupyter_python\"\n",
    "# META }\n",
    "\n",
    "# MARKDOWN ********************\n",
    "\n",
    "# ### Deploy Workspace(Config)\n",
    "\n",
    "# CELL ********************\n",
    "\n",
    "print(f\"Updating Workspace: Configuration\")\n",
    "deploy_workspaces(domain_name,workspace=configuration['workspace'], workspace_name=configuration['workspace']['name'], environment_name='config', old_id=config[\"workspaces\"][\"workspace_config\"], mapping_table=mapping_table, tasks=tasks)\n",
    "\n",
    "\n",
    "# METADATA ********************\n",
    "\n",
    "# META {\n",
    "# META   \"language\": \"python\",\n",
    "# META   \"language_group\": \"jupyter_python\"\n",
    "# META }\n",
    "\n",
    "# MARKDOWN ********************\n",
    "\n",
    "# ## Sub Domain\n",
    "\n",
    "# MARKDOWN ********************\n",
    "\n",
    "# ### Deploy and create workspaces(Sub Domains)\n",
    "\n",
    "# MARKDOWN ********************\n",
    "\n",
    "\n",
    "# CELL ********************\n",
    "\n",
    "for sub_domain_name in sub_domain_names:\n",
    "    if create_domains:\n",
    "        create_fabric_sub_domain(domain_name, sub_domain_name)\n",
    "\n",
    "    for domain in domain_deployment:\n",
    "        print(f\"--------------------------\")\n",
    "        print(f\"Updating Workspace: {domain['environment_name']}\")\n",
    "        deploy_workspaces(sub_domain_name,workspace=domain['workspaces']['gold'],workspace_name=FrameworkName +' '+ sub_domain_name + ' DATA ('+domain['environment_short']+') FMD',  environment_name=domain['environment_name'], old_id=config[\"workspaces\"][\"workspace_gold\"], mapping_table=mapping_table, tasks=tasks)\n",
    "        deploy_workspaces(sub_domain_name,workspace=domain['workspaces']['reporting'],workspace_name=FrameworkName +' '+ sub_domain_name + ' REPORTING ('+domain['environment_short']+') FMD', environment_name=domain['environment_name'], old_id=config[\"workspaces\"][\"workspace_reporting\"], mapping_table=mapping_table, tasks=tasks)\n",
    "        \n",
    "\n",
    "# METADATA ********************\n",
    "\n",
    "# META {\n",
    "# META   \"language\": \"python\",\n",
    "# META   \"language_group\": \"jupyter_python\"\n",
    "# META }\n",
    "\n",
    "# MARKDOWN ********************\n",
    "\n",
    "# ## Lakehouses\n",
    "\n",
    "# CELL ********************\n",
    "\n",
    "for environment in environments:\n",
    "    print(f\"\\n--------------------------\")\n",
    "    print(f\"Processing: {environment['environment_name']}\")\n",
    "    for workspace in [environment['workspaces']['data']]:\n",
    "        exclude = ['LH_GOLD_LAYER.Lakehouse']\n",
    "\n",
    "        for it in lakehouse_deployment:\n",
    "\n",
    "            new_id = None\n",
    "            \n",
    "            name = it[\"name\"]\n",
    "            type = it[\"type\"]\n",
    "\n",
    "            if name in exclude:\n",
    "                continue\n",
    "            deploy_item(workspace,workspace['name'],name,mapping_table, environment['environment_name'],connection_list, tasks, lakehouse_schema_enabled,None,it)\n",
    "\n",
    "   \n",
    "\n",
    "# METADATA ********************\n",
    "\n",
    "# META {\n",
    "# META   \"language\": \"python\",\n",
    "# META   \"language_group\": \"jupyter_python\"\n",
    "# META }\n",
    "\n",
    "# CELL ********************\n",
    "\n",
    "for workspace_gold in domain_deployment:\n",
    "        exclude = ['LH_DATA_LANDINGZONE.Lakehouse','LH_BRONZE_LAYER.Lakehouse','LH_SILVER_LAYER.Lakehouse']\n",
    "        for sub_domain_name in sub_domain_names:\n",
    "\n",
    "            for workspace in [workspace_gold['workspaces']['gold']]:\n",
    "                for it in lakehouse_deployment:\n",
    "\n",
    "                    new_id = None\n",
    "                    \n",
    "                    name = it[\"name\"]\n",
    "                    type = it[\"type\"]\n",
    "\n",
    "                    if name in exclude:\n",
    "                        continue\n",
    "                        \n",
    "                    # Only execute if workspace is 'gold'\n",
    "\n",
    "                    deploy_item(workspace_gold,FrameworkName +' '+ sub_domain_name + ' DATA ('+ workspace_gold['environment_short']+') FMD', name,mapping_table, workspace_gold['environment_name'],connection_list, tasks, lakehouse_schema_enabled,None,it)\n",
    "                \n",
    "\n",
    "# METADATA ********************\n",
    "\n",
    "# META {\n",
    "# META   \"language\": \"python\",\n",
    "# META   \"language_group\": \"jupyter_python\"\n",
    "# META }\n",
    "\n",
    "# MARKDOWN ********************\n",
    "\n",
    "# ## Fabric Database\n",
    "\n",
    "# CELL ********************\n",
    "\n",
    "for target_item in data_deployment:\n",
    "    if target_item['type'] in ('SQLDatabase','SQLEndpoint'):\n",
    "        target_item['name'] = configuration['DatabaseName']\n",
    "\n",
    "start = time()\n",
    "print(f\"\\n -----\")\n",
    "print(f\" - Processing: workspace {configuration['workspace']['name']}\")\n",
    "workspace_id, status = ensure_workspace_exists(workspace=configuration['workspace'],workspace_name=configuration['workspace']['name'])\n",
    "empty = True\n",
    "   \n",
    "target_items = get_items(workspace_id)\n",
    "target_items = json.loads(target_items)[\"text\"]\n",
    "\n",
    "for deployment_item in data_deployment:\n",
    "    for target_item in target_items['value']:\n",
    "\n",
    "        if target_item['displayName'] == deployment_item['name'] \\\n",
    "                and target_item['type'] == deployment_item['type']:\n",
    "            print(f\" - Skip existing: {deployment_item['name']}, {deployment_item['type']}, {target_item['id']}\")\n",
    "            break\n",
    "    else:\n",
    "        \n",
    "        print(f\" - Creating: {deployment_item['name']} {deployment_item['type']} in workspace:{workspace_id} \")\n",
    "        item = deployment_item.copy()\n",
    "        \n",
    "        if empty:\n",
    "            if item.get('definition'):\n",
    "                print(f\" - Dropping definition\")\n",
    "                item.pop('definition')\n",
    "\n",
    "        # Construct the JSON payload\n",
    "        payload = json.dumps({\"displayName\": deployment_item['name'], \"Description\": \"Note: This item was initially generated by the FMD Framework. Any modifications may introduce breaking changes. For further details, please refer to the documentation at https://github.com/edkreuk/FMD_FRAMEWORK.\"})\n",
    "        if deployment_item['type'] in ('SQLDatabase'):\n",
    "            try:\n",
    "                raw_response = run_fab_command(\n",
    "                    f\"api -X post workspaces/{workspace_id}/SQLDatabases -i '{payload}'\",\n",
    "                    capture_output=True,\n",
    "                    silently_continue=True\n",
    "                )\n",
    "                response = json.loads(raw_response)\n",
    "\n",
    "                if isinstance(response, dict) and response.get(\"status_code\", 200) >= 400:\n",
    "                    print(f\"{response.get('status_code')=}\\n{response.get('reason')=}\\n{response.get('text')=}\\n{url=}\\n{payload=}\\n{payloadtype=}\")\n",
    "                    if response.get(\"errorCode\"):\n",
    "                        print(f\"{response['errorCode']}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}. Retrying in 60 seconds...\")\n",
    "                sleep(60)\n",
    "                raw_response = run_fab_command(\n",
    "                    f\"api -X post workspaces/{workspace_id}/SQLDatabases -i '{payload}'\",\n",
    "                    capture_output=True,\n",
    "                    silently_continue=True\n",
    "                )\n",
    "                response = json.loads(raw_response)\n",
    "\n",
    "            #wait so SQL Database can be deployed and we can pick up the Guid after\n",
    "            sleep(60)\n",
    "\n",
    "# METADATA ********************\n",
    "\n",
    "# META {\n",
    "# META   \"language\": \"python\",\n",
    "# META   \"language_group\": \"jupyter_python\"\n",
    "# META }\n",
    "\n",
    "# MARKDOWN ********************\n",
    "\n",
    "# ## Get Fabric database configuration\n",
    "\n",
    "# CELL ********************\n",
    "\n",
    "for target_item in data_deployment:\n",
    "    if target_item['type'] in ('SQLDatabase','SQLEndpoint'):\n",
    "        target_item['name'] = configuration['DatabaseName']\n",
    "   \n",
    "start = time()\n",
    "print(f\"\\n -----\")\n",
    "print(f\" - Processing: workspace {configuration['workspace']['name']}\")\n",
    "workspace_id, status = ensure_workspace_exists(configuration['workspace'],configuration['workspace']['name'])\n",
    "empty = True\n",
    "\n",
    "   \n",
    "target_items = get_items(workspace_id)\n",
    "target_items = json.loads(target_items)[\"text\"]\n",
    "for deployment_item in data_deployment:\n",
    "    for target_item in target_items['value']:\n",
    "\n",
    "        print(f\" - Updating mapping table: {deployment_item['name']} {deployment_item['type']} \")\n",
    "  \n",
    "        if deployment_item['type'] in ('SQLDatabase'):\n",
    "            if deployment_item.get('endpoint', '') != '':\n",
    "                return_item =  run_fab_command(f\"api -X get workspaces/{workspace_id}/SQLDatabases/{target_item['id']}\" , capture_output = True, silently_continue= True)\n",
    "                return_item = json.loads(return_item)[\"text\"]\n",
    "                if deployment_item['type'] in ('SQLDatabase'):\n",
    "                    if return_item.get(\"properties\", {}).get(\"serverFqdn\", \"\") != '':\n",
    "                        deployment_item[\"connectionString\"] = return_item[\"properties\"][\"serverFqdn\"].replace(',1433', '')\n",
    "                        mapping_table.append({\"Description\":deployment_item['name'] , \"ItemType\": \"Database\", \"environment\": 'config', \"old_id\": config[\"database\"][\"id\"], \"new_id\": target_item['id']})\n",
    "                        mapping_table.append({\"Description\":deployment_item['name'] , \"ItemType\": \"Endpoint\", \"environment\": 'config', \"old_id\": deployment_item[\"endpoint\"], \"new_id\": deployment_item[\"connectionString\"]})\n",
    "                        mapping_table.append({\"Description\":deployment_item['name'] , \"ItemType\": \"Database\", \"environment\": 'config', \"old_id\": deployment_item[\"database\"], \"new_id\": return_item[\"properties\"][\"databaseName\"]})\n",
    "                        mapping_table.append({\"Description\":deployment_item['name'] , \"ItemType\": \"displayName\", \"environment\": 'config', \"old_id\": config[\"database\"][\"displayName\"], \"new_id\": return_item[\"displayName\"]})\n",
    "                        tasks.append({\"task_name\":f\"Update item  Definition  {configuration['workspace']['name']} - {name}\", \"task_duration\": int(time() - start), \"status\": \"success\"})\n",
    "                    if return_item.get(\"properties\", {}).get(\"databaseName\", \"\") != '':\n",
    "                        deployment_item[\"databaseName\"] = return_item[\"properties\"][\"databaseName\"]\n",
    "\n",
    "\n",
    "# METADATA ********************\n",
    "\n",
    "# META {\n",
    "# META   \"language\": \"python\",\n",
    "# META   \"language_group\": \"jupyter_python\"\n",
    "# META }\n",
    "\n",
    "# MARKDOWN ********************\n",
    "\n",
    "# # Deploy Workspace Icons\n",
    "\n",
    "# CELL ********************\n",
    "\n",
    "seen = set()\n",
    "workspaces = []\n",
    "\n",
    "for item in mapping_table:\n",
    "    if item['ItemType'] == 'Workspace':\n",
    "        key = (item['Description'], item['new_id'])\n",
    "        if key not in seen:\n",
    "            seen.add(key)\n",
    "            workspaces.append({'displayName': item['Description'], 'id': item['new_id']})\n",
    "fabric_icons = fabric_icons_fmd \n",
    "\n",
    "for workspace in workspaces:\n",
    "    display_name = workspace['displayName'].lower()\n",
    "    \n",
    "    # Check if it's a \"gold\" workspace\n",
    "    is_gold = any(sub.lower() in display_name for sub in sub_domain_names) and 'data' in display_name\n",
    "\n",
    "    display_name = 'gold' if is_gold else display_name\n",
    "\n",
    "    # Assign icon\n",
    "    for icon_key, icon_value in workspace_icon_def['icons'].items():\n",
    "        if icon_key in display_name:\n",
    "            workspace[\"icon\"] = icon_value\n",
    "            workspace_icon = fabric_icons.get(icon_value)\n",
    "            break\n",
    "    else:\n",
    "        workspace[\"icon\"] = None\n",
    "        workspace_icon = None\n",
    "\n",
    "    workspace[\"icon_base64img\"] = workspace_icon\n",
    "\n",
    "# METADATA ********************\n",
    "\n",
    "# META {\n",
    "# META   \"language\": \"python\",\n",
    "# META   \"language_group\": \"jupyter_python\"\n",
    "# META }\n",
    "\n",
    "# CELL ********************\n",
    "\n",
    "if assign_icons:\n",
    "    # Dry run - Display pre and post icons based on specified workspace filters and workspace icon definition. Will NOT update any icons!\n",
    "    display_workspace_icons(workspaces)\n",
    "    sleep(2)\n",
    "\n",
    "# METADATA ********************\n",
    "\n",
    "# META {\n",
    "# META   \"language\": \"python\",\n",
    "# META   \"language_group\": \"jupyter_python\"\n",
    "# META }\n",
    "\n",
    "# CELL ********************\n",
    "\n",
    "if assign_icons:\n",
    "    for workspace in workspaces:\n",
    "            set_workspace_icon(workspace.get('id'), workspace.get('icon_base64img'))\n",
    "\n",
    "# METADATA ********************\n",
    "\n",
    "# META {\n",
    "# META   \"language\": \"python\",\n",
    "# META   \"language_group\": \"jupyter_python\"\n",
    "# META }\n",
    "\n",
    "# MARKDOWN ********************\n",
    "\n",
    "# # Deploy Items\n",
    "\n",
    "# CELL ********************\n",
    "\n",
    "mapping_table\n",
    "\n",
    "\n",
    "# METADATA ********************\n",
    "\n",
    "# META {\n",
    "# META   \"language\": \"python\",\n",
    "# META   \"language_group\": \"jupyter_python\"\n",
    "# META }\n",
    "\n",
    "# CELL ********************\n",
    "\n",
    "for environment in environments:\n",
    "    print(f\"\\n--------------------------\")\n",
    "    print(f\"Processing: {environment['environment_name']}\")\n",
    "    for workspace in [environment['workspaces']['code']]:\n",
    "        exclude = []\n",
    "\n",
    "        for it in item_deployment:\n",
    "\n",
    "            new_id = None\n",
    "            \n",
    "            name = it[\"name\"]\n",
    "            type = it[\"type\"]\n",
    "\n",
    "            if name in exclude:\n",
    "                continue\n",
    "            deploy_item(workspace,workspace['name'],name,mapping_table,environment['environment_name'],connection_list, tasks, lakehouse_schema_enabled, None,it)\n",
    "            \n",
    "\n",
    "            for child in it.get(\"children\",[]):\n",
    "                child_name = child[\"name\"]\n",
    "                print(child_name)\n",
    "                deploy_item(workspace,workspace['name'],name,mapping_table, environment['environment_name'],connection_list, tasks, lakehouse_schema_enabled,child_name,child) \n",
    "\n",
    "# METADATA ********************\n",
    "\n",
    "# META {\n",
    "# META   \"language\": \"python\",\n",
    "# META   \"language_group\": \"jupyter_python\"\n",
    "# META }\n",
    "\n",
    "# MARKDOWN ********************\n",
    "\n",
    "# ## Create SQL deployment Manifest\n",
    "\n",
    "# MARKDOWN ********************\n",
    "\n",
    "# ### Add Connection to Fabric Database\n",
    "\n",
    "# CELL ********************\n",
    "\n",
    "custom_sql_deployment = {\"queries_stored_procedures\": []}\n",
    "for connection in connections['value']:\n",
    "    \n",
    "    display_name = connection.get('displayName', '')\n",
    "    if display_name and display_name.startswith('CON_FMD'):\n",
    "        connection_type = connection.get('connectionDetails', {}).get('type', 'Unknown')\n",
    "        connection_id = connection.get('id')\n",
    "      \n",
    "        exec_statement = (\n",
    "            f\"EXEC [integration].[sp_UpsertConnection] \"\n",
    "            f\"@ConnectionGuid = \\\"{connection_id}\\\", \"\n",
    "            f\"@Name = \\\"{display_name}\\\", \"\n",
    "            f\"@Type = \\\"{connection_type}\\\", \"\n",
    "            f\"@IsActive = 1\"\n",
    "        )\n",
    "        custom_sql_deployment[\"queries_stored_procedures\"].append(exec_statement)\n",
    "\n",
    "\n",
    "# METADATA ********************\n",
    "\n",
    "# META {\n",
    "# META   \"language\": \"python\",\n",
    "# META   \"language_group\": \"jupyter_python\"\n",
    "# META }\n",
    "\n",
    "# MARKDOWN ********************\n",
    "\n",
    "# ### Add Workspaces to Fabric Database\n",
    "\n",
    "# CELL ********************\n",
    "\n",
    "workspaces = []\n",
    "workspaces.append(configuration['workspace'])\n",
    "\n",
    "for environment in environments:\n",
    "    workspaces.append(environment['workspaces']['code'])\n",
    "    workspaces.append(environment['workspaces']['data'])\n",
    "\n",
    "    \n",
    "for workspace in workspaces:\n",
    "    print(f'EXEC [integration].[sp_UpsertWorkspace](@WorkspaceId = \"{workspace[\"id\"]}\" ,@Name = \"{workspace[\"name\"]}\")')\n",
    "    custom_sql_deployment[\"queries_stored_procedures\"].append(f'EXEC [integration].[sp_UpsertWorkspace] @WorkspaceId = \"{workspace[\"id\"]}\", @Name = \"{workspace[\"name\"]}\"')\n",
    "\n",
    "# METADATA ********************\n",
    "\n",
    "# META {\n",
    "# META   \"language\": \"python\",\n",
    "# META   \"language_group\": \"jupyter_python\"\n",
    "# META }\n",
    "\n",
    "# MARKDOWN ********************\n",
    "\n",
    "# ### Add Data Pipelines to Fabric Database\n",
    "\n",
    "# CELL ********************\n",
    "\n",
    "for environment in environments:\n",
    "    result = run_fab_command(f\"api -X get workspaces/{environment['workspaces']['code']['id']}/items\", capture_output=True, silently_continue=True)\n",
    "    existing_items = json.loads(result)['text']\n",
    "    for item in existing_items.get('value', []):\n",
    "        if item['type'] == 'DataPipeline':\n",
    "            print(f'EXEC [integration].[sp_UpsertPipeline] @PipelineId = \"{item[\"id\"]}\", @WorkspaceId = \"{environment[\"workspaces\"][\"code\"][\"id\"]}\" ,@Name = \"{item[\"displayName\"]}\"')\n",
    "            custom_sql_deployment[\"queries_stored_procedures\"].append(f'EXEC [integration].[sp_UpsertPipeline] @PipelineId = \"{item[\"id\"]}\", @WorkspaceId = \"{environment[\"workspaces\"][\"data\"][\"id\"]}\" ,@Name = \"{item[\"displayName\"]}\"')\n",
    "\n",
    "# METADATA ********************\n",
    "\n",
    "# META {\n",
    "# META   \"language\": \"python\",\n",
    "# META   \"language_group\": \"jupyter_python\"\n",
    "# META }\n",
    "\n",
    "# MARKDOWN ********************\n",
    "\n",
    "# ### Add Lakehouses to Fabric Database\n",
    "\n",
    "# CELL ********************\n",
    "\n",
    "for environment in environments:\n",
    "    result = run_fab_command(f\"api -X get workspaces/{environment['workspaces']['data']['id']}/items\", capture_output=True, silently_continue=True)\n",
    "    existing_items = json.loads(result)['text']\n",
    "    for item in existing_items.get('value', []):\n",
    "        if item['type'] == 'Lakehouse':\n",
    "            print(f'EXEC [integration].[sp_UpsertLakehouse] @LakehouseId = \"{item[\"id\"]}\", @WorkspaceId = \"{environment[\"workspaces\"][\"data\"][\"id\"]}\" ,@Name = \"{item[\"displayName\"]}\"')\n",
    "            custom_sql_deployment[\"queries_stored_procedures\"].append(f'EXEC [integration].[sp_UpsertLakehouse] @LakehouseId = \"{item[\"id\"]}\", @WorkspaceId = \"{environment[\"workspaces\"][\"data\"][\"id\"]}\" ,@Name = \"{item[\"displayName\"]}\"')\n",
    "\n",
    "# METADATA ********************\n",
    "\n",
    "# META {\n",
    "# META   \"language\": \"python\",\n",
    "# META   \"language_group\": \"jupyter_python\"\n",
    "# META }\n",
    "\n",
    "# MARKDOWN ********************\n",
    "\n",
    "# ### Add Demo data for testing to Fabric Database\n",
    "\n",
    "# CELL ********************\n",
    "\n",
    "if load_demo_data:  \n",
    "    demo_sql_deployment = {\"queries_stored_procedures\": []}\n",
    "    demo_sql_deployment[\"queries_stored_procedures\"].append(f'EXEC [integration].[sp_UpsertConnection] @ConnectionGuid = \"00000000-0000-0000-0000-000000000000\", @Name = \"CON_FMD_ONELAKE\", @Type = \"ONELAKE\", @IsActive = 1')\n",
    "    demo_sql_deployment[\"queries_stored_procedures\"].append(\"\"\"\n",
    "        DECLARE @DataSourceIdInternal INT = (SELECT DataSourceId FROM integration.DataSource WHERE Name = 'LH_DATA_LANDINGZONE' and Type='ONELAKE_TABLES_01')\n",
    "        DECLARE @ConnectionIdInternal INT = (SELECT ConnectionId FROM integration.Connection WHERE ConnectionGuid = '00000000-0000-0000-0000-000000000000')\n",
    "        EXECUTE [integration].[sp_UpsertDataSource] \n",
    "            @ConnectionId = @ConnectionIdInternal\n",
    "            ,@DataSourceId = @DataSourceIdInternal\n",
    "            ,@Name = 'LH_DATA_LANDINGZONE'\n",
    "            ,@Namespace = 'ONELAKE'\n",
    "            ,@Type = 'ONELAKE_TABLES_01'\n",
    "            ,@Description = 'ONELAKE_TABLES'\n",
    "            ,@IsActive = 1\n",
    "    \"\"\")\n",
    "    demo_sql_deployment[\"queries_stored_procedures\"].append(\"\"\"\n",
    "        DECLARE @DataSourceIdInternal INT = (SELECT DataSourceId FROM integration.DataSource WHERE Name = 'LH_DATA_LANDINGZONE' and Type ='ONELAKE_FILES_01')\n",
    "        DECLARE @ConnectionIdInternal INT = (SELECT ConnectionId FROM integration.Connection WHERE ConnectionGuid = '00000000-0000-0000-0000-000000000000')\n",
    "        EXECUTE [integration].[sp_UpsertDataSource] \n",
    "            @ConnectionId = @ConnectionIdInternal\n",
    "            ,@DataSourceId = @DataSourceIdInternal\n",
    "            ,@Name = 'LH_DATA_LANDINGZONE'\n",
    "            ,@Namespace = 'ONELAKE'\n",
    "            ,@Type = 'ONELAKE_FILES_01'\n",
    "            ,@Description = 'ONELAKE_FILES'\n",
    "            ,@IsActive = 1\n",
    "    \"\"\")\n",
    "    demo_sql_deployment[\"queries_stored_procedures\"].append(\"\"\"\n",
    "        DECLARE @LandingzoneEntityIdInternal INT = (SELECT LandingzoneEntityId FROM integration.LandingzoneEntity WHERE SourceSchema = 'in' and SourceName = 'customer')\n",
    "        DECLARE @DataSourceIdInternal INT = (SELECT DataSourceId FROM integration.DataSource WHERE Name = 'LH_DATA_LANDINGZONE' and Type='ONELAKE_TABLES_01')\n",
    "        DECLARE @LakehouseIdInternal INT = (SELECT top 1 LakehouseId FROM integration.Lakehouse WHERE Name = 'LH_DATA_LANDINGZONE')\n",
    "        EXECUTE [integration].[sp_UpsertLandingzoneEntity] \n",
    "            @LandingzoneEntityId = @LandingzoneEntityIdInternal\n",
    "            ,@DataSourceId = @DataSourceIdInternal\n",
    "            ,@LakehouseId = @LakehouseIdInternal\n",
    "            ,@SourceSchema = 'in'\n",
    "            ,@SourceName = 'customer'\n",
    "            ,@SourceCustomSelect = ''\n",
    "            ,@FileName = 'customer'\n",
    "            ,@FilePath = 'fmd'\n",
    "            ,@FileType = 'parquet'\n",
    "            ,@IsIncremental = 0\n",
    "            ,@IsIncrementalColumn = ''\n",
    "            ,@IsActive = 1\n",
    "    \"\"\")\n",
    "    demo_sql_deployment[\"queries_stored_procedures\"].append(\"\"\"\n",
    "        DECLARE @LandingzoneEntityIdInternal INT = (SELECT LandingzoneEntityId FROM integration.LandingzoneEntity WHERE SourceSchema = 'in' and SourceName = 'customer')\n",
    "        DECLARE @BronzeLayerEntityIdInternal INT = (SELECT BronzeLayerEntityId FROM integration.BronzeLayerEntity WHERE [Schema] = 'in' and [Name] = 'customer')\n",
    "        DECLARE @LakehouseIdInternal INT = (SELECT top 1 LakehouseId FROM integration.Lakehouse WHERE Name = 'LH_BRONZE_LAYER')\n",
    "        EXECUTE [integration].[sp_UpsertBronzeLayerEntity] \n",
    "            @BronzeLayerEntityId = @BronzeLayerEntityIdInternal\n",
    "            ,@LandingzoneEntityId = @LandingzoneEntityIdInternal\n",
    "            ,@Schema = 'in'\n",
    "            ,@Name = 'customer'\n",
    "            ,@FileType = 'Delta'\n",
    "            ,@LakehouseId = @LakehouseIdInternal\n",
    "            ,@PrimaryKeys = 'CustomerId'\n",
    "            ,@IsActive = 1\n",
    "    \"\"\")\n",
    "    demo_sql_deployment[\"queries_stored_procedures\"].append(\"\"\"\n",
    "        DECLARE @BronzeLayerEntityIdInternal INT = (SELECT BronzeLayerEntityId FROM integration.BronzeLayerEntity WHERE [Schema] = 'in' and [Name] = 'customer')\n",
    "        DECLARE @SilverLayerEntityIdInternal INT = (SELECT SilverLayerEntityId FROM integration.SilverLayerEntity WHERE [Schema] = 'in' and [Name] = 'customer')\n",
    "        DECLARE @LakehouseIdInternal INT = (SELECT top 1 LakehouseId FROM integration.Lakehouse WHERE Name = 'LH_SILVER_LAYER')\n",
    "        EXECUTE [integration].[sp_UpsertSilverLayerEntity] \n",
    "            @SilverLayerEntityId = @SilverLayerEntityIdInternal\n",
    "            ,@BronzeLayerEntityId = @BronzeLayerEntityIdInternal\n",
    "            ,@LakehouseId = @LakehouseIdInternal\n",
    "            ,@Name = 'customer'\n",
    "            ,@Schema = 'in'\n",
    "            ,@FileType = 'delta'\n",
    "            ,@IsActive = 1\n",
    "    \"\"\")\n",
    "\n",
    "# METADATA ********************\n",
    "\n",
    "# META {\n",
    "# META   \"language\": \"python\",\n",
    "# META   \"language_group\": \"jupyter_python\"\n",
    "# META }\n",
    "\n",
    "# MARKDOWN ********************\n",
    "\n",
    "# ## Deploy SQL Code\n",
    "\n",
    "# CELL ********************\n",
    "\n",
    "for target_item in data_deployment:\n",
    "    if isinstance(target_item, dict) and target_item.get('type') == 'SQLDatabase':\n",
    "        connstring = target_item.get(\"connectionString\")\n",
    "        database = target_item.get('databaseName')\n",
    "\n",
    "try:\n",
    "    i = 0\n",
    "\n",
    "    token = notebookutils.credentials.getToken('pbi').encode('utf-16-le')\n",
    "    token_struct = struct.pack(f'<I{len(token)}s', len(token), token)\n",
    "\n",
    "    print(f\"DRIVER={driver};SERVER={connstring};PORT=1433;DATABASE={database};\")\n",
    "    connection = pyodbc.connect(f\"DRIVER={driver};SERVER={connstring};PORT=1433;DATABASE={database};\", attrs_before={1256:token_struct}, timeout=12)\n",
    "\n",
    "    with connection.cursor() as cursor:\n",
    "        cursor.execute(\"SELECT 1\")  # Execute the warm-up query (a simple query like 'SELECT 1' can be used)\n",
    "        cursor.fetchone()\n",
    "        connection.timeout = 10  # Setting a lower timeout for subsequent queries\n",
    "    for it in sql_deployment:\n",
    "        for i, query in enumerate(it[\"queries_schemas\"]):\n",
    "            print(f' - execute \"{query}\"')\n",
    "            cursor.execute(query)\n",
    "            cursor.commit()\n",
    "        for i, query in enumerate(it[\"queries_tables\"]):\n",
    "            print(f' - execute \"{query}\"')\n",
    "            cursor.execute(query)\n",
    "            cursor.commit()\n",
    "        for i, query in enumerate(it[\"queries_views\"]):\n",
    "            print(f' - execute \"{query}\"')\n",
    "            cursor.execute(query)\n",
    "            cursor.commit()\n",
    "        for i, query in enumerate(it[\"queries_stored_procedures\"]):\n",
    "            print(f' - execute \"{query}\"')\n",
    "            cursor.execute(query)\n",
    "            cursor.commit()\n",
    "        for i, query in enumerate(it[\"queries_logging\"]):\n",
    "            print(f' - execute \"{query}\"')\n",
    "            cursor.execute(query)\n",
    "            cursor.commit()\n",
    "        for i, query in enumerate(custom_sql_deployment[\"queries_stored_procedures\"]):\n",
    "            print(f' - execute \"{query}\"')\n",
    "            cursor.execute(query)\n",
    "            cursor.commit()\n",
    "        for i, query in enumerate(demo_sql_deployment[\"queries_stored_procedures\"]):\n",
    "            print(f' - execute \"{query}\"')\n",
    "            cursor.execute(query)\n",
    "            cursor.commit()\n",
    "\n",
    "    tasks.append({\"task_name\":f\"{workspace.get('displayName')} {database} query {i}\", \"task_duration\": 1, \"status\": f\"success\"})\n",
    "except pyodbc.OperationalError as e:\n",
    "    print(e) \n",
    "    tasks.append({\"task_name\":f\"{workspace.get('displayName')} {database} query {i}\", \"task_duration\": 1, \"status\": f\"pyodbc failed: {e}\"})\n",
    "except Exception as e:\n",
    "    print(e) \n",
    "    tasks.append({\"task_name\":f\"{workspace.get('displayName')} {database} query {i}\", \"task_duration\": 1, \"status\": f\"failed: {e}\"})\n",
    "\n",
    "# METADATA ********************\n",
    "\n",
    "# META {\n",
    "# META   \"language\": \"python\",\n",
    "# META   \"language_group\": \"jupyter_python\"\n",
    "# META }\n",
    "\n",
    "# CELL ********************\n",
    "\n",
    "display(tasks)\n",
    "\n",
    "# METADATA ********************\n",
    "\n",
    "# META {\n",
    "# META   \"language\": \"python\",\n",
    "# META   \"language_group\": \"jupyter_python\"\n",
    "# META }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33eb5fa4-5bf7-408b-9630-1533ba0d031a",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Load configuration\n",
    "Create  workspace, identity and roles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937feb4e-a67f-486c-ad43-0ae628b71e34",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "base_path = './builtin/'\n",
    "config_path = os.path.join(base_path, 'config/item_config.yaml')\n",
    "\n",
    "with open(config_path, 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "\n",
    "deploy_order_path = os.path.join(base_path, 'config/item_deployment.json')\n",
    "with open(deploy_order_path, 'r') as file:\n",
    "        item_deployment =json.load(file)\n",
    "\n",
    "deploy_order_path = os.path.join(base_path, 'config/sql_deployment.json')\n",
    "with open(deploy_order_path, 'r') as file:\n",
    "        sql_deployment =json.load(file)\n",
    "\n",
    "deploy_order_path = os.path.join(base_path, 'config/data_deployment.json')\n",
    "with open(deploy_order_path, 'r') as file:\n",
    "        data_deployment =json.load(file)\n",
    "\n",
    "deploy_order_path = os.path.join(base_path, 'config/lakehouse_deployment.json')\n",
    "with open(deploy_order_path, 'r') as file:\n",
    "        lakehouse_deployment =json.load(file)\n",
    "\n",
    "mapping_table=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c962f96-4a74-41ad-a6e0-397622eefbf9",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Deploy workspaces(Code and Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe3bd18-4ed8-4f57-b9c4-b83ca72768f9",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "for environment in environments:\n",
    "    print(f\"--------------------------\")\n",
    "    print(f\"Updating Workspace: {environment['environment_name']}\")\n",
    "    deploy_workspaces(workspace=environment['workspaces']['code'], workspace_name=environment['workspaces']['code']['name'], environment_name=environment['environment_name'], old_id=config[\"workspaces\"][\"workspace_code\"], mapping_table=mapping_table, tasks=tasks)\n",
    "    deploy_workspaces(workspace=environment['workspaces']['data'], workspace_name=environment['workspaces']['data']['name'], environment_name=environment['environment_name'], old_id=config[\"workspaces\"][\"workspace_data\"], mapping_table=mapping_table, tasks=tasks)\n",
    "\n",
    "    # Append the remaining connections\n",
    "    mapping_table.append({\"Description\": \"CON_FMD_FABRIC_SQL\" ,\"environment\": environment['environment_name'],\"old_id\": config['connections'][\"CON_FMD_FABRIC_SQL\"], \"new_id\": environment['connections']['CON_FMD_FABRIC_SQL']})\n",
    "    mapping_table.append({\"Description\": \"CON_FMD_FABRIC_PIPELINES\" ,\"environment\": environment['environment_name'] ,\"old_id\": config[\"connections\"][\"CON_FMD_FABRIC_PIPELINES\"], \"new_id\": environment['connections']['CON_FMD_FABRIC_PIPELINES']})\n",
    "    mapping_table.append({\"Description\": \"CON_FMD_ADF_PIPELINES\" ,\"environment\": environment['environment_name'] ,\"old_id\": config[\"connections\"][\"CON_FMD_ADF_PIPELINES\"], \"new_id\": environment['connections']['CON_FMD_ADF_PIPELINES']})\n",
    "\n",
    "print(f\"--------------------------\")   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c278cbf-efca-46a6-b8b7-0b1d9a19aba9",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Deploy workspaces(Domains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07949aa-d029-4e69-addd-14e03fbae79d",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "for domain_name in datamesh_domains:\n",
    "\n",
    "    for domain in domain_deployment:\n",
    "        print(f\"--------------------------\")\n",
    "        print(f\"Updating Workspace: {domain['environment_name']}\")\n",
    "        deploy_workspaces(workspace=domain['workspaces']['gold'],workspace_name=FrameworkName +' '+ domain_name + ' DATA ('+domain['environment_short']+') FMD',  environment_name=domain['environment_name'], old_id=config[\"workspaces\"][\"workspace_gold\"], mapping_table=mapping_table, tasks=tasks)\n",
    "        deploy_workspaces(workspace=domain['workspaces']['reporting'],workspace_name=FrameworkName +' '+ domain_name + ' REPORTING ('+domain['environment_short']+') FMD', environment_name=domain['environment_name'], old_id=config[\"workspaces\"][\"workspace_reporting\"], mapping_table=mapping_table, tasks=tasks)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b978e7-12ed-4697-a9fd-dc55a947a44e",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Deploy Workspace(Config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc31d7e-150e-440d-ad2d-95974b74babe",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Updating Workspace: Configuration\")\n",
    "deploy_workspaces(workspace=configuration['workspace'], workspace_name=configuration['workspace']['name'], environment_name='config', old_id=config[\"workspaces\"][\"workspace_config\"], mapping_table=mapping_table, tasks=tasks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7ae39f-a061-42d7-b33a-fa4c546b0574",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Deploy Lakehouses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcc14a9-0843-4f05-91c5-5efced447f73",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "for environment in environments:\n",
    "    print(f\"\\n--------------------------\")\n",
    "    print(f\"Processing: {environment['environment_name']}\")\n",
    "    for workspace in [environment['workspaces']['data']]:\n",
    "        exclude = ['LH_GOLD_LAYER.Lakehouse']\n",
    "\n",
    "        for it in lakehouse_deployment:\n",
    "\n",
    "            new_id = None\n",
    "            \n",
    "            name = it[\"name\"]\n",
    "            type = it[\"type\"]\n",
    "\n",
    "            if name in exclude:\n",
    "                continue\n",
    "            deploy_item(workspace,workspace['name'],name,mapping_table, environment['environment_name'],connection_list, tasks, lakehouse_schema_enabled,None,it)\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8715fa24-16d7-4768-82d4-5c03e1a18258",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "for domain in domain_deployment:\n",
    "        exclude = ['LH_DATA_LANDINGZONE.Lakehouse','LH_BRONZE_LAYER.Lakehouse','LH_SILVER_LAYER.Lakehouse']\n",
    "        for domain_name in datamesh_domains:\n",
    "\n",
    "            for workspace in [domain['workspaces']['gold']]:\n",
    "                for it in lakehouse_deployment:\n",
    "\n",
    "                    new_id = None\n",
    "                    \n",
    "                    name = it[\"name\"]\n",
    "                    type = it[\"type\"]\n",
    "\n",
    "                    if name in exclude:\n",
    "                        continue\n",
    "                        \n",
    "                    # Only execute if workspace is 'gold'\n",
    "\n",
    "                    deploy_item(workspace,FrameworkName +' '+ domain_name + ' DATA ('+domain['environment_short']+') FMD', name,mapping_table, domain['environment_name'],connection_list, tasks, lakehouse_schema_enabled,None,it)\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5a35e1-75e0-40f4-9e81-107f9654bccc",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Deploy Fabric Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834a61eb-31dc-48ee-ab6a-08b9cf3439c0",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "for target_item in data_deployment:\n",
    "    if target_item['type'] in ('SQLDatabase','SQLEndpoint'):\n",
    "        target_item['name'] = configuration['DatabaseName']\n",
    "\n",
    "start = time()\n",
    "print(f\"\\n -----\")\n",
    "print(f\" - Processing: workspace {configuration['workspace']['name']}\")\n",
    "workspace_id, status = ensure_workspace_exists(workspace=configuration['workspace'],workspace_name=configuration['workspace']['name'])\n",
    "empty = True\n",
    "   \n",
    "target_items = fab_get_items(workspace_id)\n",
    "target_items = json.loads(target_items)[\"text\"]\n",
    "\n",
    "for deployment_item in data_deployment:\n",
    "    for target_item in target_items['value']:\n",
    "\n",
    "        if target_item['displayName'] == deployment_item['name'] \\\n",
    "                and target_item['type'] == deployment_item['type']:\n",
    "            print(f\" - Skip existing: {deployment_item['name']}, {deployment_item['type']}, {target_item['id']}\")\n",
    "            break\n",
    "    else:\n",
    "        \n",
    "        print(f\" - Creating: {deployment_item['name']} {deployment_item['type']} in workspace:{workspace_id} \")\n",
    "        item = deployment_item.copy()\n",
    "        \n",
    "        if empty:\n",
    "            if item.get('definition'):\n",
    "                print(f\" - Dropping definition\")\n",
    "                item.pop('definition')\n",
    "\n",
    "        # Construct the JSON payload\n",
    "        payload = json.dumps({\"displayName\": deployment_item['name'], \"Description\": \"Note: This item was initially generated by the FMD Framework. Any modifications may introduce breaking changes. For further details, please refer to the documentation at https://github.com/edkreuk/FMD_FRAMEWORK.\"})\n",
    "        if deployment_item['type'] in ('SQLDatabase'):\n",
    "            try:\n",
    "                raw_response = run_fab_command(\n",
    "                    f\"api -X post workspaces/{workspace_id}/SQLDatabases -i '{payload}'\",\n",
    "                    capture_output=True,\n",
    "                    silently_continue=True\n",
    "                )\n",
    "                response = json.loads(raw_response)\n",
    "\n",
    "                if isinstance(response, dict) and response.get(\"status_code\", 200) >= 400:\n",
    "                    print(f\"{response.get('status_code')=}\\n{response.get('reason')=}\\n{response.get('text')=}\\n{url=}\\n{payload=}\\n{payloadtype=}\")\n",
    "                    if response.get(\"errorCode\"):\n",
    "                        print(f\"{response['errorCode']}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}. Retrying in 60 seconds...\")\n",
    "                sleep(60)\n",
    "                raw_response = run_fab_command(\n",
    "                    f\"api -X post workspaces/{workspace_id}/SQLDatabases -i '{payload}'\",\n",
    "                    capture_output=True,\n",
    "                    silently_continue=True\n",
    "                )\n",
    "                response = json.loads(raw_response)\n",
    "\n",
    "            #wait so SQL Database can be deployed and we can pick up the Guid after\n",
    "            sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf50a36-141d-4096-b169-d9b235678049",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Get Fabric database configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837c87ad-dbad-443d-84e8-08adf6e7dd21",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "for target_item in data_deployment:\n",
    "    if target_item['type'] in ('SQLDatabase','SQLEndpoint'):\n",
    "        target_item['name'] = configuration['DatabaseName']\n",
    "   \n",
    "start = time()\n",
    "print(f\"\\n -----\")\n",
    "print(f\" - Processing: workspace {configuration['workspace']['name']}\")\n",
    "workspace_id, status = ensure_workspace_exists(configuration['workspace'],configuration['workspace']['name'])\n",
    "empty = True\n",
    "\n",
    "   \n",
    "target_items = fab_get_items(workspace_id)\n",
    "target_items = json.loads(target_items)[\"text\"]\n",
    "for deployment_item in data_deployment:\n",
    "    for target_item in target_items['value']:\n",
    "\n",
    "        print(f\" - Updating mapping table: {deployment_item['name']} {deployment_item['type']} \")\n",
    "  \n",
    "        if deployment_item['type'] in ('SQLDatabase'):\n",
    "            if deployment_item.get('endpoint', '') != '':\n",
    "                return_item =  run_fab_command(f\"api -X get workspaces/{workspace_id}/SQLDatabases/{target_item['id']}\" , capture_output = True, silently_continue= True)\n",
    "                return_item = json.loads(return_item)[\"text\"]\n",
    "                if deployment_item['type'] in ('SQLDatabase'):\n",
    "                    if return_item.get(\"properties\", {}).get(\"serverFqdn\", \"\") != '':\n",
    "                        deployment_item[\"connectionString\"] = return_item[\"properties\"][\"serverFqdn\"].replace(',1433', '')\n",
    "                        mapping_table.append({\"Description\":deployment_item['name'] , \"environment\": 'config', \"old_id\": config[\"database\"][\"id\"], \"new_id\": target_item['id']})\n",
    "                        mapping_table.append({\"Description\":deployment_item['name'] , \"environment\": 'config', \"old_id\": deployment_item[\"endpoint\"], \"new_id\": deployment_item[\"connectionString\"]})\n",
    "                        mapping_table.append({\"Description\":deployment_item['name'] , \"environment\": 'config', \"old_id\": deployment_item[\"database\"], \"new_id\": return_item[\"properties\"][\"databaseName\"]})\n",
    "                        mapping_table.append({\"Description\":deployment_item['name'] , \"environment\": 'config', \"old_id\": config[\"database\"][\"displayName\"], \"new_id\": return_item[\"displayName\"]})\n",
    "                        tasks.append({\"task_name\":f\"Update item  Definition  {configuration['workspace']['name']} - {name}\", \"task_duration\": int(time() - start), \"status\": \"success\"})\n",
    "                    if return_item.get(\"properties\", {}).get(\"databaseName\", \"\") != '':\n",
    "                        deployment_item[\"databaseName\"] = return_item[\"properties\"][\"databaseName\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cc05d7-a761-452b-8c24-32f3b3d24c02",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Deploy Items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296baf6d-620f-4d13-9bf9-85506b864b41",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "for environment in environments:\n",
    "    print(f\"\\n--------------------------\")\n",
    "    print(f\"Processing: {environment['environment_name']}\")\n",
    "    for workspace in [environment['workspaces']['code']]:\n",
    "        exclude = []\n",
    "\n",
    "        for it in item_deployment:\n",
    "\n",
    "            new_id = None\n",
    "            \n",
    "            name = it[\"name\"]\n",
    "            type = it[\"type\"]\n",
    "\n",
    "            if name in exclude:\n",
    "                continue\n",
    "            deploy_item(workspace,workspace['name'],name,mapping_table,environment['environment_name'],connection_list, tasks, lakehouse_schema_enabled, None,it)\n",
    "            \n",
    "\n",
    "            for child in it.get(\"children\",[]):\n",
    "                child_name = child[\"name\"]\n",
    "                print(child_name)\n",
    "                deploy_item(workspace,workspace['name'],name,mapping_table, environment['environment_name'],connection_list, tasks, lakehouse_schema_enabled,child_name,child) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc252ff-6f50-41f9-9c6d-165300837bb6",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Create SQL deployment Manifest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1031bbb1-ae9a-4a26-8e14-e942a4fca2f8",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Add Connection to Fabric Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438153da-a957-4c48-9394-3cfc7c8aa014",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "custom_sql_deployment = {\"queries_stored_procedures\": []}\n",
    "for connection in connections['value']:\n",
    "    \n",
    "    display_name = connection.get('displayName', '')\n",
    "    if display_name and display_name.startswith('CON_FMD'):\n",
    "        connection_type = connection.get('connectionDetails', {}).get('type', 'Unknown')\n",
    "        connection_id = connection.get('id')\n",
    "      \n",
    "        exec_statement = (\n",
    "            f\"EXEC [integration].[sp_UpsertConnection] \"\n",
    "            f\"@ConnectionGuid = \\\"{connection_id}\\\", \"\n",
    "            f\"@Name = \\\"{display_name}\\\", \"\n",
    "            f\"@Type = \\\"{connection_type}\\\", \"\n",
    "            f\"@IsActive = 1\"\n",
    "        )\n",
    "        custom_sql_deployment[\"queries_stored_procedures\"].append(exec_statement)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed267d7-4aca-445c-9189-243307a972e6",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Add Workspaces to Fabric Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a6fd01-081e-48a0-9210-89d895cf2eb9",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "workspaces = []\n",
    "workspaces.append(configuration['workspace'])\n",
    "\n",
    "for environment in environments:\n",
    "    workspaces.append(environment['workspaces']['code'])\n",
    "    workspaces.append(environment['workspaces']['data'])\n",
    "\n",
    "    \n",
    "for workspace in workspaces:\n",
    "    print(f'EXEC [integration].[sp_UpsertWorkspace](@WorkspaceId = \"{workspace[\"id\"]}\" ,@Name = \"{workspace[\"name\"]}\")')\n",
    "    custom_sql_deployment[\"queries_stored_procedures\"].append(f'EXEC [integration].[sp_UpsertWorkspace] @WorkspaceId = \"{workspace[\"id\"]}\", @Name = \"{workspace[\"name\"]}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aacd228-4711-4836-a45d-0818ea6dc14e",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Add Data Pipelines to Fabric Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa8751f-ddad-47ed-9de2-1fbe2530d76f",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "for environment in environments:\n",
    "    result = run_fab_command(f\"api -X get workspaces/{environment['workspaces']['code']['id']}/items\", capture_output=True, silently_continue=True)\n",
    "    existing_items = json.loads(result)['text']\n",
    "    for item in existing_items.get('value', []):\n",
    "        if item['type'] == 'DataPipeline':\n",
    "            print(f'EXEC [integration].[sp_UpsertPipeline] @PipelineId = \"{item[\"id\"]}\", @WorkspaceId = \"{environment[\"workspaces\"][\"code\"][\"id\"]}\" ,@Name = \"{item[\"displayName\"]}\"')\n",
    "            custom_sql_deployment[\"queries_stored_procedures\"].append(f'EXEC [integration].[sp_UpsertPipeline] @PipelineId = \"{item[\"id\"]}\", @WorkspaceId = \"{environment[\"workspaces\"][\"data\"][\"id\"]}\" ,@Name = \"{item[\"displayName\"]}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4867745a-f0e7-4a9a-b326-5609b1b5bb3a",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Add Lakehouses to Fabric Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc60a4a-1e3d-4a57-b2f7-2a8cb23115c6",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "for environment in environments:\n",
    "    result = run_fab_command(f\"api -X get workspaces/{environment['workspaces']['data']['id']}/items\", capture_output=True, silently_continue=True)\n",
    "    existing_items = json.loads(result)['text']\n",
    "    for item in existing_items.get('value', []):\n",
    "        if item['type'] == 'Lakehouse':\n",
    "            print(f'EXEC [integration].[sp_UpsertLakehouse] @LakehouseId = \"{item[\"id\"]}\", @WorkspaceId = \"{environment[\"workspaces\"][\"data\"][\"id\"]}\" ,@Name = \"{item[\"displayName\"]}\"')\n",
    "            custom_sql_deployment[\"queries_stored_procedures\"].append(f'EXEC [integration].[sp_UpsertLakehouse] @LakehouseId = \"{item[\"id\"]}\", @WorkspaceId = \"{environment[\"workspaces\"][\"data\"][\"id\"]}\" ,@Name = \"{item[\"displayName\"]}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d60c485-1f24-4dbc-807b-45a71e14cf55",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Add Demo data for testing to Fabric Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a476a2-d34d-4540-ad24-439d447c5289",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "if load_demo_data_data:  \n",
    "    demo_sql_deployment = {\"queries_stored_procedures\": []}\n",
    "    demo_sql_deployment[\"queries_stored_procedures\"].append(f'EXEC [integration].[sp_UpsertConnection] @ConnectionGuid = \"00000000-0000-0000-0000-000000000000\", @Name = \"CON_FMD_ONELAKE\", @Type = \"ONELAKE\", @IsActive = 1')\n",
    "    demo_sql_deployment[\"queries_stored_procedures\"].append(\"\"\"\n",
    "        DECLARE @DataSourceIdInternal INT = (SELECT DataSourceId FROM integration.DataSource WHERE Name = 'LH_DATA_LANDINGZONE' and Type='ONELAKE_TABLES_01')\n",
    "        DECLARE @ConnectionIdInternal INT = (SELECT ConnectionId FROM integration.Connection WHERE ConnectionGuid = '00000000-0000-0000-0000-000000000000')\n",
    "        EXECUTE [integration].[sp_UpsertDataSource] \n",
    "            @ConnectionId = @ConnectionIdInternal\n",
    "            ,@DataSourceId = @DataSourceIdInternal\n",
    "            ,@Name = 'LH_DATA_LANDINGZONE'\n",
    "            ,@Namespace = 'ONELAKE'\n",
    "            ,@Type = 'ONELAKE_TABLES_01'\n",
    "            ,@Description = 'ONELAKE_TABLES'\n",
    "            ,@IsActive = 1\n",
    "    \"\"\")\n",
    "    demo_sql_deployment[\"queries_stored_procedures\"].append(\"\"\"\n",
    "        DECLARE @DataSourceIdInternal INT = (SELECT DataSourceId FROM integration.DataSource WHERE Name = 'LH_DATA_LANDINGZONE' and Type ='ONELAKE_FILES_01')\n",
    "        DECLARE @ConnectionIdInternal INT = (SELECT ConnectionId FROM integration.Connection WHERE ConnectionGuid = '00000000-0000-0000-0000-000000000000')\n",
    "        EXECUTE [integration].[sp_UpsertDataSource] \n",
    "            @ConnectionId = @ConnectionIdInternal\n",
    "            ,@DataSourceId = @DataSourceIdInternal\n",
    "            ,@Name = 'LH_DATA_LANDINGZONE'\n",
    "            ,@Namespace = 'ONELAKE'\n",
    "            ,@Type = 'ONELAKE_FILES_01'\n",
    "            ,@Description = 'ONELAKE_FILES'\n",
    "            ,@IsActive = 1\n",
    "    \"\"\")\n",
    "    demo_sql_deployment[\"queries_stored_procedures\"].append(\"\"\"\n",
    "        DECLARE @LandingzoneEntityIdInternal INT = (SELECT LandingzoneEntityId FROM integration.LandingzoneEntity WHERE SourceSchema = 'in' and SourceName = 'customer')\n",
    "        DECLARE @DataSourceIdInternal INT = (SELECT DataSourceId FROM integration.DataSource WHERE Name = 'LH_DATA_LANDINGZONE' and Type='ONELAKE_TABLES_01')\n",
    "        DECLARE @LakehouseIdInternal INT = (SELECT top 1 LakehouseId FROM integration.Lakehouse WHERE Name = 'LH_DATA_LANDINGZONE')\n",
    "        EXECUTE [integration].[sp_UpsertLandingzoneEntity] \n",
    "            @LandingzoneEntityId = @LandingzoneEntityIdInternal\n",
    "            ,@DataSourceId = @DataSourceIdInternal\n",
    "            ,@LakehouseId = @LakehouseIdInternal\n",
    "            ,@SourceSchema = 'in'\n",
    "            ,@SourceName = 'customer'\n",
    "            ,@SourceCustomSelect = ''\n",
    "            ,@FileName = 'customer'\n",
    "            ,@FilePath = 'fmd'\n",
    "            ,@FileType = 'parquet'\n",
    "            ,@IsIncremental = 0\n",
    "            ,@IsIncrementalColumn = ''\n",
    "            ,@IsActive = 1\n",
    "    \"\"\")\n",
    "    demo_sql_deployment[\"queries_stored_procedures\"].append(\"\"\"\n",
    "        DECLARE @LandingzoneEntityIdInternal INT = (SELECT LandingzoneEntityId FROM integration.LandingzoneEntity WHERE SourceSchema = 'in' and SourceName = 'customer')\n",
    "        DECLARE @BronzeLayerEntityIdInternal INT = (SELECT BronzeLayerEntityId FROM integration.BronzeLayerEntity WHERE [Schema] = 'in' and [Name] = 'customer')\n",
    "        DECLARE @LakehouseIdInternal INT = (SELECT top 1 LakehouseId FROM integration.Lakehouse WHERE Name = 'LH_BRONZE_LAYER')\n",
    "        EXECUTE [integration].[sp_UpsertBronzeLayerEntity] \n",
    "            @BronzeLayerEntityId = @BronzeLayerEntityIdInternal\n",
    "            ,@LandingzoneEntityId = @LandingzoneEntityIdInternal\n",
    "            ,@Schema = 'in'\n",
    "            ,@Name = 'customer'\n",
    "            ,@FileType = 'Delta'\n",
    "            ,@LakehouseId = @LakehouseIdInternal\n",
    "            ,@PrimaryKeys = 'CustomerId'\n",
    "            ,@IsActive = 1\n",
    "    \"\"\")\n",
    "    demo_sql_deployment[\"queries_stored_procedures\"].append(\"\"\"\n",
    "        DECLARE @BronzeLayerEntityIdInternal INT = (SELECT BronzeLayerEntityId FROM integration.BronzeLayerEntity WHERE [Schema] = 'in' and [Name] = 'customer')\n",
    "        DECLARE @SilverLayerEntityIdInternal INT = (SELECT SilverLayerEntityId FROM integration.SilverLayerEntity WHERE [Schema] = 'in' and [Name] = 'customer')\n",
    "        DECLARE @LakehouseIdInternal INT = (SELECT top 1 LakehouseId FROM integration.Lakehouse WHERE Name = 'LH_SILVER_LAYER')\n",
    "        EXECUTE [integration].[sp_UpsertSilverLayerEntity] \n",
    "            @SilverLayerEntityId = @SilverLayerEntityIdInternal\n",
    "            ,@BronzeLayerEntityId = @BronzeLayerEntityIdInternal\n",
    "            ,@LakehouseId = @LakehouseIdInternal\n",
    "            ,@Name = 'customer'\n",
    "            ,@Schema = 'in'\n",
    "            ,@FileType = 'delta'\n",
    "            ,@IsActive = 1\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e3bfba-b56e-47c7-a1e5-c764c2c3e622",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Deploy SQL Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb6c1ec-7fac-46d6-812a-50c00e413c5e",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "for target_item in data_deployment:\n",
    "    if isinstance(target_item, dict) and target_item.get('type') == 'SQLDatabase':\n",
    "        connstring = target_item.get(\"connectionString\")\n",
    "        database = target_item.get('databaseName')\n",
    "\n",
    "try:\n",
    "    i = 0\n",
    "\n",
    "    token = notebookutils.credentials.getToken('pbi').encode('utf-16-le')\n",
    "    token_struct = struct.pack(f'<I{len(token)}s', len(token), token)\n",
    "\n",
    "    print(f\"DRIVER={driver};SERVER={connstring};PORT=1433;DATABASE={database};\")\n",
    "    connection = pyodbc.connect(f\"DRIVER={driver};SERVER={connstring};PORT=1433;DATABASE={database};\", attrs_before={1256:token_struct}, timeout=12)\n",
    "\n",
    "    with connection.cursor() as cursor:\n",
    "        cursor.execute(\"SELECT 1\")  # Execute the warm-up query (a simple query like 'SELECT 1' can be used)\n",
    "        cursor.fetchone()\n",
    "        connection.timeout = 10  # Setting a lower timeout for subsequent queries\n",
    "    for it in sql_deployment:\n",
    "        for i, query in enumerate(it[\"queries_schemas\"]):\n",
    "            print(f' - execute \"{query}\"')\n",
    "            cursor.execute(query)\n",
    "            cursor.commit()\n",
    "        for i, query in enumerate(it[\"queries_tables\"]):\n",
    "            print(f' - execute \"{query}\"')\n",
    "            cursor.execute(query)\n",
    "            cursor.commit()\n",
    "        for i, query in enumerate(it[\"queries_views\"]):\n",
    "            print(f' - execute \"{query}\"')\n",
    "            cursor.execute(query)\n",
    "            cursor.commit()\n",
    "        for i, query in enumerate(it[\"queries_stored_procedures\"]):\n",
    "            print(f' - execute \"{query}\"')\n",
    "            cursor.execute(query)\n",
    "            cursor.commit()\n",
    "        for i, query in enumerate(it[\"queries_logging\"]):\n",
    "            print(f' - execute \"{query}\"')\n",
    "            cursor.execute(query)\n",
    "            cursor.commit()\n",
    "        for i, query in enumerate(custom_sql_deployment[\"queries_stored_procedures\"]):\n",
    "            print(f' - execute \"{query}\"')\n",
    "            cursor.execute(query)\n",
    "            cursor.commit()\n",
    "        for i, query in enumerate(demo_sql_deployment[\"queries_stored_procedures\"]):\n",
    "            print(f' - execute \"{query}\"')\n",
    "            cursor.execute(query)\n",
    "            cursor.commit()\n",
    "\n",
    "    tasks.append({\"task_name\":f\"{workspace.get('displayName')} {database} query {i}\", \"task_duration\": 1, \"status\": f\"success\"})\n",
    "except pyodbc.OperationalError as e:\n",
    "    print(e) \n",
    "    tasks.append({\"task_name\":f\"{workspace.get('displayName')} {database} query {i}\", \"task_duration\": 1, \"status\": f\"pyodbc failed: {e}\"})\n",
    "except Exception as e:\n",
    "    print(e) \n",
    "    tasks.append({\"task_name\":f\"{workspace.get('displayName')} {database} query {i}\", \"task_duration\": 1, \"status\": f\"failed: {e}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c61b1ee-8f24-4760-b2a6-f207bb10eea4",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "display(tasks)"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "lakehouse": {}
  },
  "kernel_info": {
   "jupyter_kernel_name": "python3.11",
   "name": "jupyter"
  },
  "kernelspec": {
   "display_name": "Jupyter",
   "language": "Jupyter",
   "name": "jupyter"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "jupyter_python",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
