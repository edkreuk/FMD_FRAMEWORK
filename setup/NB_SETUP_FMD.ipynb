{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce6a663d-0e92-4d07-99e6-365c48b9c726",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "\"![FMD_Overview](https://github.com/edkreuk/FMD_FRAMEWORK/blob/main/Images/FMD_Overview.png?raw=true)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b167cef1-f936-4ca4-b2b2-22a88592ac11",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "%pip install ms-fabric-cli --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830b2b3e-9bdc-4318-af14-a855fff10a75",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Create Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ee43ea-7826-43e0-ac5f-43fa97b477f5",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "# Set environment parameters for Fabric CLI\n",
    "token = notebookutils.credentials.getToken('pbi')\n",
    "os.environ['FAB_TOKEN'] = token\n",
    "os.environ['FAB_TOKEN_ONELAKE'] = token\n",
    "\n",
    "tasks=[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f028883-3c89-4adf-8da2-9a1d7f5b979d",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "from time import sleep, time\n",
    "import json\n",
    "from zipfile import ZipFile \n",
    "import shutil\n",
    "import re\n",
    "import requests\n",
    "import zipfile\n",
    "from io import BytesIO\n",
    "import yaml\n",
    "import sempy.fabric as fabric\n",
    "import struct\n",
    "import pyodbc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ed7375-2e5f-4b5a-ae9f-672b337ddd94",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Configuration and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ada47fa-6823-4cdc-bebe-3f573f4f2bd1",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "load_demo_data_data= False         # Set to True if you want to load the demo data, otherwise set to False\n",
    "lakehouse_schema_enabled = True    # Set to True if you want to use the lakehouse schema, otherwise set to False\n",
    "\n",
    "driver = '{ODBC Driver 18 for SQL Server}' # Change this if you use a different driver\n",
    "# target connections guid \n",
    "# add the correct id for every connections\n",
    "# change the capacity id\n",
    "# change the workspace roles\n",
    "\n",
    "FrameworkName= 'FMD' # max 6 characters for better visibility, no spaces and the end of the name\n",
    "\n",
    "capacity_name_dvlm = '***********************'                  # Which capacity will be used for these workspaces in development\n",
    "capacity_name_prod = '***********************'                  # Which capacity will be used for these workspaces in production\n",
    "capacity_name_config = '***********************'                #'Which capacity will be used for this workspace for the FMD Database\n",
    "\n",
    "\n",
    "## original data\n",
    "\n",
    "workspace_roles = [ # Keep emtpy [] if you only want to assign this to your personal account\n",
    "                    {\n",
    "                        \"principal\": {\n",
    "                            \"id\": '***********************',\n",
    "                            \"displayName\": \"sg-fmd-fabric-contributor\", # Name of the group or user to assign the role to\n",
    "                            \"type\": \"Group\"\n",
    "                        },\n",
    "                        \"role\": \"contributor\"  #(choose from 'admin', 'member', 'contributor', 'viewer')\n",
    "\n",
    "                        }\n",
    "                    ]\n",
    "\n",
    "\n",
    "configuration = {\n",
    "                    'workspace': {\n",
    "                        'name' : FrameworkName + ' CONFIG FMD',             # Name of target workspace\n",
    "                        'roles' : workspace_roles,                          # Roles to assign to the workspace\n",
    "                        'capacity_name' : capacity_name_config              # Name of target capacity for the configuration workspace\n",
    "                    },\n",
    "                       'DatabaseName' : 'SQL_'+FrameworkName+'_FRAMEWORK'   # Name of target configuration SQL Database\n",
    "}\n",
    "environments = [\n",
    "                    {\n",
    "                        'environment_name' : 'development',                 # Name of target environment\n",
    "                        'workspaces': {\n",
    "                            'data' : {\n",
    "                                'name' : FrameworkName + ' DATA (D) FMD',   # Name of target code workspace for development\n",
    "                                'roles' : workspace_roles,                  # Roles to assign to the workspace\n",
    "                                'capacity_name' : capacity_name_dvlm        # Name of target data workspace for development\n",
    "                            },\n",
    "                            'code' : {\n",
    "                                'name' : FrameworkName + ' CODE (D) FMD',   # Name of target data workspace for development\n",
    "                                'roles' : workspace_roles,                  # Roles to assign to the workspace\n",
    "                                'capacity_name' : capacity_name_dvlm        # Name of target code workspace for development\n",
    "                            }\n",
    "                        },\n",
    "                        'connections' : {\n",
    "                            'CON_FMD_FABRIC_SQL' : '372237f9-709a-48f8-8fb2-ce06940c990e',          # Required Guid to the Fabric SQL connection\n",
    "                            'CON_FMD_FABRIC_PIPELINES' : '6d8146c6-a438-47df-94e2-540c552eb6d7',    # Required Guid to the Fabric datapipelines connection\n",
    "                            'CON_FMD_ADF_PIPELINES' : None                                          #'10000000-0000-0000-0000-000000000000'\n",
    "\n",
    "                        }\n",
    "                    },\n",
    "                    {\n",
    "                        'environment_name' : 'production',                  # Name of target environment\n",
    "                        'workspaces': {\n",
    "                            'data' : {\n",
    "                                'name' : FrameworkName + ' DATA (P) FMD',   # Name of target data workspace for production\n",
    "                                'roles' : workspace_roles,                  # Roles to assign to the workspace\n",
    "                                'capacity_name' : capacity_name_prod        # Name of target data workspace for production   \n",
    "                            },\n",
    "                            'code' : {\n",
    "                                'name' : FrameworkName + ' CODE (P) FMD',   # Name of target code workspace for production\n",
    "                                'roles' : workspace_roles,                  # Roles to assign to the workspace\n",
    "                                'capacity_name' : capacity_name_prod        # Name of target code workspace for production\n",
    "                            }\n",
    "                        },\n",
    "                        'connections' : {\n",
    "                            'CON_FMD_FABRIC_SQL' : '372237f9-709a-48f8-8fb2-ce06940c990e',          # Required Guid to the Fabric SQL connection\n",
    "                            'CON_FMD_FABRIC_PIPELINES' : '6d8146c6-a438-47df-94e2-540c552eb6d7',    # Required Guid to the Fabric datapipelines connection\n",
    "                            'CON_FMD_ADF_PIPELINES' : None                                          #'10000000-0000-0000-0000-000000000000'\n",
    "\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868f7b0f-a0ca-4d6c-ab64-b231eb3b0cba",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Repo Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e453b88-bb97-4378-b3fb-de6faa6b6d1d",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "#FMD Framework code\n",
    "##### DO NET CHANGE UNLESS SPECIFIED OTHERWISE ####\n",
    "repo_owner = \"edkreuk\"              # Owner of the repository\n",
    "repo_name = \"FMD_FRAMEWORK\"         # Name of the repository\n",
    "branch = \"main\"                     #\"main\" is default                    \n",
    "folder_prefix = \"\"\n",
    "###################################################\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32269821-d83f-41ac-a3d4-a19f8cb555de",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Download source & config files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22cc332-d939-4def-8971-84f762931ce2",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "def download_folder_as_zip(repo_owner, repo_name, output_zip, branch=\"main\", folder_to_extract=\"src\",  remove_folder_prefix = \"\"):\n",
    "    # Construct the URL for the GitHub API to download the repository as a zip file\n",
    "    url = f\"https://api.github.com/repos/{repo_owner}/{repo_name}/zipball/{branch}\"\n",
    "    \n",
    "    # Make a request to the GitHub API\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    folder_to_extract = f\"/{folder_to_extract}\" if folder_to_extract[0] != \"/\" else folder_to_extract\n",
    "    \n",
    "    # Ensure the directory for the output zip file exists\n",
    "    os.makedirs(os.path.dirname(output_zip), exist_ok=True)\n",
    "    \n",
    "    # Create a zip file in memory\n",
    "    with zipfile.ZipFile(BytesIO(response.content)) as zipf:\n",
    "        with zipfile.ZipFile(output_zip, 'w') as output_zipf:\n",
    "            for file_info in zipf.infolist():\n",
    "                parts = file_info.filename.split('/')\n",
    "                if  re.sub(r'^.*?/', '/', file_info.filename).startswith(folder_to_extract): \n",
    "                    # Extract only the specified folder\n",
    "                    file_data = zipf.read(file_info.filename)  \n",
    "                    if folder_prefix != \"\":\n",
    "                        parts.remove(remove_folder_prefix)\n",
    "                    output_zipf.writestr(('/'.join(parts[1:])), file_data)\n",
    "\n",
    "def uncompress_zip_to_folder(zip_path, extract_to):\n",
    "    # Ensure the directory for extraction exists\n",
    "    os.makedirs(extract_to, exist_ok=True)\n",
    "    \n",
    "    # Uncompress all files from the zip into the specified folder\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "    \n",
    "    # Delete the original zip file\n",
    "    os.remove(zip_path)\n",
    "\n",
    "download_folder_as_zip(repo_owner, repo_name, output_zip = \"./builtin/src/src.zip\", branch = branch, folder_to_extract= f\"{folder_prefix}/src\", remove_folder_prefix = f\"{folder_prefix}\")\n",
    "download_folder_as_zip(repo_owner, repo_name, output_zip = \"./builtin/config/config.zip\", branch = branch, folder_to_extract= f\"{folder_prefix}/config\" , remove_folder_prefix = f\"{folder_prefix}\")\n",
    "uncompress_zip_to_folder(zip_path = \"./builtin/config/config.zip\", extract_to= \"./builtin\")\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5c3526-9a63-430c-90e4-00ece0aebfa5",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## CLI Login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00e2fa8-e10d-45a9-82a7-266feaedc4ba",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "# Set environment parameters for Fabric CLI\n",
    "token = notebookutils.credentials.getToken('pbi')\n",
    "os.environ['FAB_TOKEN'] = token\n",
    "os.environ['FAB_TOKEN_ONELAKE'] = token\n",
    "\n",
    "tasks=[]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2171308-8526-4564-b8cb-21c588d2ae24",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Get connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0c6781-b3b5-47d0-a9ff-e192c7bdee9d",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "result = subprocess.run([\"fab\", \"api\", \"-X\", \"get\", \"connections\"], capture_output=True, text=True)\n",
    "\n",
    "#print(\"STDOUT:\", result.stdout)\n",
    "#print(\"STDERR:\", result.stderr)\n",
    "connections=json.loads(result.stdout)[\"text\"]\n",
    "connection_list = [item['id'] for item in connections['value']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b39ba9-62fb-421b-be07-2607f79fccdb",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Deployment functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7202e909-634f-42f5-a8b6-034430532d7a",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# FABRIC CLI Utilities\n",
    "# -------------------------------\n",
    "\n",
    "def run_fab_command(command, capture_output=False, silently_continue=False, raw_output=False):\n",
    "    \"\"\"\n",
    "    Executes a Fabric CLI command with optional output capture and error handling.\n",
    "    \"\"\"\n",
    "    result = subprocess.run([\"fab\", \"-c\", command], capture_output=capture_output, text=True)\n",
    "    if not silently_continue and (result.returncode > 0 or result.stderr):\n",
    "        raise Exception(f\"Error running fab command. exit_code: '{result.returncode}'; stderr: '{result}'\")\n",
    "    if capture_output:\n",
    "        return result if raw_output else result.stdout.strip()\n",
    "    return None\n",
    "\n",
    "# -------------------------------\n",
    "# Workspace Management\n",
    "# -------------------------------\n",
    "\n",
    "def get_workspace_id_by_name(workspace_name):\n",
    "    \"\"\"\n",
    "    Retrieves the workspace ID by its display name.\n",
    "    \"\"\"\n",
    "    result = run_fab_command(\"api -X get workspaces/\", capture_output=True, silently_continue=True)\n",
    "    workspaces = json.loads(result)[\"text\"][\"value\"]\n",
    "    normalized_name = workspace_name.strip().lower()\n",
    "    match = next((w for w in workspaces if w['displayName'].strip().lower() == normalized_name), None)\n",
    "    return match['id'] if match else None\n",
    "\n",
    "def ensure_workspace_exists(workspace):\n",
    "    \"\"\"\n",
    "    Ensures the workspace exists; creates it if not found.\n",
    "    \"\"\"\n",
    "    workspace_name = workspace['name']\n",
    "    workspace_id = get_workspace_id_by_name(workspace_name)\n",
    "    if workspace_id:\n",
    "        print(f\" - Workspace '{workspace_name}' found. Workspace ID: {workspace_id}- assign capacity: {workspace['capacity_name']}\")\n",
    "        run_fab_command(f'assign \".capacities/{workspace[\"capacity_name\"]}.Capacity\" -W \"{workspace_name}.Workspace\" -f', silently_continue=True)\n",
    "        return workspace_id, \"exists\"\n",
    "\n",
    "    print(f\" - Workspace '{workspace_name}' not found. Creating new workspace...\")\n",
    "    run_fab_command(f'mkdir \"{workspace_name}.workspace\" -P capacityName=\"{workspace[\"capacity_name\"]}\"', silently_continue=True)\n",
    "    workspace_id = get_workspace_id_by_name(workspace_name)\n",
    "    if workspace_id:\n",
    "        print(f\" - Created workspace '{workspace_name}'. ID: {workspace_id}\")\n",
    "        return workspace_id, \"created\"\n",
    "    else:\n",
    "        raise RuntimeError(f\"Workspace '{workspace_name}' could not be created or found.\")\n",
    "\n",
    "# -------------------------------\n",
    "# Item Utilities\n",
    "# -------------------------------\n",
    "\n",
    "def fab_get_id(workspace_name, name):\n",
    "    \"\"\"\n",
    "    Retrieves the item ID from a workspace.\n",
    "    \"\"\"\n",
    "    return run_fab_command(f\"get /{workspace_name}.Workspace/{name} -q id\", capture_output=True, silently_continue=True)\n",
    "\n",
    "def fab_get_display_name(workspace_name, name):\n",
    "    \"\"\"\n",
    "    Retrieves the display name of an item.\n",
    "    \"\"\"\n",
    "    return run_fab_command(f\"get /{workspace_name}.Workspace/{name} -q displayName\", capture_output=True, silently_continue=True)\n",
    "\n",
    "def fab_get_items(workspace_id, item_id=''):\n",
    "    \"\"\"\n",
    "    Retrieves item definitions or lists from a workspace.\n",
    "    \"\"\"\n",
    "    if item_id:\n",
    "        return run_fab_command(f\"api -X post workspaces/{workspace_id}/items/{item_id}/getDefinition\", capture_output=True, silently_continue=True)\n",
    "    return run_fab_command(f\"api -X get workspaces/{workspace_id}/items/{item_id}\", capture_output=True, silently_continue=True)\n",
    "\n",
    "# -------------------------------\n",
    "# File and ID Replacement\n",
    "# -------------------------------\n",
    "\n",
    "def copy_to_tmp(name, child=None):\n",
    "    \"\"\"\n",
    "    Extracts item files from a ZIP archive to a temporary directory.\n",
    "    \"\"\"\n",
    "    child_path = \"\" if child is None else f\".children/{child}/\"\n",
    "    shutil.rmtree(\"./builtin/tmp\", ignore_errors=True)\n",
    "    path2zip = \"./builtin/src/src.zip\"\n",
    "    with ZipFile(path2zip) as archive:\n",
    "        for file in archive.namelist():\n",
    "            if file.startswith(f'src/{name}/{child_path}'):\n",
    "                archive.extract(file, './builtin/tmp')\n",
    "    return f\"./builtin/tmp/src/{name}/{child_path}\"\n",
    "\n",
    "def replace_ids_in_folder(folder_path, mapping_table, environment_name):\n",
    "    \"\"\"\n",
    "    Replaces old IDs with new ones in specified file types within a folder.\n",
    "    \"\"\"\n",
    "    for root, _, files in os.walk(folder_path):\n",
    "        for file_name in files:\n",
    "            if file_name.endswith(('.py', '.json', '.pbir', '.platform', '.ipynb', '.tmdl')) and not file_name.endswith('report.json'):\n",
    "                file_path = os.path.join(root, file_name)\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    content = file.read()\n",
    "                    for mapping in mapping_table:\n",
    "                        if mapping[\"environment\"] in (environment_name, \"config\"):\n",
    "                            content = content.replace(mapping[\"old_id\"], mapping[\"new_id\"])\n",
    "                with open(file_path, 'w', encoding='utf-8') as file:\n",
    "                    file.write(content)\n",
    "\n",
    "def replace_ids_and_mark_inactive(folder_path, mapping_table, environment_name, target_guids):\n",
    "    \"\"\"\n",
    "    Replaces old IDs with new ones in JSON-based files and deactivates activities\n",
    "    that reference connections not in the target_guids list.\n",
    "\n",
    "    Parameters:\n",
    "    - folder_path (str): Path to the folder containing files to process.\n",
    "    - mapping_table (list): List of dictionaries with 'old_id', 'new_id', and 'environment'.\n",
    "    - environment_name (str): Current environment name to filter applicable mappings.\n",
    "    - target_guids (list): List of valid connection GUIDs to retain as active.\n",
    "\n",
    "    Returns:\n",
    "    - None. Files are modified in-place.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import json\n",
    "\n",
    "    def find_externalReferences_in_dict(j):\n",
    "        externalReferences = {}\n",
    "        for key, value in j.items():\n",
    "            if isinstance(value, dict):\n",
    "                externalReferences.update(find_externalReferences_in_dict(value))\n",
    "            if key == \"externalReferences\":\n",
    "                externalReferences[key] = value\n",
    "        return externalReferences\n",
    "\n",
    "    def should_deactivate(connection):\n",
    "        return (\n",
    "            connection not in target_guids and\n",
    "            connection not in ['@item().ConnectionGuid', '@pipeline().parameters.ConnectionGuid']\n",
    "        )\n",
    "\n",
    "    def process_nested_activities(activities):\n",
    "        for activity in activities:\n",
    "            result = find_externalReferences_in_dict(activity)\n",
    "            connection = result.get('externalReferences', {}).get('connection')\n",
    "            if connection and should_deactivate(connection):\n",
    "                print(f\"Deactivate activity {activity.get('name')} for connection {connection}\")\n",
    "                activity[\"state\"] = \"Inactive\"\n",
    "                activity[\"onInactiveMarkAs\"] = \"Succeeded\"\n",
    "\n",
    "    for root, _, files in os.walk(folder_path):\n",
    "        for file_name in files:\n",
    "            if file_name.endswith(('.py', '.json', '.pbir', '.platform', '.ipynb', '.tmdl')) and not file_name.endswith('report.json'):\n",
    "                file_path = os.path.join(root, file_name)\n",
    "\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    content = file.read()\n",
    "\n",
    "                # Replace IDs\n",
    "                for mapping in mapping_table:\n",
    "                    if mapping[\"environment\"] in (environment_name, \"config\"):\n",
    "                        if  mapping[\"new_id\"] != None:\n",
    "                            content = content.replace(mapping[\"old_id\"], mapping[\"new_id\"])\n",
    "\n",
    "                try:\n",
    "                    data = json.loads(content)\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "\n",
    "                if not data or not data.get(\"properties\") or not data[\"properties\"].get(\"activities\"):\n",
    "                    continue\n",
    "\n",
    "                for activity in data[\"properties\"][\"activities\"]:\n",
    "                    process_nested_activities([activity])\n",
    "                    for key in [\"activities\", \"ifFalseActivities\", \"ifTrueActivities\"]:\n",
    "                        nested = activity.get(\"typeProperties\", {}).get(key)\n",
    "                        if nested:\n",
    "                            process_nested_activities(nested)\n",
    "\n",
    "                content = json.dumps(data, indent=2)\n",
    "                with open(file_path, 'w', encoding='utf-8') as file:\n",
    "                    file.write(content)\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Description and Identity Assignment\n",
    "# -------------------------------\n",
    "\n",
    "def assign_workspace_description(workspace):\n",
    "    \"\"\"\n",
    "    Assigns a standard description to the workspace.\n",
    "    \"\"\"\n",
    "    workspace_name = workspace['name']\n",
    "    payload = 'This items are generated by the FMD Framework, everytime you run the setup Notebook all changes are overwritten. Check for more details on https://github.com/edkreuk/FMD_FRAMEWORK'\n",
    "    run_fab_command(f'set \"/{workspace_name}.workspace -q description -i {payload} -f', silently_continue=True)\n",
    "    print(f\" - Description applied to '{workspace_name}'\")\n",
    "\n",
    "def assign_item_description(workspace, item):\n",
    "    \"\"\"\n",
    "    Assigns a standard description to an item.\n",
    "    \"\"\"\n",
    "    workspace_name = workspace['name']\n",
    "    payload = 'This Item is initially generated by the FMD Framework, check for more details on https://github.com/edkreuk/FMD_FRAMEWORK'\n",
    "    run_fab_command(f'set \"/{workspace_name}.workspace/{item} -q description -i {payload} -f', silently_continue=True)\n",
    "    print(f\" - Description applied to {item} in '{workspace_name}'\")\n",
    "\n",
    "def assign_managed_identity(workspace):\n",
    "    \"\"\"\n",
    "    Assigns a managed identity to the workspace.\n",
    "    \"\"\"\n",
    "    workspace_name = workspace['name']\n",
    "    try:\n",
    "        run_fab_command(f'create \"/{workspace_name}.workspace/.managedidentities/{workspace_name}.ManagedIdentity\"', silently_continue=True)\n",
    "        print(f\" - Managed identity assigned to '{workspace_name}'\")\n",
    "    except Exception as e:\n",
    "        print(f\" - Failed to assign managed identity: {e}\")\n",
    "\n",
    "# -------------------------------\n",
    "# Role Assignment\n",
    "# -------------------------------\n",
    "\n",
    "def assign_roles(workspace):\n",
    "    \"\"\"\n",
    "    Assigns roles to principals in the workspace.\n",
    "    \"\"\"\n",
    "    workspace_path = f\"/{workspace['name']}.workspace\"\n",
    "    print(f\" - Assigning Workspace roles\")\n",
    "    for role in workspace['roles']:\n",
    "        try:\n",
    "            print(f\"Assigning role '{role['role']}' to '{role['principal']['displayName']}' in workspace '{workspace['name']}'\")\n",
    "            run_fab_command(\n",
    "                f'acl set \"{workspace_path}\" -I {role[\"principal\"][\"id\"]} -R {role[\"role\"]} -f',\n",
    "                silently_continue=True\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\" - Failed to assign role: {e}\")\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Folder Handling\n",
    "# -------------------------------\n",
    "\n",
    "def fab_get_folders(workspace_id):\n",
    "    \"\"\"\n",
    "    Retrieves all folders in a workspace.\n",
    "    \"\"\"\n",
    "    response = run_fab_command(f\"api workspaces/{workspace_id}/folders\", capture_output=True, silently_continue=True)\n",
    "    return json.loads(response).get('text', {}).get('value', [])\n",
    "\n",
    "def fab_get_folder(workspace_id, folder_name):\n",
    "    \"\"\"\n",
    "    Retrieves folder metadata by name.\n",
    "    \"\"\"\n",
    "    for f in fab_get_folders(workspace_id):\n",
    "        if f.get('displayName') == folder_name:\n",
    "            return f\n",
    "    return None\n",
    "\n",
    "def fab_assign_item_folder(workspace_id, item_id, folder):\n",
    "    \"\"\"\n",
    "    Assigns an item to a folder, creating the folder if it doesn't exist.\n",
    "    \"\"\"\n",
    "    folder_details = fab_get_folder(workspace_id, folder)\n",
    "    if folder_details is None:\n",
    "        payload = json.dumps({\"displayName\": folder})\n",
    "        print(f\"Folder does not exist, creating {payload}\")\n",
    "        folder_details = run_fab_command(f\"api -X post workspaces/{workspace_id}/folders -i {payload}\", capture_output=True, silently_continue=False)\n",
    "        folder_details = json.loads(folder_details).get('text', {})\n",
    "    payload = json.dumps({'folder': folder_details.get('id')})\n",
    "    result = run_fab_command(f\"api -X patch workspaces/{workspace_id}/items/{item_id} -i {payload}\", capture_output=True, silently_continue=False)\n",
    "    print(result)\n",
    "\n",
    "\n",
    "def deploy_workspaces(workspace, environment_name, old_id, mapping_table, tasks):\n",
    "    \"\"\"\n",
    "    Deploys a workspace by ensuring its existence, assigning identity, roles, and description.\n",
    "    Updates the mapping table and logs the deployment task.\n",
    "\n",
    "    Parameters:\n",
    "    - workspace (dict): Workspace configuration including name and capacity.\n",
    "    - environment_name (str): Target environment name.\n",
    "    - old_id (str): Previous workspace ID to be replaced.\n",
    "    - mapping_table (list): List to store ID mappings.\n",
    "    - tasks (list): List to store task execution logs.\n",
    "    \"\"\"\n",
    "    start = time()\n",
    "    print(\"\\n#############################################\")\n",
    "    print(f\" - Processing: workspace {workspace['name']}\")\n",
    "\n",
    "    workspace_id, status = ensure_workspace_exists(workspace)\n",
    "    workspace[\"id\"] = workspace_id\n",
    "\n",
    "    print(\"--------------------------\")\n",
    "    print(f\"Updating Mapping Table: {environment_name}\")\n",
    "    mapping_table.append({\n",
    "        \"Description\": workspace['name'],\n",
    "        \"environment\": environment_name,\n",
    "        \"old_id\": old_id,\n",
    "        \"new_id\": workspace_id\n",
    "    })\n",
    "    mapping_table.append({\n",
    "        \"Description\": workspace['name'],\n",
    "        \"environment\": environment_name,\n",
    "        \"old_id\": \"00000000-0000-0000-0000-000000000000\",\n",
    "        \"new_id\": workspace_id\n",
    "    })\n",
    "\n",
    "    assign_managed_identity(workspace)\n",
    "    assign_roles(workspace)\n",
    "    assign_workspace_description(workspace)\n",
    "\n",
    "    tasks.append({\n",
    "        \"task_name\": f\"Update workspace {workspace['name']}\",\n",
    "        \"task_duration\": int(time() - start),\n",
    "        \"status\": \"success\"\n",
    "    })\n",
    "\n",
    "\n",
    "\n",
    "def deploy_item(workspace, name, mapping_table, environment_name, connection_list, tasks, lakehouse_schema_enabled, child=None, it=None):\n",
    "    \"\"\"\n",
    "    Deploys an item (Notebook, Lakehouse, DataPipeline) into a workspace.\n",
    "    Handles ID replacement, description assignment, and updates mapping and task logs.\n",
    "\n",
    "    Parameters:\n",
    "    - workspace (dict): Workspace configuration including name.\n",
    "    - name (str): Name of the item to deploy.\n",
    "    - mapping_table (list): List to store ID mappings.\n",
    "    - environment_name (str): Target environment name.\n",
    "    - connection_list (list): List of valid connection GUIDs.\n",
    "    - tasks (list): List to store task execution logs.\n",
    "    - lakehouse_schema_enabled (bool): Flag to enable schema creation for lakehouses.\n",
    "    - child (str, optional): Child item name if applicable.\n",
    "    - it (dict, optional): Item metadata including old ID.\n",
    "    \"\"\"\n",
    "    start = time()\n",
    "    print(\"\\n#############################################\")\n",
    "    print(f\"Deploying in {workspace['name']}: {name}\")\n",
    "\n",
    "    tmp_path = copy_to_tmp(name, child)\n",
    "    name = name if child is None else child\n",
    "    workspace_name = workspace['name']\n",
    "    workspace_id = get_workspace_id_by_name(workspace_name)\n",
    "    cli_parameter = ''\n",
    "\n",
    "    if \"Notebook\" in name:\n",
    "        cli_parameter += \" --format .py\"\n",
    "        result = run_fab_command(\n",
    "            f\"import / {workspace_name}.Workspace/{name} -i {tmp_path} -f {cli_parameter}\",\n",
    "            capture_output=True, silently_continue=True, raw_output=False\n",
    "        )\n",
    "        assign_item_description(workspace, name)\n",
    "        new_id = fab_get_id(workspace_name, name)\n",
    "\n",
    "    elif \"Lakehouse\" in name:\n",
    "        if lakehouse_schema_enabled:\n",
    "            result = run_fab_command(\n",
    "                f\"create {workspace_name}.Workspace/{name} -P enableschemas=true\",\n",
    "                capture_output=True, silently_continue=True, raw_output=False\n",
    "            )\n",
    "        else:\n",
    "            result = run_fab_command(\n",
    "                f\"create {workspace_name}.Workspace/{name} -P enableschemas=false\",\n",
    "                capture_output=True, silently_continue=True, raw_output=False\n",
    "            )\n",
    "        assign_item_description(workspace, name)\n",
    "        new_id = fab_get_id(workspace_name, name)\n",
    "\n",
    "    elif \"DataPipeline\" in name:\n",
    "        print(f\"Replacing connections guid in {workspace['name']}: {name}\")\n",
    "        replace_ids_and_mark_inactive(tmp_path, mapping_table, environment_name, connection_list)\n",
    "        result = run_fab_command(\n",
    "            f\"import / {workspace_name}.Workspace/{name} -i {tmp_path} -f {cli_parameter}\",\n",
    "            capture_output=True, silently_continue=True, raw_output=False\n",
    "        )\n",
    "        assign_item_description(workspace, name)\n",
    "        new_id = fab_get_id(workspace_name, name)\n",
    "\n",
    "    elif \"VariableLibrary\" in name:   #Not working yet, import is giving error back\n",
    "        print(f\"Creating {workspace['name']}: {name}\")\n",
    "        result = run_fab_command(\n",
    "            f\"import / {workspace_name}.Workspace/{name} -i {tmp_path} -f\",\n",
    "            capture_output=True, silently_continue=True, raw_output=False\n",
    "        )\n",
    "\n",
    "        new_id = fab_get_id(workspace_name, name)\n",
    "    print(result)\n",
    "    if it:\n",
    "        mapping_table.append({\n",
    "            \"Description\": name,\n",
    "            \"environment\": environment_name,\n",
    "            \"old_id\": it[\"id\"],\n",
    "            \"new_id\": new_id\n",
    "        })\n",
    "\n",
    "    tasks.append({\n",
    "        \"task_name\": f\"Update item Definition {workspace_name} - {name}\",\n",
    "        \"task_duration\": int(time() - start),\n",
    "        \"status\": result\n",
    "    })\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33eb5fa4-5bf7-408b-9630-1533ba0d031a",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Load configuration\n",
    "Create  workspace, identity and roles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937feb4e-a67f-486c-ad43-0ae628b71e34",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "base_path = './builtin/'\n",
    "config_path = os.path.join(base_path, 'config/item_config.yaml')\n",
    "\n",
    "with open(config_path, 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "\n",
    "deploy_order_path = os.path.join(base_path, 'config/item_deployment.json')\n",
    "with open(deploy_order_path, 'r') as file:\n",
    "        item_deployment =json. load(file)\n",
    "\n",
    "deploy_order_path = os.path.join(base_path, 'config/sql_deployment.json')\n",
    "with open(deploy_order_path, 'r') as file:\n",
    "        sql_deployment =json. load(file)\n",
    "\n",
    "deploy_order_path = os.path.join(base_path, 'config/data_deployment.json')\n",
    "with open(deploy_order_path, 'r') as file:\n",
    "        data_deployment =json. load(file)\n",
    "\n",
    "deploy_order_path = os.path.join(base_path, 'config/lakehouse_deployment.json')\n",
    "with open(deploy_order_path, 'r') as file:\n",
    "        lakehouse_deployment =json. load(file)\n",
    "\n",
    "mapping_table=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c962f96-4a74-41ad-a6e0-397622eefbf9",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Deploy workspaces(Code and Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07949aa-d029-4e69-addd-14e03fbae79d",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "for environment in environments:\n",
    "    print(f\"--------------------------\")\n",
    "    print(f\"Updating Workspace: {environment['environment_name']}\")\n",
    "    deploy_workspaces(workspace=environment['workspaces']['code'], environment_name=environment['environment_name'], old_id=config[\"workspaces\"][\"workspace_code\"], mapping_table=mapping_table, tasks=tasks)\n",
    "    deploy_workspaces(workspace=environment['workspaces']['data'], environment_name=environment['environment_name'], old_id=config[\"workspaces\"][\"workspace_data\"], mapping_table=mapping_table, tasks=tasks)\n",
    "    \n",
    "    # Append the remaining connections\n",
    "    mapping_table.append({\"Description\": \"CON_FMD_FABRIC_SQL\" ,\"environment\": environment['environment_name'],\"old_id\": config['connections'][\"CON_FMD_FABRIC_SQL\"], \"new_id\": environment['connections']['CON_FMD_FABRIC_SQL']})\n",
    "    mapping_table.append({\"Description\": \"CON_FMD_FABRIC_PIPELINES\" ,\"environment\": environment['environment_name'] ,\"old_id\": config[\"connections\"][\"CON_FMD_FABRIC_PIPELINES\"], \"new_id\": environment['connections']['CON_FMD_FABRIC_PIPELINES']})\n",
    "    mapping_table.append({\"Description\": \"CON_FMD_ADF_PIPELINES\" ,\"environment\": environment['environment_name'] ,\"old_id\": config[\"connections\"][\"CON_FMD_ADF_PIPELINES\"], \"new_id\": environment['connections']['CON_FMD_ADF_PIPELINES']})\n",
    "\n",
    "print(f\"--------------------------\")   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b978e7-12ed-4697-a9fd-dc55a947a44e",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Deploy Workspace(Config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc31d7e-150e-440d-ad2d-95974b74babe",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Updating Workspace: Configuration\")\n",
    "deploy_workspaces(workspace=configuration['workspace'], environment_name='config', old_id=config[\"workspaces\"][\"workspace_config\"], mapping_table=mapping_table, tasks=tasks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7ae39f-a061-42d7-b33a-fa4c546b0574",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Deploy Lakehouses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcc14a9-0843-4f05-91c5-5efced447f73",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "for environment in environments:\n",
    "    print(f\"\\n--------------------------\")\n",
    "    print(f\"Processing: {environment['environment_name']}\")\n",
    "    for workspace in [environment['workspaces']['data']]:\n",
    "        exclude = []\n",
    "\n",
    "        for it in lakehouse_deployment:\n",
    "\n",
    "            new_id = None\n",
    "            \n",
    "            name = it[\"name\"]\n",
    "            type = it[\"type\"]\n",
    "\n",
    "            if name in exclude:\n",
    "                continue\n",
    "            deploy_item(workspace,name,mapping_table, environment['environment_name'],connection_list, tasks, lakehouse_schema_enabled,None,it)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5a35e1-75e0-40f4-9e81-107f9654bccc",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Deploy Fabric Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834a61eb-31dc-48ee-ab6a-08b9cf3439c0",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "for target_item in data_deployment:\n",
    "    if target_item['type'] in ('SQLDatabase','SQLEndpoint'):\n",
    "        target_item['name'] = configuration['DatabaseName']\n",
    "\n",
    "start = time()\n",
    "print(f\"\\n -----\")\n",
    "print(f\" - Processing: workspace {configuration['workspace']['name']}\")\n",
    "workspace_id, status = ensure_workspace_exists(configuration['workspace'])\n",
    "empty = True\n",
    "\n",
    "   \n",
    "target_items = fab_get_items(workspace_id)\n",
    "target_items = json.loads(target_items)[\"text\"]\n",
    "\n",
    "for deployment_item in data_deployment:\n",
    "    for target_item in target_items['value']:\n",
    "\n",
    "        if target_item['displayName'] == deployment_item['name'] \\\n",
    "                and target_item['type'] == deployment_item['type']:\n",
    "            print(f\" - Skip existing: {deployment_item['name']}, {deployment_item['type']}, {target_item['id']}\")\n",
    "            break\n",
    "    else:\n",
    "        \n",
    "        print(f\" - Creating: {deployment_item['name']} {deployment_item['type']} in workspace:{workspace_id} \")\n",
    "        item = deployment_item.copy()\n",
    "        \n",
    "        if empty:\n",
    "            if item.get('definition'):\n",
    "                print(f\" - Dropping definition\")\n",
    "                item.pop('definition')\n",
    "\n",
    "        # Construct the JSON payload\n",
    "        payload = json.dumps({\"displayName\": deployment_item['name'], \"Description\": \"Fabric SQL Database used by FMD_FRAMEWORK\"})\n",
    "        if deployment_item['type'] in ('SQLDatabase'):\n",
    "            try:\n",
    "                raw_response = run_fab_command(\n",
    "                    f\"api -X post workspaces/{workspace_id}/SQLDatabases -i '{payload}'\",\n",
    "                    capture_output=True,\n",
    "                    silently_continue=True\n",
    "                )\n",
    "                response = json.loads(raw_response)\n",
    "\n",
    "                if isinstance(response, dict) and response.get(\"status_code\", 200) >= 400:\n",
    "                    print(f\"{response.get('status_code')=}\\n{response.get('reason')=}\\n{response.get('text')=}\\n{url=}\\n{payload=}\\n{payloadtype=}\")\n",
    "                    if response.get(\"errorCode\"):\n",
    "                        print(f\"{response['errorCode']}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}. Retrying in 60 seconds...\")\n",
    "                sleep(60)\n",
    "                raw_response = run_fab_command(\n",
    "                    f\"api -X post workspaces/{workspace_id}/SQLDatabases -i '{payload}'\",\n",
    "                    capture_output=True,\n",
    "                    silently_continue=True\n",
    "                )\n",
    "                response = json.loads(raw_response)\n",
    "\n",
    "            #wait so SQL Database can be deployed and we can pick up the Guid after\n",
    "            sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf50a36-141d-4096-b169-d9b235678049",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Get Fabric database configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837c87ad-dbad-443d-84e8-08adf6e7dd21",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "for target_item in data_deployment:\n",
    "    if target_item['type'] in ('SQLDatabase','SQLEndpoint'):\n",
    "        target_item['name'] = configuration['DatabaseName']\n",
    "        \n",
    "start = time()\n",
    "print(f\"\\n -----\")\n",
    "print(f\" - Processing: workspace {configuration['workspace']['name']}\")\n",
    "workspace_id, status = ensure_workspace_exists(configuration['workspace'])\n",
    "empty = True\n",
    "\n",
    "   \n",
    "target_items = fab_get_items(workspace_id)\n",
    "target_items = json.loads(target_items)[\"text\"]\n",
    "for deployment_item in data_deployment:\n",
    "    for target_item in target_items['value']:\n",
    "\n",
    "        print(f\" - Updating mapping table: {deployment_item['name']} {deployment_item['type']} \")\n",
    "  \n",
    "        if deployment_item['type'] in ('SQLDatabase'):\n",
    "            if deployment_item.get('endpoint', '') != '':\n",
    "                return_item =  run_fab_command(f\"api -X get workspaces/{workspace_id}/SQLDatabases/{target_item['id']}\" , capture_output = True, silently_continue= True)\n",
    "                return_item = json.loads(return_item)[\"text\"]\n",
    "                if deployment_item['type'] in ('SQLDatabase'):\n",
    "                    if return_item.get(\"properties\", {}).get(\"serverFqdn\", \"\") != '':\n",
    "                        deployment_item[\"connectionString\"] = return_item[\"properties\"][\"serverFqdn\"].replace(',1433', '')\n",
    "                        mapping_table.append({\"Description\":deployment_item['name'] , \"environment\": 'config', \"old_id\": config[\"database\"][\"id\"], \"new_id\": target_item['id']})\n",
    "                        mapping_table.append({\"Description\":deployment_item['name'] , \"environment\": 'config', \"old_id\": deployment_item[\"endpoint\"], \"new_id\": deployment_item[\"connectionString\"]})\n",
    "                        tasks.append({\"task_name\":f\"Update item  Definition  {configuration['workspace']['name']} - {name}\", \"task_duration\": int(time() - start), \"status\": \"success\"})\n",
    "                    if return_item.get(\"properties\", {}).get(\"databaseName\", \"\") != '':\n",
    "                        deployment_item[\"databaseName\"] = return_item[\"properties\"][\"databaseName\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cc05d7-a761-452b-8c24-32f3b3d24c02",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Deploy Items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296baf6d-620f-4d13-9bf9-85506b864b41",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "for environment in environments:\n",
    "    print(f\"\\n--------------------------\")\n",
    "    print(f\"Processing: {environment['environment_name']}\")\n",
    "    for workspace in [environment['workspaces']['code']]:\n",
    "        exclude = []\n",
    "\n",
    "        for it in item_deployment:\n",
    "\n",
    "            new_id = None\n",
    "            \n",
    "            name = it[\"name\"]\n",
    "            type = it[\"type\"]\n",
    "\n",
    "            if name in exclude:\n",
    "                continue\n",
    "            deploy_item(workspace,name,mapping_table,environment['environment_name'],connection_list, tasks, lakehouse_schema_enabled, None,it)\n",
    "            \n",
    "\n",
    "            for child in it.get(\"children\",[]):\n",
    "                child_name = child[\"name\"]\n",
    "                print(child_name)\n",
    "                deploy_item(workspace,name,mapping_table, environment['environment_name'],connection_list, tasks, lakehouse_schema_enabled,child_name,child) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc252ff-6f50-41f9-9c6d-165300837bb6",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Create SQL deployment Manifest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1031bbb1-ae9a-4a26-8e14-e942a4fca2f8",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Add Connection to Fabric Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438153da-a957-4c48-9394-3cfc7c8aa014",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "custom_sql_deployment = {\"queries_stored_procedures\": []}\n",
    "for connection in connections['value']:\n",
    "    \n",
    "    display_name = connection.get('displayName', '')\n",
    "    if display_name and display_name.startswith('CON_FMD'):\n",
    "        connection_type = connection.get('connectionDetails', {}).get('type', 'Unknown')\n",
    "        connection_id = connection.get('id')\n",
    "      \n",
    "        exec_statement = (\n",
    "            f\"EXEC [integration].[sp_UpsertConnection] \"\n",
    "            f\"@ConnectionGuid = \\\"{connection_id}\\\", \"\n",
    "            f\"@Name = \\\"{display_name}\\\", \"\n",
    "            f\"@Type = \\\"{connection_type}\\\", \"\n",
    "            f\"@IsActive = 1\"\n",
    "        )\n",
    "        custom_sql_deployment[\"queries_stored_procedures\"].append(exec_statement)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed267d7-4aca-445c-9189-243307a972e6",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Add Workspaces to Fabric Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a6fd01-081e-48a0-9210-89d895cf2eb9",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "workspaces = []\n",
    "workspaces.append(configuration['workspace'])\n",
    "\n",
    "for environment in environments:\n",
    "    workspaces.append(environment['workspaces']['code'])\n",
    "    workspaces.append(environment['workspaces']['data'])\n",
    "    \n",
    "for workspace in workspaces:\n",
    "    print(f'EXEC [integration].[sp_UpsertWorkspace](@WorkspaceId = \"{workspace[\"id\"]}\" ,@Name = \"{workspace[\"name\"]}\")')\n",
    "    custom_sql_deployment[\"queries_stored_procedures\"].append(f'EXEC [integration].[sp_UpsertWorkspace] @WorkspaceId = \"{workspace[\"id\"]}\", @Name = \"{workspace[\"name\"]}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aacd228-4711-4836-a45d-0818ea6dc14e",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Add Data Pipelines to Fabric Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa8751f-ddad-47ed-9de2-1fbe2530d76f",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "for environment in environments:\n",
    "    result = run_fab_command(f\"api -X get workspaces/{environment['workspaces']['code']['id']}/items\", capture_output=True, silently_continue=True)\n",
    "    existing_items = json.loads(result)['text']\n",
    "    for item in existing_items.get('value', []):\n",
    "        if item['type'] == 'DataPipeline':\n",
    "            print(f'EXEC [integration].[sp_UpsertPipeline] @PipelineId = \"{item[\"id\"]}\", @WorkspaceId = \"{environment[\"workspaces\"][\"code\"][\"id\"]}\" ,@Name = \"{item[\"displayName\"]}\"')\n",
    "            custom_sql_deployment[\"queries_stored_procedures\"].append(f'EXEC [integration].[sp_UpsertPipeline] @PipelineId = \"{item[\"id\"]}\", @WorkspaceId = \"{environment[\"workspaces\"][\"data\"][\"id\"]}\" ,@Name = \"{item[\"displayName\"]}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4867745a-f0e7-4a9a-b326-5609b1b5bb3a",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Add Lakehouses to Fabric Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc60a4a-1e3d-4a57-b2f7-2a8cb23115c6",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "for environment in environments:\n",
    "    result = run_fab_command(f\"api -X get workspaces/{environment['workspaces']['data']['id']}/items\", capture_output=True, silently_continue=True)\n",
    "    existing_items = json.loads(result)['text']\n",
    "    for item in existing_items.get('value', []):\n",
    "        if item['type'] == 'Lakehouse':\n",
    "            print(f'EXEC [integration].[sp_UpsertLakehouse] @LakehouseId = \"{item[\"id\"]}\", @WorkspaceId = \"{environment[\"workspaces\"][\"data\"][\"id\"]}\" ,@Name = \"{item[\"displayName\"]}\"')\n",
    "            custom_sql_deployment[\"queries_stored_procedures\"].append(f'EXEC [integration].[sp_UpsertLakehouse] @LakehouseId = \"{item[\"id\"]}\", @WorkspaceId = \"{environment[\"workspaces\"][\"data\"][\"id\"]}\" ,@Name = \"{item[\"displayName\"]}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d60c485-1f24-4dbc-807b-45a71e14cf55",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Add Demo data for testing to Fabric Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a476a2-d34d-4540-ad24-439d447c5289",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "if load_demo_data_data:  \n",
    "    demo_sql_deployment = {\"queries_stored_procedures\": []}\n",
    "    demo_sql_deployment[\"queries_stored_procedures\"].append(f'EXEC [integration].[sp_UpsertConnection] @ConnectionGuid = \"00000000-0000-0000-0000-000000000000\", @Name = \"CON_FMD_ONELAKE\", @Type = \"ONELAKE\", @IsActive = 1')\n",
    "    demo_sql_deployment[\"queries_stored_procedures\"].append(\"\"\"\n",
    "        DECLARE @DataSourceIdInternal INT = (SELECT DataSourceId FROM integration.DataSource WHERE Name = 'LH_DATA_LANDINGZONE' and Type='ONELAKE_TABLES_01')\n",
    "        DECLARE @ConnectionIdInternal INT = (SELECT ConnectionId FROM integration.Connection WHERE ConnectionGuid = '00000000-0000-0000-0000-000000000000')\n",
    "        EXECUTE [integration].[sp_UpsertDataSource] \n",
    "            @ConnectionId = @ConnectionIdInternal\n",
    "            ,@DataSourceId = @DataSourceIdInternal\n",
    "            ,@Name = 'LH_DATA_LANDINGZONE'\n",
    "            ,@Namespace = 'ONELAKE'\n",
    "            ,@Type = 'ONELAKE_TABLES_01'\n",
    "            ,@Description = 'ONELAKE_TABLES'\n",
    "            ,@IsActive = 1\n",
    "    \"\"\")\n",
    "    demo_sql_deployment[\"queries_stored_procedures\"].append(\"\"\"\n",
    "        DECLARE @DataSourceIdInternal INT = (SELECT DataSourceId FROM integration.DataSource WHERE Name = 'LH_DATA_LANDINGZONE' and Type ='ONELAKE_FILES_01')\n",
    "        DECLARE @ConnectionIdInternal INT = (SELECT ConnectionId FROM integration.Connection WHERE ConnectionGuid = '00000000-0000-0000-0000-000000000000')\n",
    "        EXECUTE [integration].[sp_UpsertDataSource] \n",
    "            @ConnectionId = @ConnectionIdInternal\n",
    "            ,@DataSourceId = @DataSourceIdInternal\n",
    "            ,@Name = 'LH_DATA_LANDINGZONE'\n",
    "            ,@Namespace = 'ONELAKE'\n",
    "            ,@Type = 'ONELAKE_FILES_01'\n",
    "            ,@Description = 'ONELAKE_FILES'\n",
    "            ,@IsActive = 1\n",
    "    \"\"\")\n",
    "    demo_sql_deployment[\"queries_stored_procedures\"].append(\"\"\"\n",
    "        DECLARE @LandingzoneEntityIdInternal INT = (SELECT LandingzoneEntityId FROM integration.LandingzoneEntity WHERE SourceSchema = 'in' and SourceName = 'customer')\n",
    "        DECLARE @DataSourceIdInternal INT = (SELECT DataSourceId FROM integration.DataSource WHERE Name = 'LH_DATA_LANDINGZONE' and Type='ONELAKE_TABLES_01')\n",
    "        DECLARE @LakehouseIdInternal INT = (SELECT top 1 LakehouseId FROM integration.Lakehouse WHERE Name = 'LH_DATA_LANDINGZONE')\n",
    "        EXECUTE [integration].[sp_UpsertLandingzoneEntity] \n",
    "            @LandingzoneEntityId = @LandingzoneEntityIdInternal\n",
    "            ,@DataSourceId = @DataSourceIdInternal\n",
    "            ,@LakehouseId = @LakehouseIdInternal\n",
    "            ,@SourceSchema = 'in'\n",
    "            ,@SourceName = 'customer'\n",
    "            ,@SourceCustomSelect = ''\n",
    "            ,@FileName = 'customer'\n",
    "            ,@FilePath = 'fmd'\n",
    "            ,@FileType = 'parquet'\n",
    "            ,@IsIncremental = 0\n",
    "            ,@IsIncrementalColumn = ''\n",
    "            ,@IsActive = 1\n",
    "    \"\"\")\n",
    "    demo_sql_deployment[\"queries_stored_procedures\"].append(\"\"\"\n",
    "        DECLARE @LandingzoneEntityIdInternal INT = (SELECT LandingzoneEntityId FROM integration.LandingzoneEntity WHERE SourceSchema = 'in' and SourceName = 'customer')\n",
    "        DECLARE @BronzeLayerEntityIdInternal INT = (SELECT BronzeLayerEntityId FROM integration.BronzeLayerEntity WHERE [Schema] = 'in' and [Name] = 'customer')\n",
    "        DECLARE @LakehouseIdInternal INT = (SELECT top 1 LakehouseId FROM integration.Lakehouse WHERE Name = 'LH_BRONZE_LAYER')\n",
    "        EXECUTE [integration].[sp_UpsertBronzeLayerEntity] \n",
    "            @BronzeLayerEntityId = @BronzeLayerEntityIdInternal\n",
    "            ,@LandingzoneEntityId = @LandingzoneEntityIdInternal\n",
    "            ,@Schema = 'in'\n",
    "            ,@Name = 'customer'\n",
    "            ,@FileType = 'Delta'\n",
    "            ,@LakehouseId = @LakehouseIdInternal\n",
    "            ,@PrimaryKeys = 'CustomerId'\n",
    "            ,@IsActive = 1\n",
    "    \"\"\")\n",
    "    demo_sql_deployment[\"queries_stored_procedures\"].append(\"\"\"\n",
    "        DECLARE @BronzeLayerEntityIdInternal INT = (SELECT BronzeLayerEntityId FROM integration.BronzeLayerEntity WHERE [Schema] = 'in' and [Name] = 'customer')\n",
    "        DECLARE @SilverLayerEntityIdInternal INT = (SELECT SilverLayerEntityId FROM integration.SilverLayerEntity WHERE [Schema] = 'in' and [Name] = 'customer')\n",
    "        DECLARE @LakehouseIdInternal INT = (SELECT top 1 LakehouseId FROM integration.Lakehouse WHERE Name = 'LH_SILVER_LAYER')\n",
    "        EXECUTE [integration].[sp_UpsertSilverLayerEntity] \n",
    "            @SilverLayerEntityId = @SilverLayerEntityIdInternal\n",
    "            ,@BronzeLayerEntityId = @BronzeLayerEntityIdInternal\n",
    "            ,@LakehouseId = @LakehouseIdInternal\n",
    "            ,@Name = 'customer'\n",
    "            ,@Schema = 'in'\n",
    "            ,@FileType = 'delta'\n",
    "            ,@IsActive = 1\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e3bfba-b56e-47c7-a1e5-c764c2c3e622",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Deploy SQL Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb6c1ec-7fac-46d6-812a-50c00e413c5e",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "for target_item in data_deployment:\n",
    "    if target_item['type'] == 'SQLDatabase':\n",
    "        connstring = target_item[\"connectionString\"]\n",
    "        database = target_item['databaseName']\n",
    "\n",
    "\n",
    "try:\n",
    "    i = 0\n",
    "\n",
    "    token = notebookutils.credentials.getToken('pbi').encode('utf-16-le')\n",
    "    token_struct = struct.pack(f'<I{len(token)}s', len(token), token)\n",
    "\n",
    "    print(f\"DRIVER={driver};SERVER={connstring};PORT=1433;DATABASE={database};\")\n",
    "    connection = pyodbc.connect(f\"DRIVER={driver};SERVER={connstring};PORT=1433;DATABASE={database};\", attrs_before={1256:token_struct}, timeout=12)\n",
    "\n",
    "    with connection.cursor() as cursor:\n",
    "        cursor.execute(\"SELECT 1\")  # Execute the warm-up query (a simple query like 'SELECT 1' can be used)\n",
    "        cursor.fetchone()\n",
    "        connection.timeout = 10  # Setting a lower timeout for subsequent queries\n",
    "    for it in sql_deployment:\n",
    "        for i, query in enumerate(it[\"queries_tables\"]):\n",
    "            print(f' - execute \"{query}\"')\n",
    "            cursor.execute(query)\n",
    "            cursor.commit()\n",
    "        for i, query in enumerate(it[\"queries_views\"]):\n",
    "            print(f' - execute \"{query}\"')\n",
    "            cursor.execute(query)\n",
    "            cursor.commit()\n",
    "        for i, query in enumerate(it[\"queries_stored_procedures\"]):\n",
    "            print(f' - execute \"{query}\"')\n",
    "            cursor.execute(query)\n",
    "            cursor.commit()\n",
    "        for i, query in enumerate(it[\"queries_logging\"]):\n",
    "            print(f' - execute \"{query}\"')\n",
    "            cursor.execute(query)\n",
    "            cursor.commit()\n",
    "    for i, query in enumerate(custom_sql_deployment[\"queries_stored_procedures\"]):\n",
    "        print(f' - execute \"{query}\"')\n",
    "        cursor.execute(query)\n",
    "        cursor.commit()\n",
    "    for i, query in enumerate(demo_sql_deployment[\"queries_stored_procedures\"]):\n",
    "        print(f' - execute \"{query}\"')\n",
    "        cursor.execute(query)\n",
    "        cursor.commit()\n",
    "\n",
    "    tasks.append({\"task_name\":f\"{workspace.get('displayName')} {database} query {i}\", \"task_duration\": 1, \"status\": f\"success\"})\n",
    "except pyodbc.OperationalError as e:\n",
    "    print(e) \n",
    "    tasks.append({\"task_name\":f\"{workspace.get('displayName')} {database} query {i}\", \"task_duration\": 1, \"status\": f\"pyodbc failed: {e}\"})\n",
    "except Exception as e:\n",
    "    print(e) \n",
    "    tasks.append({\"task_name\":f\"{workspace.get('displayName')} {database} query {i}\", \"task_duration\": 1, \"status\": f\"failed: {e}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c61b1ee-8f24-4760-b2a6-f207bb10eea4",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "display(tasks)"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "lakehouse": {}
  },
  "kernel_info": {
   "jupyter_kernel_name": "python3.11",
   "name": "jupyter"
  },
  "kernelspec": {
   "display_name": "Jupyter",
   "language": "Jupyter",
   "name": "jupyter"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "jupyter_python",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
